{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6e8d86e-397f-4127-adab-d585dae98e6e",
   "metadata": {},
   "source": [
    "# Land Use / Land Cover Segmentation Using Sentinel-2 and Random Forest\n",
    "\n",
    "This workflow demonstrates how to use a [Sentinel-2](https://www.esa.int/Applications/Observing_the_Earth/Copernicus/Sentinel-2) [GeoMedian annual satellite imagery composite](https://github.com/digitalearthpacific/dep-geomad) for segmenting land use / land cover (LULC) using a [GPU-accelerated Random Forest classifier](https://developer.nvidia.com/blog/accelerating-random-forests-up-to-45x-using-cuml/). We will pursue this objective by integrating ground truth land use land cover data from the VBoS from 2022. To make this scalable to all of Vanuatu, we use an [administrative boundaries dataset from Pacific data hub](https://pacificdata.org/data/dataset/2016_vut_phc_admin_boundaries/resource/66ae054b-9b67-4876-b59c-0b078c31e800).\n",
    "\n",
    "In this notebook, we will demonstrate the following:\n",
    "\n",
    "1. **Data Acquisition**:\n",
    "   - We use **Sentinel-2 L2A** data accessed via the [Digital Earth Pacific STAC catalog](http://stac.digitalearthpacific.org/). The search is filtered by parameters like a region of interest (AOI) and time range to obtain suitable imagery.\n",
    "   \n",
    "2. **Preprocessing**:\n",
    "   - The Sentinel-2 imagery contains several spectral bands (e.g., Red, Green, Blue, Near-Infrared, Short-wave Infrared). These are extracted and combined into a single dataset for analysis. Remote sensing indices useful for land use / land cover mapping are calculated from these bands. Additionally, the imagery is masked to remove areas outside the regions of interest so as to focus on the relevant pixels. We use 5 out of 6 provinces making up the nation of Vanuatu for training, and one for testing.\n",
    "  \n",
    "3. **Feature Extraction**:\n",
    "   - Features for the classifier are extracted from the Sentinel-2 spectral bands. Here, we will use the reflectance values from the Red, Green, Blue, Near-Infrared (NIR), and Short-wave Infrared (SWIR) bands. We will compute remote sensing indices (NDVI, MNDWI, SAVI, BSI) from these bands as the final feature set.\n",
    "\n",
    "4. **Ground Truth Data Integration**:\n",
    "   - A shapefile containing polygons attributed by land cover/land use is loaded into a [GeoDataFrame](https://geopandas.org/en/stable/docs/reference/api/geopandas.GeoDataFrame.html). This allows us to create multi-class labels for the pixels in the Sentinel-2 imagery.\n",
    "  \n",
    "5. **Data Splitting**:\n",
    "   - To ensure correct model training, we split the features and labels into training (80%) and testing (20%) sets. A 'seed' value is used for the random number generator to ensure this random split is reproducible.\n",
    "\n",
    "6. **Random Forest Classification**:\n",
    "   - We train a **Random Forest** classifier to predict land use/land cover on a pixel-wise basis. The `n_estimators` parameter is a key hyperparameter, determining the number of decision trees in the forest. Random Forest leverages the collective wisdom of multiple decision trees to make accurate predictions.\n",
    "\n",
    "7. **Prediction**:\n",
    "   - We will use the trained classifier to predict the likelihood of lulc types for each pixel in the test image/province. \n",
    "\n",
    "8. **Evaluation**:\n",
    "   - After making predictions on the test partition, we evaluate the model's performance using metrics such as accuracy and F1-score. This allows us to assess the performance of the Random Forest model and the effectiveness of the selected features.\n",
    "\n",
    "9. **Visualization**:\n",
    "   - We visualize the predictions by plotting the classified map, where lulc types are indicated by specific color codes.\n",
    "\n",
    "At the end, you will have trained a model to predict land use + land cover in Vanuatu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a38a354-23cf-4dec-9b4c-b7b4115335ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mamba install --channel rapidsai --quiet --yes cuml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25784d0e-fb2d-4efc-bba7-a98f51727ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1426b4f7-3f69-40ca-aeba-dd4fe0b0855b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "import dask.dataframe as dd\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import hvplot.xarray\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import odc.stac\n",
    "import rasterio.features\n",
    "import rioxarray as rxr\n",
    "import xarray as xr\n",
    "from cuml import RandomForestClassifier\n",
    "from dask import compute, delayed\n",
    "from dask_ml.model_selection import train_test_split\n",
    "from geocube.api.core import make_geocube\n",
    "from pystac_client import Client\n",
    "from shapely.geometry import Polygon, box, mapping, shape\n",
    "from sklearn.metrics import (\n",
    "    ConfusionMatrixDisplay,\n",
    "    accuracy_score,\n",
    "    classification_report,\n",
    ")\n",
    "from tqdm import tqdm  # for progress bar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4054c9d-c6f0-45fd-b420-3b2a54df7506",
   "metadata": {},
   "source": [
    "## Data Acquisition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735b7671-0113-4ddb-983b-bb8af4f2b637",
   "metadata": {},
   "source": [
    "Let's read the LULC data into a GeoDataFrame. \n",
    "\n",
    "A [GeoDataFrame](https://geopandas.org/en/stable/docs/reference/geodataframe.html) is a type of data structure used to store geographic data in Python, provided by the [GeoPandas](https://geopandas.org/en/stable/) library. It extends the functionality of a pandas DataFrame to handle spatial data, enabling geospatial analysis and visualization. Like a pandas DataFrame, a GeoDataFrame is a tabular data structure with labeled axes (rows and columns), but it adds special features to work with geometric objects, such as:\n",
    "- a geometry column\n",
    "- a CRS\n",
    "- accessibility to spatial operations (e.g.  intersection, union, buffering, and spatial joins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4548ff5-1a2e-4abb-8cc7-949ac308a373",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the LULC ROI data (ROIs.zip)\n",
    "!gdown \"https://drive.google.com/uc?id=1i0T3RqEgqcNXEUnPuDXod94EZE4IgQ7J\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24656fe9-45e0-444e-8db9-175e437f9ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip ROIs.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97fe226d-8880-44fb-b367-364a36f73991",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the administrative boundaries (2016_phc_vut_pid_4326.geojson)\n",
    "!wget https://pacificdata.org/data/dataset/9dba1377-740c-429e-92ce-6a484657b4d9/resource/3d490d87-99c0-47fd-98bd-211adaf44f71/download/2016_phc_vut_pid_4326.geojson"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d58704b-b5ca-49aa-acdb-62fd5af8fd89",
   "metadata": {},
   "source": [
    "Read and inspect the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f276e07b-8261-40ed-9b05-128b5a803195",
   "metadata": {},
   "outputs": [],
   "source": [
    "lulc_gdf = gpd.read_file(\"./ROIs/ROIs_v5.shp\") #\"./ROIs_v5.shp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3baeacd5-db00-4374-9e6f-2302385abf40",
   "metadata": {},
   "outputs": [],
   "source": [
    "admin_boundaries_gdf = gpd.read_file(\"./2016_phc_vut_pid_4326.geojson\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f198c37-d514-4347-b17e-26b90d4898c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "admin_boundaries_gdf.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb0abfd7-e312-4ea3-9be9-926f442a14f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(lulc_gdf), len(admin_boundaries_gdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b854c0fe-3f49-40cc-ab61-9e1f71a93ab9",
   "metadata": {},
   "source": [
    "We can check out the attributes associated with this dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9110fdc8-efde-402f-9c4b-b05aebc8dfb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "lulc_gdf.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c9f0cf-e919-4cd0-be8a-522326940524",
   "metadata": {},
   "source": [
    "Let's see which classes are available to us in the most recent LULC column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01cc0168-d323-49ff-919f-35481efbacb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "lulc_gdf.ROI.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e300eb09-5b6e-4a1a-85ba-0b5b040147d3",
   "metadata": {},
   "source": [
    "And view a subset of the data (shuffled for more variety in the 10 samples):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae82fb65-e95e-4f04-bbbb-5657321552e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "lulc_gdf.sample(frac=1).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8485a8-140a-45aa-a0de-5848a97507f0",
   "metadata": {},
   "source": [
    "We can also plot the vector dataset, and color code the polygons by the relevant LULC column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a28e47c-a617-4575-b9c4-f0b73b5695ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "lulc_gdf.plot(column='ROI')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b00cc243-9891-488c-8860-8e163dad2ae5",
   "metadata": {},
   "source": [
    "Create raster image and label xarray datarrays for each province."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f780c3ca-c0e1-4959-92e2-a24951140ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "admin_boundaries_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f931db5-88ee-42a0-9f78-5fe40d9fe68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "YEAR = 2022 # year matching label data\n",
    "PROVINCES_TRAIN = [\"TORBA\", \"SANMA\", \"PENAMA\", \"MALAMPA\", \"SHEFA\"]\n",
    "PROVINCE_TEST = \"TAFEA\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd96951-74e0-419f-a245-08d9dddc2632",
   "metadata": {},
   "outputs": [],
   "source": [
    "admin_boundaries_gdf = admin_boundaries_gdf.set_index(keys=\"pname\")  # set province name as the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645fd19c-baf7-4e47-8db0-fe71ec563a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "admin_boundaries_gdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "392b394b-6ac5-4a34-9ea1-c5f2f3426a8a",
   "metadata": {},
   "source": [
    "Get geometries of each province."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86fbe34-228e-4ae6-a441-6da150fda482",
   "metadata": {},
   "outputs": [],
   "source": [
    "GEOMS_TRAIN = admin_boundaries_gdf.loc[PROVINCES_TRAIN].geometry.tolist()\n",
    "GEOMS_TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143a24ed-dec1-496e-83bf-2a5a0b0a6a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "GEOM_TEST = admin_boundaries_gdf.loc[PROVINCE_TEST].geometry\n",
    "GEOM_TEST"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92524bb7-59b6-47a5-9764-684f5ef25ef6",
   "metadata": {},
   "source": [
    "Get Sentinel-2 GeoMedian composite data for 2022 for each province."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58045aca-5dfd-4fa8-b41b-d71f26a574d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "STAC_URL = \"http://stac.digitalearthpacific.org/\"\n",
    "stac_client = Client.open(STAC_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f73002-5656-4e8e-bf1a-89225210f1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect s2_data per train province in a list\n",
    "s2_data_train_list = []\n",
    "\n",
    "for pname, geom in tqdm(zip(PROVINCES_TRAIN, GEOMS_TRAIN), total=len(GEOMS_TRAIN), desc=\"Loading GeoMAD per province\"):\n",
    "    try:\n",
    "        # Query STAC for this province\n",
    "        s2_search = stac_client.search(\n",
    "            collections=[\"dep_s2_geomad\"], # Sentinel-2 Geometric Median and Absolute Deviations (GeoMAD) over the Pacific.\n",
    "            intersects=mapping(geom),  # GeoJSON dict\n",
    "            datetime=str(YEAR),\n",
    "        )\n",
    "        s2_items = s2_search.item_collection()\n",
    "\n",
    "        if len(s2_items) == 0:\n",
    "            print(f\"No items found for {pname}\")\n",
    "            continue\n",
    "\n",
    "        # Load data from items\n",
    "        s2_data = odc.stac.load(\n",
    "            items=s2_items,\n",
    "            bands=[\"blue\", \"green\", \"red\", \"nir08\", \"swir16\"],\n",
    "            chunks={\"x\": 1024, \"y\": 1024, \"bands\": -1, \"time\": -1},\n",
    "            resolution=20,\n",
    "        )\n",
    "\n",
    "        s2_data_train_list.append(s2_data)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {pname}: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de1d985-ba4f-4e2c-92a5-a606f946676b",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_test = lulc_gdf.query(expr=f\"Pname == '{PROVINCE_TEST}'\")\n",
    "\n",
    "s2_search = stac_client.search(\n",
    "    collections=[\"dep_s2_geomad\"],\n",
    "    intersects=GEOM_TEST, \n",
    "    datetime=str(YEAR),\n",
    ")\n",
    "# Retrieve all items from search results\n",
    "s2_items = s2_search.item_collection()\n",
    "print(\"len(s2_items): \", len(s2_items))\n",
    "\n",
    "s2_data_test = odc.stac.load(\n",
    "    items=s2_items,\n",
    "    bands=[\"blue\", \"green\", \"red\", \"nir08\", \"swir16\"],\n",
    "    chunks={'x': 1024, 'y': 1024, 'bands': -1, 'time': -1},\n",
    "    resolution=20,\n",
    ")\n",
    "s2_data_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82bbc580-77aa-4259-be6f-497e31b94698",
   "metadata": {},
   "source": [
    "Buffer the geometries to include some coastal offshore areas to account for any classes/ROIs that might be relevant and overlapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64217bbc-bf35-4137-a118-52f31167eb4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep projection aligned with raster\n",
    "raster_crs = s2_data_train_list[0].rio.crs\n",
    "\n",
    "# Reproject the full subset once\n",
    "gdf_reprojected_train = admin_boundaries_gdf.loc[PROVINCES_TRAIN].to_crs(crs=raster_crs)\n",
    "\n",
    "# Create a dictionary of buffered geometries per province\n",
    "geom_buffered_train = {\n",
    "    pname: gdf_reprojected_train.loc[pname].geometry.buffer(5000)\n",
    "    for pname in PROVINCES_TRAIN\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01161c59-fbd4-4dec-8432-78f61429d349",
   "metadata": {},
   "outputs": [],
   "source": [
    "geom_train_buffered_list = list(geom_buffered_train.values())\n",
    "geom_train_buffered_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1adc06-1cd2-460a-9c1b-8e990d0d79bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "geom_train_buffered_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76fe6847-8a6d-46b3-ab76-5dbbe7dd148d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep projection aligned with raster\n",
    "raster_crs = s2_data_test.rio.crs\n",
    "\n",
    "# Get only the select province and reproject\n",
    "gdf_reprojected_test = admin_boundaries_gdf.loc[[PROVINCE_TEST]].to_crs(crs=raster_crs)\n",
    "\n",
    "# Buffer in raster units (meters if UTM)\n",
    "geom_buffered_test = gdf_reprojected_test.buffer(distance=5000)[PROVINCE_TEST]\n",
    "geom_buffered_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de4cb45-2a7f-44bc-aacd-1f563b9939e5",
   "metadata": {},
   "source": [
    "Clip the Sentinel-2 data to be within the buffered geometries only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6516b75-c5fa-4deb-9976-848f5665bd34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure the keys match — we'll zip province names, geometries, and s2 datasets\n",
    "s2_train_clipped_list = []\n",
    "\n",
    "for pname, geom, s2_data in zip(PROVINCES_TRAIN, geom_buffered_train.values(), s2_data_train_list):\n",
    "    try:\n",
    "        # Clip the dataset to the buffered province geometry\n",
    "        s2_clipped = s2_data.rio.clip(\n",
    "            geometries=[mapping(geom)],\n",
    "            crs=s2_data.rio.crs,\n",
    "            drop=True\n",
    "        )\n",
    "        s2_train_clipped_list.append(s2_clipped)\n",
    "    except Exception as e:\n",
    "        print(f\"Error clipping data for {pname}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0197b1a3-9ea0-43ff-b360-f27d85dd5515",
   "metadata": {},
   "outputs": [],
   "source": [
    "s2_train_clipped_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0b3d7a-62e5-489d-9909-9e6067b23841",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clip test province\n",
    "s2_clipped_test = s2_data_test.rio.clip(geometries=[geom_buffered_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799c71f7-ce95-4840-9cea-c8469eacb477",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot sample train province\n",
    "s2_rgb = s2_train_clipped_list[0][[\"red\", \"green\", \"blue\"]] \n",
    "s2_rgb_array = s2_rgb.to_array(\"band\")  # now dims: band, y, x\n",
    "s2_rgb_array_squeezed = s2_rgb_array.squeeze(dim=\"time\", drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633a2510-1d7f-424d-8e9a-5a32a96d272a",
   "metadata": {},
   "outputs": [],
   "source": [
    "s2_rgb_array_squeezed.plot.imshow(size=4, vmin=0, vmax=4000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "969c6b4d-f958-49b6-9a51-2b010ab4da09",
   "metadata": {},
   "source": [
    "Calculate remote sensing indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870709ee-e4e6-4bf7-ae5f-ce68c63e520b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate remote sensing indices useful for mapping LULC\n",
    "def compute_indices(ds):\n",
    "    red = ds[\"red\"]\n",
    "    green = ds[\"green\"]\n",
    "    blue = ds[\"blue\"]\n",
    "    nir = ds[\"nir08\"]\n",
    "    swir = ds[\"swir16\"]\n",
    "    eps = 1e-6\n",
    "    return xr.Dataset({\n",
    "        \"NDVI\": (nir - red) / (nir + red + eps),\n",
    "        \"MNDWI\": (green - swir) / (green + swir + eps),\n",
    "        \"SAVI\": ((nir - red) / (nir + red + eps)) * 1.5,\n",
    "        \"BSI\": ((swir + red) - (nir + blue)) / ((swir + red) + (nir + blue) + eps),\n",
    "    })\n",
    "\n",
    "index_data_train_list = []\n",
    "\n",
    "for s2_clipped in s2_train_clipped_list:\n",
    "    index_data = compute_indices(s2_clipped).squeeze(\"time\", drop=True)\n",
    "    index_data_train_list.append(index_data)\n",
    "\n",
    "index_data_test = compute_indices(s2_clipped_test).squeeze(\"time\", drop=True)\n",
    "print(index_data_train_list[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "090572ea-2a72-4f01-9a70-8193cb23cde8",
   "metadata": {},
   "source": [
    "Rasterize labels from the ROIs for training and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed12d39a-7fdb-484e-8dfc-fedc6b9d7f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rasterize labels\n",
    "width_test, height_test = s2_clipped_test.x.size, s2_clipped_test.y.size\n",
    "bands = ['red', 'green', 'blue', 'nir08']\n",
    "\n",
    "#print(gdf_.ROI.unique(), gdf_.ROI_numeric.unique())\n",
    "gdf_test = gdf_test.to_crs(epsg=s2_clipped_test.rio.crs.to_epsg())\n",
    "\n",
    "# Define the resolution and bounds based on Sentinel-2 features\n",
    "resolution = s2_clipped_test.rio.resolution()\n",
    "bounds_test = s2_clipped_test.rio.bounds()\n",
    "\n",
    "gdf_rpg = lulc_gdf.to_crs(s2_clipped_test.rio.crs)\n",
    "\n",
    "unique_classes = gdf_rpg['ROI'].unique()\n",
    "#class_mapping = {cls: i+1 for i, cls in enumerate(unique_classes)}\n",
    "class_mapping = {cls: i for i, cls in enumerate(unique_classes)} # zero-based, assumes existence of no data\n",
    "\n",
    "# Add numerical column\n",
    "gdf_rpg['ROI_numeric'] =  gdf_rpg['ROI'].map(class_mapping)\n",
    "\n",
    "raster_bounds_test = box(*s2_clipped_test.rio.bounds())\n",
    "gdf_test_clipped = gdf_rpg[gdf_rpg.intersects(raster_bounds_test)]\n",
    "\n",
    "print(f\"Before: {len(gdf_rpg)} | After: {len(gdf_test_clipped)}\")\n",
    "\n",
    "# Rasterize the vector dataset to match Sentinel-2\n",
    "rasterized_labels_test = make_geocube(\n",
    "    vector_data=gdf_test_clipped,\n",
    "    measurements=[\"ROI_numeric\"], \n",
    "    like=s2_clipped_test,  # Align with the features dataset\n",
    ")\n",
    "\n",
    "print(\"rasterized_labels_test: \", rasterized_labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79b68cb-b855-49f8-b62e-f851d63dbcce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rasterize labels\n",
    "rasterized_labels_train_dict = {}\n",
    "gdf_train_clipped_dict = {}\n",
    "metadata_dict = {}\n",
    "\n",
    "for s2_clipped_train, pname in zip(s2_train_clipped_list, PROVINCES_TRAIN):\n",
    "    print(f\"\\n Processing province: {pname}\")\n",
    "\n",
    "    # Sentinel-2 metadata\n",
    "    width = s2_clipped_train.sizes['x']\n",
    "    height = s2_clipped_train.sizes['y']\n",
    "    resolution = s2_clipped_train.rio.resolution()\n",
    "    bounds = s2_clipped_train.rio.bounds()\n",
    "    epsg = s2_clipped_train.rio.crs.to_epsg()\n",
    "    raster_bounds = box(*bounds)\n",
    "\n",
    "    # Reproject LULC GeoDataFrame to match S2 CRS\n",
    "    gdf_rpg = lulc_gdf.to_crs(s2_clipped_train.rio.crs.to_epsg())\n",
    "\n",
    "    # Class mapping (safe per-province if needed)\n",
    "    unique_classes = gdf_rpg['ROI'].unique()\n",
    "    #class_mapping = {cls: i+1 for i, cls in enumerate(unique_classes)}\n",
    "    class_mapping = {cls: i for i, cls in enumerate(unique_classes)} # zero-based, assumes existence of no data    \n",
    "    gdf_rpg['ROI_numeric'] = gdf_rpg['ROI'].map(class_mapping)\n",
    "\n",
    "    # Clip vector LULC to S2 bounds\n",
    "    gdf_train_clipped = gdf_rpg[gdf_rpg.intersects(raster_bounds)]\n",
    "    print(f\"Vector features: {len(gdf_rpg)} → Clipped: {len(gdf_train_clipped)}\")\n",
    "\n",
    "    if len(gdf_train_clipped) == 0:\n",
    "        print(f\"No vector data found for province: {pname}, skipping rasterization.\")\n",
    "        continue\n",
    "\n",
    "    # Rasterize clipped vector labels\n",
    "    rasterized_labels_train = make_geocube(\n",
    "        vector_data=gdf_train_clipped,\n",
    "        measurements=[\"ROI_numeric\"],\n",
    "        like=s2_clipped_train\n",
    "    )\n",
    "\n",
    "    # Store outputs\n",
    "    rasterized_labels_train_dict[pname] = rasterized_labels_train\n",
    "    gdf_train_clipped_dict[pname] = gdf_train_clipped\n",
    "    metadata_dict[pname] = {\n",
    "        \"width\": width,\n",
    "        \"height\": height,\n",
    "        \"epsg\": epsg,\n",
    "        \"resolution\": resolution,\n",
    "        \"bounds\": bounds,\n",
    "        \"class_mapping\": class_mapping\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3df4e8-3a99-4478-8d36-478f97368766",
   "metadata": {},
   "outputs": [],
   "source": [
    "rasterized_labels_train_dict[\"SHEFA\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95874c3-2d86-4e47-950a-bd7f7b5af3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "gdf_train_clipped_dict[\"TORBA\"].plot(ax=ax, facecolor=\"none\", edgecolor=\"red\")\n",
    "bbox = box(*s2_train_clipped_list[0].rio.bounds())\n",
    "gpd.GeoSeries([bbox], crs=s2_train_clipped_list[0].rio.crs).plot(ax=ax, facecolor=\"none\", edgecolor=\"blue\")\n",
    "plt.title(\"gdf_train (red) vs. Raster Bounds (blue)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4000304-f6e4-4644-a939-d75e0476d761",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_train_clipped_dict[\"TORBA\"].ROI.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428c7bc7-084f-47ee-a6c9-b07567c8a95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(rasterized_labels_train_dict[\"TORBA\"]['ROI_numeric'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05093e3-cd52-4387-84e2-74d0b5ce4356",
   "metadata": {},
   "source": [
    "Flatten pixels and only retain the those that overlap with an ROI. The labels (ROIs) are sparse, so we will throw out pixels in regions between ROIs (unlabeled). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b0922a-7dcf-47fd-99db-c09f47033eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_list = []\n",
    "labels_list = []\n",
    "\n",
    "for i, prov_name in enumerate(PROVINCES_TRAIN):\n",
    "    # Stack spatial dimensions first\n",
    "    features_train = index_data_train_list[i].to_array().stack(flattened_pixel=(\"y\", \"x\"))\n",
    "    labels_train = rasterized_labels_train_dict[prov_name].to_array().stack(flattened_pixel=(\"y\", \"x\"))\n",
    "    \n",
    "    # Compute mask for valid pixels (no NaNs across all features or labels)\n",
    "    mask = (\n",
    "        np.isfinite(features_train).all(dim=\"variable\") &\n",
    "        np.isfinite(labels_train).all(dim=\"variable\")\n",
    "    ).compute()\n",
    "    \n",
    "    # Apply the mask to drop invalid pixels\n",
    "    features_train = features_train[:, mask].transpose(\"flattened_pixel\", \"variable\").compute()\n",
    "    labels_train = labels_train[:, mask].transpose(\"flattened_pixel\", \"variable\").squeeze().compute()\n",
    "    \n",
    "    labels_train = labels_train.astype(int)\n",
    "\n",
    "    features_list.append(features_train)\n",
    "    labels_list.append(labels_train)\n",
    "\n",
    "# Concatenate all provinces along the flattened_pixel dimension\n",
    "features_train = xr.concat(features_list, dim=\"flattened_pixel\")\n",
    "labels_train = xr.concat(labels_list, dim=\"flattened_pixel\")\n",
    "\n",
    "print(\"Combined features_train shape:\", features_train.shape)\n",
    "print(\"Combined zero-based labels_train shape:\", labels_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e76c0f8f-41f7-456f-a8ee-02f3d35e4d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(labels_train.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46bdd89-9f7e-4cfa-a93a-5f4e7e84a564",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_test = index_data_test.to_array().stack(flattened_pixel=(\"y\", \"x\"))\n",
    "labels_test = rasterized_labels_test.to_array().stack(flattened_pixel=(\"y\", \"x\"))\n",
    "\n",
    "test_mask = (\n",
    "    np.isfinite(features_test).all(dim=\"variable\") &\n",
    "    np.isfinite(labels_test).all(dim=\"variable\")\n",
    ").compute()\n",
    "\n",
    "features_test = features_test[:].transpose(\"flattened_pixel\", \"variable\").compute()\n",
    "labels_test = labels_test[:].transpose(\"flattened_pixel\", \"variable\").squeeze().compute()\n",
    "print(\"labels_test values:\", np.unique(labels_test.values))\n",
    "\n",
    "# Convert nan to 255\n",
    "labels_test = labels_test.fillna(255).astype(int)\n",
    "labels_test = labels_test.astype(int)\n",
    "print(\"labels_test values:\", np.unique(labels_test.values))\n",
    "\n",
    "print(\"features_test shape:\", features_test.shape)\n",
    "print(\"labels_test shape:\", labels_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dafa686a-85d3-4ce3-9b37-fbd7a27f84d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(features_train), len(labels_train), len(features_test), len(labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05016c65-2812-43f2-adf5-ea3d4ee22540",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564c4a76-87f1-410d-b708-e4fc2a1dd9b0",
   "metadata": {},
   "source": [
    "## Data Splitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb071561-2fc2-4717-ae65-123c934368ab",
   "metadata": {},
   "source": [
    "Now that we have the arrays flattened, we can split the datasets into training and testing partitions. We will reserve 80 percent of the data for training, and 20 percent for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d14aae-24b8-4564-a31e-3e1832b822c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    features_train, labels_train, test_size=0.2, random_state=42, shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab55889b-5db0-4348-a346-144420c2fc2b",
   "metadata": {},
   "source": [
    "Ensure all labels are in each partition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0807d03-2295-4a53-98ce-fb792c48de56",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(y_train), np.unique(y_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c696912-1f27-4b34-8007-47fb573cb75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X_train), len(X_test), len(y_train), len(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc10505a-cc3a-4448-bf71-eeab83304e26",
   "metadata": {},
   "source": [
    "## Random Forest Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fef16a1-524c-4b3f-b52c-99361fa6e953",
   "metadata": {},
   "source": [
    "Now we will set up a small [random forest classifider](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) with 10 trees. We use a [seed](https://towardsdatascience.com/why-do-we-set-a-random-state-in-machine-learning-models-bb2dc68d8431) (`random_state`) to ensure reproducibility. Calling the `.fit()` method on the classifier will initiate training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b87a9afc-4675-4155-b59b-64ae7a1b9e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Train a Random Forest classifier\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42) #n_estimators=10\n",
    "clf.fit(X_train.data, y_train.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb7ba1e5-9daf-47b7-93b4-e3f2f8766c9e",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e91599a4-a99a-461a-b25b-107c636a4b9f",
   "metadata": {},
   "source": [
    "Once the classifier is finished training, we can use it to make predictions on our test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c3ba9c-5197-442a-a3b0-1d58556b75f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the classifier\n",
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d52804fc-5536-4835-bcb7-bc17d2708989",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e98b15a-5c20-4484-8566-8384759ebf4c",
   "metadata": {},
   "source": [
    "It's important to know how well our classifier performs relative to the true labels (`y_test`). For this, we can calculate the [accuracy metric](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html) to measure agreement between the true and predicted labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881f753f-8181-4a91-adf1-dd8ef62cf61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd23d68a-622f-4062-ba90-eda7e5862534",
   "metadata": {},
   "source": [
    "We can also produce a [classification report](https://scikit-learn.org/1.7/modules/generated/sklearn.metrics.classification_report.html)\n",
    "to check the precision, recall and F1 scores for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea3fa0d-adad-461b-b10c-3f0f030d8812",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_true=y_test, y_pred=y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1439772-ae4f-4b4c-8176-d73e41907404",
   "metadata": {},
   "source": [
    "As a reminder, these are what each class number represents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9fba31f-2728-4b21-823b-fcfafa62b4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Class mapping:\")\n",
    "for key, val in class_mapping.items():\n",
    "    print(val, key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd33be5c-f705-49de-98bf-6a51de412ebe",
   "metadata": {},
   "source": [
    "We can also plot a confusion matrix to explore per-class performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941c49f1-118f-4771-9f8f-cc53185e9078",
   "metadata": {},
   "outputs": [],
   "source": [
    "ConfusionMatrixDisplay.from_predictions(\n",
    "    y_true=y_test, y_pred=y_pred, normalize=\"true\", values_format=\".2f\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e012f8-9ef3-471e-87f1-ab420774e4df",
   "metadata": {},
   "source": [
    "Notice that we see a high variability in the performance across classes. This is likely due to a class imbalance or inter-class differentiation challenge within our training dataset. It's possible that augmentations or class revision may help to address this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba2bb751-296e-47b0-8f31-5a37fbd38a0d",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6c9d34-7821-45d7-a3d4-28f399c79a2e",
   "metadata": {},
   "source": [
    "If we want to generate predictions for the entire dataset in order to plot a map of predicted LULC for the entire area of interest, we can do this using the test province dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "558a43da-bd66-4552-9783-7c6a2e8bc0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(features_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e32fcb3-3bbd-4b2f-8899-ab927c19aa15",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predicted_map = y_pred.reshape((height_test, width_test))\n",
    "predicted_map_xr = xr.DataArray(data=predicted_map, coords=rasterized_labels_test.coords)\n",
    "print(np.unique(y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935e6900-74d0-49df-af04-135928b78be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_map_xr.hvplot.image(height=600, rasterize=True, cmap=\"set1_r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7b45f8-502b-4f99-957f-f205cf02ba60",
   "metadata": {},
   "outputs": [],
   "source": [
    "rasterized_labels_test.ROI_numeric.hvplot.image(rasterize=True, cmap=\"set1_r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50064063-1bb5-4c53-b4ca-1f63acfc06a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "rasterized_labels_test[\"ROI_numeric\"].plot(ax=axes[0], cmap=\"viridis\")\n",
    "axes[0].set_title(\"Ground truth\")\n",
    "axes[0].set_aspect('equal')\n",
    "\n",
    "predicted_map_xr.plot(ax=axes[1], cmap=\"viridis\")\n",
    "axes[1].set_title(\"Predictions\")\n",
    "axes[1].set_aspect('equal')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be4bf4a-6fdf-4f02-8ebd-b8f4202e5159",
   "metadata": {},
   "outputs": [],
   "source": [
    "compatible_array = predicted_map_xr.astype(\"int32\")\n",
    "\n",
    "# Rasterize to polygons\n",
    "polygons = list(\n",
    "    rasterio.features.shapes(compatible_array.values, transform=compatible_array.rio.transform())\n",
    ")\n",
    "\n",
    "# Convert polygons to GeoDataFrame\n",
    "prediction_gdf = gpd.GeoDataFrame(\n",
    "    [{\"geometry\": shape(geom), \"value\": value} for geom, value in polygons],\n",
    "    crs=\"EPSG:4326\",\n",
    ")\n",
    "#print(prediction_gdf)\n",
    "print(prediction_gdf.value.unique())\n",
    "\n",
    "prediction_gdf.to_file(f\"./predicted_lulc_utm_{PROVINCE_TEST}_{YEAR}.geojson\", driver=\"GeoJSON\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb80046-1623-478b-afa7-2ac0f3d64f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_gdf.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a09a7d",
   "metadata": {},
   "source": [
    "You can run these predictions on every province, collect the geodataframes in a list, and combine them into a final, unfiied nationwide LULC vector dataset like so (placeholder code, you need to generate the predictions first):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93de9b62-6e0a-4a4f-b25b-4ad7832f020c",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_gdf_merged_nationwide = pd.concat([prediction_gdf_TORBA, prediction_gdf_SANMA, prediction_gdf_PENAMA,\n",
    "                        prediction_gdf_MALAMPA, prediction_gdf_SHEFA, prediction_gdf_TAFEA], ignore_index=True)\n",
    "\n",
    "prediction_gdf_merged_nationwide.to_file(f\"./predicted_lulc_utm_nationwide_{YEAR}.geojson\", driver=\"GeoJSON\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6fceef-679c-4e89-af27-c65e41343027",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
