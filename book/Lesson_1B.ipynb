{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6e8d86e-397f-4127-adab-d585dae98e6e",
   "metadata": {},
   "source": [
    "# Land Use / Land Cover Segmentation Using Sentinel-2 and Random Forest\n",
    "\n",
    "This workflow demonstrates how to use [Sentinel-2](https://www.esa.int/Applications/Observing_the_Earth/Copernicus/Sentinel-2) satellite imagery for segmenting land use / land cover (LULC) using a [Random Forest](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) classifier. We focus on detecting planted forests by integrating ground truth forest areas from the **National Forest Classification Dataset (LULC)** from 2018.\n",
    "\n",
    "In this notebook, we will demonstrate the following:\n",
    "\n",
    "1. **Data Acquisition**:\n",
    "   - We use **Sentinel-2 L2A** data (Level-2A provides surface reflectance) accessed via the [AWS STAC catalog](https://registry.opendata.aws/). The search is filtered by parameters like a region of interest (AOI), time range, and cloud cover percentage to obtain suitable imagery.\n",
    "   \n",
    "2. **Preprocessing**:\n",
    "   - The Sentinel-2 imagery contains several spectral bands (e.g., Red, Green, Blue, and Near-Infrared). These are extracted and combined into a single dataset for analysis. Additionally, the imagery is masked to remove areas outside the AOI and focus on the relevant pixels.\n",
    "  \n",
    "3. **Feature Extraction**:\n",
    "   - Features for the classifier are extracted from the Sentinel-2 spectral bands. Here, we will use the reflectance values from the Red, Green, Blue, and Near-Infrared (NIR) bands. We will mask out clouds from these bands before further analysis.\n",
    "\n",
    "4. **Ground Truth Data Integration**:\n",
    "   - A shapefile containing polygons attributed by land cover/land use is loaded into a [GeoDataFrame](https://geopandas.org/en/stable/docs/reference/api/geopandas.GeoDataFrame.html). This allows us to create multi-class labels for the pixels in the Sentinel-2 imagery.\n",
    "  \n",
    "5. **Data Splitting**:\n",
    "   - To ensure correct model training, we split the features and labels into training (80%) and testing (20%) sets. A 'seed' value is used for the random number generator to ensure this random split is reproducible.\n",
    "\n",
    "6. **Random Forest Classification**:\n",
    "   - We train a **Random Forest** classifier to predict planted forest areas. The `n_estimators` parameter is a key hyperparameter, determining the number of decision trees in the forest. Random Forest leverages the collective wisdom of multiple decision trees to make accurate predictions.\n",
    "\n",
    "7. **Prediction**:\n",
    "   - We will use the trained classifier to predict the likelihood of lulc types for each pixel in the image. \n",
    "\n",
    "8. **Evaluation**:\n",
    "   - After making predictions on the test set, we evaluate the model's performance using metrics such as accuracy and F1-score. This allows us to assess the performance of the Random Forest model and the effectiveness of the selected features.\n",
    "\n",
    "9. **Visualization**:\n",
    "   - We visualize the predictions by plotting the classified map, where lulc types are indicated by specific color codes.\n",
    "\n",
    "At the end, you will have trained a model to predict land use + land cover in Vanuatu.\n",
    "\n",
    "![result](https://github.com/user-attachments/assets/6794df2b-45b4-4c6a-923b-98b33e305a39)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1426b4f7-3f69-40ca-aeba-dd4fe0b0855b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import odc.stac\n",
    "import xarray as xr\n",
    "from geocube.api.core import make_geocube\n",
    "from pystac_client import Client\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4054c9d-c6f0-45fd-b420-3b2a54df7506",
   "metadata": {},
   "source": [
    "## Data Acquisition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735b7671-0113-4ddb-983b-bb8af4f2b637",
   "metadata": {},
   "source": [
    "Let's read the LULC data into a GeoDataFrame. \n",
    "\n",
    "A [GeoDataFrame](https://geopandas.org/en/stable/docs/reference/geodataframe.html) is a type of data structure used to store geographic data in Python, provided by the [GeoPandas](https://geopandas.org/en/stable/) library. It extends the functionality of a pandas DataFrame to handle spatial data, enabling geospatial analysis and visualization. Like a pandas DataFrame, a GeoDataFrame is a tabular data structure with labeled axes (rows and columns), but it adds special features to work with geometric objects, such as:\n",
    "- a geometry column\n",
    "- a CRS\n",
    "- accessibility to spatial operations (e.g.  intersection, union, buffering, and spatial joins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f276e07b-8261-40ed-9b05-128b5a803195",
   "metadata": {},
   "outputs": [],
   "source": [
    "lulc_gdf = gpd.read_file(\"./lulc_utm_subset.geojson\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b854c0fe-3f49-40cc-ab61-9e1f71a93ab9",
   "metadata": {},
   "source": [
    "We can check out the attributes associated with this dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9110fdc8-efde-402f-9c4b-b05aebc8dfb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "lulc_gdf.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c9f0cf-e919-4cd0-be8a-522326940524",
   "metadata": {},
   "source": [
    "Let's see which classes are available to us in the most recent LULC column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01cc0168-d323-49ff-919f-35481efbacb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "lulc_gdf.lulc_2018.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e300eb09-5b6e-4a1a-85ba-0b5b040147d3",
   "metadata": {},
   "source": [
    "And view a subset of the data (shuffled for more variety in the 10 samples):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae82fb65-e95e-4f04-bbbb-5657321552e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "lulc_gdf.sample(frac=1).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8485a8-140a-45aa-a0de-5848a97507f0",
   "metadata": {},
   "source": [
    "We can also plot the vector dataset, and color code the polygons by the relevant LULC column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a28e47c-a617-4575-b9c4-f0b73b5695ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "lulc_gdf.plot(column='lulc_2018')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67340000-903a-46fd-9ac7-4270aa1ee6a1",
   "metadata": {},
   "source": [
    "Let's get the bounds of the dataset to provide in a query for overlapping satellite imagery."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a7e49a-d0fb-4876-913f-2778ac7c0ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the bounds of the forest polygons to define the AOI\n",
    "aoi = lulc_gdf.to_crs(epsg=\"4326\").total_bounds  # (minx, miny, maxx, maxy)\n",
    "aoi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ebf3d9-f4c3-4eba-ac7c-b839eb90a5f2",
   "metadata": {},
   "source": [
    "We are going to search the AWS open data STAC API just as before for Sentinel-2 L2A atmospherically corrected satellite imagery."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438c2f58-dcb2-40d4-8a5b-710de636654c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access AWS STAC for Sentinel-2 Data\n",
    "aws_stac_url = \"https://earth-search.aws.element84.com/v1\"\n",
    "stac_client = Client.open(aws_stac_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4096dc19-c35c-4625-9ea6-6e2bdfad2032",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search Sentinel-2 data on AWS with cloud cover less than 20%\n",
    "s2_search = stac_client.search(\n",
    "    collections=[\"sentinel-2-l2a\"],\n",
    "    bbox=list(aoi),\n",
    "    datetime=\"2018-01-01/2018-12-31\", \n",
    "    query={\"eo:cloud_cover\": {\"lt\": 5}}  # Filter by cloud cover < 5%\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874e68c0-6e51-417b-99bd-0b219b5ff8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve all items from search results\n",
    "s2_items = s2_search.item_collection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1dd6bfd-4072-414c-909c-00c89ce72ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(s2_items)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef21ba30-6332-4a46-8230-ea0ca6c7cd91",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f693f9-a5e8-4a26-8491-2aaa267a2a30",
   "metadata": {},
   "source": [
    "Now that we have a list of relevant image scenes, let's read the relevant assets into an `xarray.Dataset`. We will chunk the data with dask to increase processing speed and efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c727769a-efe2-4c86-98af-0de8657472c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "s2_data = odc.stac.load(\n",
    "    items=s2_items,\n",
    "    bands=[\"red\", \"green\", \"blue\", \"nir\", \"scl\"],\n",
    "    bbox=aoi,\n",
    "    chunks={'x': 1024, 'y': 1024, 'bands': -1, 'time': -1},\n",
    "    resolution=80,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4815bb-c18a-43cb-8d5b-55e3765010d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "s2_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28302539-4127-4fc9-aa94-33fbc72b520f",
   "metadata": {},
   "source": [
    "Let's define our no data value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58256f57-2cfa-4d29-ab7d-4771fff77728",
   "metadata": {},
   "outputs": [],
   "source": [
    "s2_data_nodata = s2_data[\"scl\"].nodata\n",
    "s2_data_nodata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4171b00c-d034-492f-9c81-27c61c747d7b",
   "metadata": {},
   "source": [
    "Also, width and height (in pixel space, NOT latitude and longitude)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1e0489-335e-4e76-bacb-2e0b5e153d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "width, height = s2_data.x.size, s2_data.y.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e8c7f3-65ac-4852-ac93-c40d1c88e18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "width, height"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b324a591-e40b-48d1-900d-914ae3378d14",
   "metadata": {},
   "source": [
    "We will also need to know the CRS for the satellite imagery."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05957e3-57ce-4540-a77d-78c29efb28e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "epsg = s2_data.rio.crs.to_epsg()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130f5e6f-aa7b-462e-a450-1b4863c14bfd",
   "metadata": {},
   "source": [
    "## Feature Extraction\n",
    "\n",
    "Now we will proceed with feature extraction, which will entail selection for the bands we will use for training an LULC model (the features). Cloud cover can obstruct the signal in these bands, so we will mask those out before creating a temporal composite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019b433a-d4dd-429a-bc67-ef5698a64899",
   "metadata": {},
   "outputs": [],
   "source": [
    "bands = ['red', 'green', 'blue', 'nir']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4508108-2148-4034-b6b4-2a01d42b4b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "cloud_classes = [3, 7, 8, 9, 10]  # Cloud-related SCL classes\n",
    "cloud_mask = s2_data['scl'].isin(cloud_classes)\n",
    "s2_data_masked = s2_data[bands].where(~cloud_mask, drop=False)  # Keep all pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44fdf3a4-7abc-4477-b35c-06288f86b4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#nodata_mask = s2_data['scl'].isin(s2_data_nodata)\n",
    "#s2_data_composite = s2_data[bands].where(~s2_data_nodata, drop=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a4eeac6-7f76-43c7-9337-6f7ab840d02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average across the time dimension\n",
    "s2_data_composite = s2_data_masked.median(dim='time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf782bb2-0556-4102-9b47-5aade3cd6dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "s2_data_composite[[\"red\", \"green\", \"blue\"]].to_array(\"band\").plot.imshow(rgb=\"band\", robust=True, size=5, aspect=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb808460-5e58-4596-82e7-4517f75e7655",
   "metadata": {},
   "outputs": [],
   "source": [
    "cloud_mask.median(dim='time').plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a997d50-f464-4f75-9cf2-374ddb1f3926",
   "metadata": {},
   "source": [
    "## Ground Truth Data Integration\n",
    "\n",
    "Now that we have our features, let's obtain our labels. The first step is to generate numerical equivalents for the string classes. We index the valid classes starting from 1 so as to reserve 0 for no data pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183a535b-c226-4da3-9cce-9f9cc209bd4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get unique classes and assign integers\n",
    "unique_classes = lulc_gdf['lulc_2018'].unique()\n",
    "class_mapping = {cls: i+1 for i, cls in enumerate(unique_classes)}\n",
    "\n",
    "# Add numerical column\n",
    "lulc_gdf['lulc_2018_numeric'] = lulc_gdf['lulc_2018'].map(class_mapping)\n",
    "\n",
    "print(lulc_gdf.lulc_2018.unique(), lulc_gdf.lulc_2018_numeric.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f32b684f-3dfb-404a-926f-68786cffe122",
   "metadata": {},
   "source": [
    "We need these in raster format. Thus, we will inctroduce a new tool called [geocube](https://corteva.github.io/geocube/stable/) that \"rasterizes\" vector data into a gridded `xarray.Dataset` that matches up with the raster features! \n",
    "\n",
    "We start by making sure the LULC vector data is projected in the same CRS as the satellite imagery."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c5dca8-b8f6-4713-b352-06eea1b9b9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "lulc_gdf = lulc_gdf.to_crs(epsg=epsg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ccce8d4-4ce8-423a-afc8-e3c8ba5dcb93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the resolution and bounds based on Sentinel-2 features\n",
    "resolution = s2_data.rio.resolution()\n",
    "bounds = s2_data.rio.bounds()\n",
    "\n",
    "# Rasterize the vector dataset to match Sentinel-2\n",
    "rasterized_labels = make_geocube(\n",
    "    vector_data=lulc_gdf,\n",
    "    measurements=[\"lulc_2018_numeric\"], \n",
    "    like=s2_data,  # Align with the features dataset\n",
    ")\n",
    "\n",
    "# The rasterized output is an xarray.Dataset\n",
    "print(np.unique(rasterized_labels[\"lulc_2018_numeric\"].values))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ec2b28-7539-440c-b0c3-ef84fc5f0761",
   "metadata": {},
   "source": [
    "Let's convert no data encoded as `nan` to zero. We need all pixels to have a numeric value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d887de1-f4a0-4923-9321-9c72a307077a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rasterized_labels = rasterized_labels.where(~np.isnan(rasterized_labels), other=0)  # Replace NaNs with 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4a6048-bb67-4ec4-bf5b-aeb840db01c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.unique(rasterized_labels[\"lulc_2018_numeric\"].values))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba2e06e0-f5b0-4dec-93ba-a18c1cba072c",
   "metadata": {},
   "source": [
    "We also need our labels to be of type integer, not float."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a94d9e6-5d5a-422f-86d5-d0ab2040df6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rasterized_labels = rasterized_labels.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97081425-d96d-4c71-827f-f2b23b592451",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.unique(rasterized_labels[\"lulc_2018_numeric\"].values))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5714253d-c654-44d0-ba9f-a854b0fade86",
   "metadata": {},
   "source": [
    "Let's plot our new labels `xarray.Dataset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8c50b8-50ec-4600-9a6c-b9075fa163d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "rasterized_labels[\"lulc_2018_numeric\"].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0403e5-d113-4239-a291-9c044d781c7e",
   "metadata": {},
   "source": [
    "Now we have to flatten the feature and labels datasets because the random forest segmentor expects the inputs to be 1-dimensional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916a99b4-8a11-4c75-a633-9af473862bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = s2_data_composite.to_array().stack(flattened_pixel=(\"y\", \"x\")).transpose(\"flattened_pixel\", \"variable\")\n",
    "labels = rasterized_labels.to_array().stack(flattened_pixel=(\"y\", \"x\")).transpose(\"flattened_pixel\", \"variable\").squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "756ffd7a-c3fd-4697-bcdf-f141fce39b19",
   "metadata": {},
   "source": [
    "Let's check the new shapes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502940fe-5df2-4235-bb66-f1a8225080c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "features.shape, labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564c4a76-87f1-410d-b708-e4fc2a1dd9b0",
   "metadata": {},
   "source": [
    "## Data Splitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb071561-2fc2-4717-ae65-123c934368ab",
   "metadata": {},
   "source": [
    "Now that we have the arrays flattened, we can split the datasets into training and testing partitions. We will reserve 80 percent of the data for training, and 20 percent for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb715b97-07de-4f24-9ec3-b3efe9b2dfbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c696912-1f27-4b34-8007-47fb573cb75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X_train), len(X_test), len(y_train), len(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc10505a-cc3a-4448-bf71-eeab83304e26",
   "metadata": {},
   "source": [
    "## Random Forest Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fef16a1-524c-4b3f-b52c-99361fa6e953",
   "metadata": {},
   "source": [
    "Now we will set up a small [random forest classifider](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) with 10 trees. We use a [seed](https://towardsdatascience.com/why-do-we-set-a-random-state-in-machine-learning-models-bb2dc68d8431) (`random_state`) to ensure reproducibility. Calling the `.fit()` method on the classifier will initiate training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b87a9afc-4675-4155-b59b-64ae7a1b9e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Train a Random Forest classifier\n",
    "clf = RandomForestClassifier(n_estimators=10, random_state=42, n_jobs=-1)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb7ba1e5-9daf-47b7-93b4-e3f2f8766c9e",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e91599a4-a99a-461a-b25b-107c636a4b9f",
   "metadata": {},
   "source": [
    "Once the classifier is finished training, we can use it to make predictions on our test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c3ba9c-5197-442a-a3b0-1d58556b75f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the classifier\n",
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d52804fc-5536-4835-bcb7-bc17d2708989",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e98b15a-5c20-4484-8566-8384759ebf4c",
   "metadata": {},
   "source": [
    "It's important to know how well our classifier performs relative to the true labels (`y_test`). For this, we can calculate the [accuracy metric](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html) to measure agreement between the true and predicted labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a018a78-f76c-4622-a241-8628cbfde1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the performance (you can use metrics like accuracy, F1-score, etc.)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd33be5c-f705-49de-98bf-6a51de412ebe",
   "metadata": {},
   "source": [
    "We can also plot a confusion matrix to explore per-class performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941c49f1-118f-4771-9f8f-cc53185e9078",
   "metadata": {},
   "outputs": [],
   "source": [
    "ConfusionMatrixDisplay.from_predictions(y_true=y_test, y_pred=y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e012f8-9ef3-471e-87f1-ab420774e4df",
   "metadata": {},
   "source": [
    "Notice that we see a high variability in the performance across classes. This is likely due to a class imbalance or inter-class differentiation challenge within our training dataset. It's possible that augmentations or class revision may help to address this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba2bb751-296e-47b0-8f31-5a37fbd38a0d",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6c9d34-7821-45d7-a3d4-28f399c79a2e",
   "metadata": {},
   "source": [
    "If we want to generate predictions for the entire dataset in order to plot a map of predicted LULC for the entire area of interest, we can do this using the full (un-partitioned) features dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4c4fa6-ec17-4139-92a3-cd007e20ec4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_full = clf.predict(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30730c1-9b9a-448f-907c-cd9d790bb08c",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(y_pred_full)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba06a33-43f7-43f4-8d07-e04e6169fe00",
   "metadata": {},
   "source": [
    "Now, we will reshape the predictions back to the 2-dimensional shape and plot the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "085a783a-90f9-4b47-9539-3fbfc35e1241",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_map = y_pred_full.reshape((height, width))  # Reshape back to original dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e28447-dd4b-414f-ac7f-d3d345cc21ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an Xarray DataArray\n",
    "predicted_map_xr = xr.DataArray(data=predicted_map, coords=rasterized_labels.coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9545c20-62aa-4bec-bc36-3583cd61fd9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "rasterized_labels[\"lulc_2018_numeric\"].plot(ax=axes[0], cmap=\"viridis\")\n",
    "axes[0].set_title(\"Ground truth\")\n",
    "axes[0].set_aspect('equal')\n",
    "\n",
    "predicted_map_xr.plot(ax=axes[1], cmap=\"viridis\")\n",
    "axes[1].set_title(\"Predictions\")\n",
    "axes[1].set_aspect('equal')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0dc8aed-83d1-4b85-851b-07fd7bde0237",
   "metadata": {},
   "source": [
    "We can save the predicted LULC image to a GeoTIFF image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b26d60a-7ea5-4a0f-8919-9f47451914ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_map_xr.rio.to_raster(raster_path=\"predicted_lulc.tif\", driver=\"COG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5671ff96-0d8c-46d4-ae2c-107b1712fa86",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
