{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6e8d86e-397f-4127-adab-d585dae98e6e",
   "metadata": {},
   "source": [
    "# Intro to Selecting and Processing Raster Data\n",
    "\n",
    "This workflow introduces how to search for, acquire and process raster data efficiently. Specfically, we demonstrate finding [Sentinel-2](https://www.esa.int/Applications/Observing_the_Earth/Copernicus/Sentinel-2) satellite imagery relevant for an area and time defined by a subset of Vanuatu with the same temporal range as the VBoS-provided GIS data (2018). \n",
    "\n",
    "Note: some portions of this notebook are inspired by the [Introduction to Geospatial Raster and Vector Data with Python course](https://carpentries-incubator.github.io/geospatial-python/index.html).\n",
    "\n",
    "\n",
    "## Main Objectives\n",
    "\n",
    "- **Access Sentinel-2 data**: Locate and retrieve relevant Sentinel-2 data for a specific area and timeframe via STAC.\n",
    "- **Inspect and visualize raster data**: Examine metadata, including projections, bands, dimensions no data pixels. Plot raster data correctly.\n",
    "- **Process multi-spectral raster data**: Work with vector data to establish bounds, manage coordinate systems, and set up the area of interest (AOI).\n",
    "- **Interpret time-series raster data**: Learn to explore the time dimension for raster data and search for temporal patterns.\n",
    "- **Cloud masking and compositing**: Handle clouds and create composites from multiple image scenes.\n",
    "- **Speed up raster processing with Dask**: Learn how to optimize raster processing steps using a parallel processing library.\n",
    "\n",
    "\n",
    "At the end, you'll be able to efficiently process raster imagery for a region and time of interest!\n",
    "\n",
    "![Screenshot 2024-10-28 at 11.46.08 AM.png](https://github.com/user-attachments/assets/ecb8d053-1872-4a7f-914c-eb09d73e8488)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1426b4f7-3f69-40ca-aeba-dd4fe0b0855b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "import geopandas as gpd\n",
    "import odc.stac\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import numpy as np\n",
    "import pystac_client\n",
    "import rasterio\n",
    "import requests\n",
    "import rioxarray\n",
    "import time\n",
    "import xarray as xr\n",
    "from lonboard import viz\n",
    "from pyproj import CRS\n",
    "from rasterio.features import rasterize\n",
    "from shapely.geometry import box\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4054c9d-c6f0-45fd-b420-3b2a54df7506",
   "metadata": {},
   "source": [
    "## Accessing raster data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f4947c3-86d0-4dd3-a91b-a417376deee6",
   "metadata": {},
   "source": [
    "First let's start by defining a geometry for a subset of Vanuatu. We will use this to obtain geographic bounds to select and acquire raster data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177f8770-9342-4ef7-9c52-3181bccfbf5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "bbox_coords = [169.22028555, -19.65564097, 169.4692925 , -19.41891452]\n",
    "\n",
    "bounding_box = box(*bbox_coords)\n",
    "\n",
    "# Create a GeoDataFrame with the bounding box\n",
    "gdf = gpd.GeoDataFrame({'geometry': [bounding_box]}, crs=\"EPSG:4326\")\n",
    "\n",
    "print(gdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df27be48-7014-465e-a523-7418b7ca62b9",
   "metadata": {},
   "source": [
    "Note that we set the coordinate reference system (CRS) to the projection that is used to load web map tiles and store coordinate metadata in the STAC catalog, `EPSG:4326 (WGS 84)`. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d2887a-54a5-4a72-a0ba-3b22a423e96f",
   "metadata": {},
   "source": [
    "The subset is located in the following region of Vanuatu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc575a3b-c9da-4454-9d74-a0d83c00a1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "viz(gdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa1442d0-eebf-4fc2-b0a0-fdaec390d799",
   "metadata": {},
   "source": [
    "Now let's actually search for and retrieve some raster data. \n",
    "\n",
    "[STAC](https://stacspec.org/en), which stands for SpatioTemporal Asset Catalog, is an open standard specification designed to organize and describe geospatial assets in a consistent manner. It has become a crucial development in the geospatial industry as it establishes a standard language and structure for describing geospatial data. Ultimately, we use STAC for indexing earth observation data, along with associated metadata for efficient search and access.\n",
    "\n",
    "You can explore available datasets archived with this spec via the [STAC browser](https://radiantearth.github.io/stac-browser/#/?.language=en). Generally, this resource provides an up-to-date overview of existing STAC catalogs. \n",
    "\n",
    "Let's visit and select the [\"Earth Search\" catalog](https://radiantearth.github.io/stac-browser/#/external/earth-search.aws.element84.com/v1/?.language=en), which serves as the entry point for accessing the archive of Sentinel-2 images [hosted on AWS](https://registry.opendata.aws/sentinel-2-l2a-cogs/).\n",
    "\n",
    "To locate the API URL for a catalog in the STAC browser, click the \"Source\" button on the top-right corner. This URL grants access to the catalog's data. For example, in the Earth Search STAC catalog, we can see that the API URL is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438c2f58-dcb2-40d4-8a5b-710de636654c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access AWS STAC for Sentinel-2 Data\n",
    "aws_stac_url = \"https://earth-search.aws.element84.com/v1\"\n",
    "\n",
    "# You can query a STAC API endpoint from Python using the pystac_client library:\n",
    "stac_client = pystac_client.Client.open(aws_stac_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1634689-4512-45cf-af8e-e23b329d442b",
   "metadata": {},
   "source": [
    "In the following steps, we request scenes from the Sentinel-2 L2A collection, which contains Sentinel-2 data products pre-processed to Level 2A (bottom-of-atmosphere reflectance) and stored in Cloud Optimized GeoTIFF (COG) format.\n",
    "\n",
    "In the next steps, we will request scenes from the Sentinel-2 L2A collection. These Sentinel-2 data products have been processed to Level 2A, which correlates with bottom-of-atmosphere reflectance. Also worth noting, these image scenes are conveniently stored in a format optimized for cloud storage: [Cloud Optimized GeoTIFF (COG)](http://cogeo.org/).\n",
    "\n",
    "> Cloud Optimized GeoTIFFs (COGs) are a type of GeoTIFF that incorporate features making them particularly effective for cloud storage and online applications. COGs retain the standard GeoTIFF format but are specially structured to enhance remote access. A key aspect of this structure is the organization of data into \"blocks,\" allowing specific parts of the file to be accessed through HTTP requests, so users don’t have to download the entire file. Additionally, COGs often include \"overviews\"—multiple lower-resolution versions of the image—enabling users to quickly retrieve less detailed versions if high resolution is unnecessary, which greatly reduces data transfer times.\n",
    "\n",
    "We specifically search for all available Sentinel-2 scenes in the `sentinel-2-l2a` collection that meet the following conditions:\n",
    "- intersect with a given bounding box (based on the boundaries of the area of interest);\n",
    "- were captured between January 1, 2018, and December 31, 2018;\n",
    "- have less than 5% cloud cover;\n",
    "- have less than 25% invalid pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4096dc19-c35c-4625-9ea6-6e2bdfad2032",
   "metadata": {},
   "outputs": [],
   "source": [
    "s2_search = stac_client.search(\n",
    "    collections=[\"sentinel-2-l2a\"], # Sentinel-2, Level 2A, Cloud Optimized GeoTiffs (COGs)\n",
    "    bbox=list(bbox_coords),\n",
    "    datetime=\"2018-01-01/2018-12-31\",\n",
    "    query={\"eo:cloud_cover\": {\"lt\": 5}, \"s2:nodata_pixel_percentage\": {\"lt\": 25}}, \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e5e82a2-4c2f-4d91-8448-4c5de9710919",
   "metadata": {},
   "source": [
    "Note: At this stage, we've only retrieved metadata, meaning no images have been loaded into memory yet. Keep in mind, though, that even metadata can amount to a lot of memory when there are many images that fit the search criteria! To manage this, it’s possible to set a cap on the number of search results by providing another parameter `limit=n` where `n` is the maximum allowed number of items to be returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874e68c0-6e51-417b-99bd-0b219b5ff8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve all items (still just metadata) from search results\n",
    "s2_items = s2_search.item_collection()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f941727-b303-484c-afe4-94926ffed785",
   "metadata": {},
   "source": [
    "Now that we've executed the query to find image scenes meeting our search criteria, we can determine how many that is with the following `.matched()` method. Keep in mind that this result may vary as more data is continually added to the respective catalog):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed92fc4-4ffc-4201-bfa4-12437ea789ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(s2_search.matched())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c06cb0-ff73-4619-9152-b49169d7d8c8",
   "metadata": {},
   "source": [
    "Which should equate to the number of items collected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1dd6bfd-4072-414c-909c-00c89ce72ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(s2_items)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed4f408-7d24-41a7-abb6-95d05a6fb2bf",
   "metadata": {},
   "source": [
    "Each item correlates with one image scene for which the metadata contains attributes such as the scene's geometry and image capture time, to name a few. These attributes can be accessed through the item's properties.\n",
    "\n",
    "\n",
    "We can see an example for the first item in the search results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1612422-c1b0-4634-b9c8-f29817c6dee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "item = s2_items[0]\n",
    "print(item.datetime)\n",
    "print(item.geometry)\n",
    "print(item.properties)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "243564fb-be39-4d32-b5d0-5915949ee63f",
   "metadata": {},
   "source": [
    "As previously mentioned, we have only retrieved metadata. Now, we will access the actual rasters (image pixels) for the scenes. In STAC terminology, the image data itself is referred to as “assets”, and for any given image, there is one asset per band. One simple method for reading the image data is through the URLs accessible in the item's assets attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1c1eea-5677-4a44-b187-d2a9b23bb953",
   "metadata": {},
   "outputs": [],
   "source": [
    "assets = s2_items[0].assets  # first item's asset dictionary\n",
    "print(assets.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbda70f8-653f-469e-b71a-c98cc28bf295",
   "metadata": {},
   "source": [
    "We can see a description of the available assets for the respective sensor/instrument like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb61477-d5bd-403d-a4e8-2b7c447f6a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, asset in assets.items():\n",
    "    print(f\"{key}: {asset.title}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf3608f-1531-4747-a3eb-befba7864fd2",
   "metadata": {},
   "source": [
    "The Sentinel-2 L2A data product includes several assets: raster files representing each optical band captured by the multispectral instrument, a thumbnail true-color image (\"visual\") image, as well as metadata from the instrument and scene classification data (\"SCL\"). Let's gather the URLs for these assets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c0b601-5d8c-4add-a35a-132a53761719",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(assets[\"thumbnail\"].href)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d5df0f-9c5b-4211-89b0-18e872b49d98",
   "metadata": {},
   "source": [
    "That URL represents the location of the image in cloud storage. As it is remote raster data, we will use a library that allows us to directly access it without having to download the image to our file system first.  This library is called `rioxarray`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fdc8aa3-406f-408a-9c94-51d8107c8fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "nir_href = assets[\"nir\"].href\n",
    "nir = rioxarray.open_rasterio(nir_href)\n",
    "print(nir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd3462e-ec46-418e-9643-048592ecb1b3",
   "metadata": {},
   "source": [
    "Optionally, we can then save the raster data to a file on disk if needed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935748f0-7338-4aba-a9e9-a64b9632e38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save whole image to disk\n",
    "#nir.rio.to_raster(\"nir.tif\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd9a818-7cae-4bc7-9a9a-60407d3eada8",
   "metadata": {},
   "source": [
    "Since processing large rasters can be time-consuming (for example, the 10-meter NIR band has over 100 million pixels), it's often more efficient to work with a smaller subset of the data. Fortunately, because the raster is in Cloud Optimized GeoTIFF (COG) format, we can download just the portion we need! Let's first check the internal tile size for this band of the COG:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b4a164-b9ef-453c-8beb-f93a926d3c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nir.shape)\n",
    "tile_size = nir.rio._manager.acquire().block_shapes\n",
    "tile_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af56d4b8-601c-44eb-b8c0-2818cf207cfa",
   "metadata": {},
   "source": [
    "In this case, we specify that we want to download the first band in the TIFF file and extract a subset by slicing the width and height dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace65da1-658e-4d16-b7c5-62529f456321",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save portion of an image to disk\n",
    "nir[0,1024:2048,1024:2048].rio.to_raster(\"nir_subset.tif\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e6423fb-9669-4db3-9747-75fbf578faf8",
   "metadata": {},
   "source": [
    "We can read the newly saved image subset and confirm the size is what we expect:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a2c420-3393-4e9d-bfba-1bee3df782c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "nir = rioxarray.open_rasterio(\"nir_subset.tif\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a454f2db-8859-4558-a406-f958cf6bfa97",
   "metadata": {},
   "outputs": [],
   "source": [
    "nir.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55194b5a-9aac-421e-8974-1646b70830ba",
   "metadata": {},
   "source": [
    "## Reading raster data\n",
    "\n",
    "In this section, we cover the core principles, tools, and metadata/raster attributes necessary for handling raster data in Python. We will also explore common ways of managing missing or invalid data values.\n",
    "\n",
    "The `rioxarray` library will be our primary tool for working with raster data in this lesson. It builds upon the functionality of `rasterio` (a package for working with raster data) and `xarray` (for multi-dimensional arrays). `rioxarray`, simply put, extends `xarray` by adding higher-level functions, such as `open_rasterio` for reading raster datasets, and provides additional methods to `xarray` objects like `Dataset` and `DataArray` that specifically enable geospatial operations. These methods, available through the `rio` accessor, become accessible in `xarray` once `rioxarray` is imported.\n",
    "\n",
    "For demonstration, we'll focus on the first scene retrieved and load the `nir09` band. This can be done by using the `rioxarray.open_rasterio()` function with the band asset's Hypertext Reference (`href`, or URL)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a83d32-5bbf-4246-93d6-eb10f91715e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "raster_vanu_b9 = rioxarray.open_rasterio(s2_items[0].assets[\"nir09\"].href)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b002d3d-5ab4-4dd9-af3e-778d4610c885",
   "metadata": {},
   "source": [
    "We can quickly inspect the shape and attributes of the newly opened `nir09` dataset we titled `raster_vanu_b9` by printing the variable name. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6190f779-3d99-4f14-baf3-5144d7d004fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "raster_vanu_b9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a0238f2-077e-4254-b8e3-dd65060c506d",
   "metadata": {},
   "source": [
    "The initial call to `rioxarray.open_rasterio()` retrieves the file from either local or remote storage, returning a `xarray.DataArray`. As noted, this object is assigned to a variable like `raster_vanu_b9`. While using `xarray` also returns a `xarray.DataArray`, it won’t include the geospatial metadata (such as scene geometry and projection). You can apply `NumPy` functions or Python’s math operators to a `xarray.DataArray` just as you would with a `NumPy` array, but without `rioxarray` you can't make use of any geospatial information.\n",
    "\n",
    "The output shows the variable name of the `xarray.DataArray`, and indicates that the data includes 1 band, 1830 rows, and 1830 columns. It also shows the total pixel count and the pixel data type, which is an unsigned integer (`uint16`). \n",
    "\n",
    "\n",
    "In addition, thanks to `rioxarray`, we can see that the `DataArray` has spatial coordinates (x and y) and band information, with each having its own data type—`float64` for spatial coordinates and `int64` for bands. Furthermore, `rioxarray` enables us to see other important geospatial attributes via methods like `.rio.crs` and `.rio.bounds()`. Note: most metadata can be accessed directly as attributes (e.g., `.rio.crs`), but some methods like `.rio.bounds()` require parentheses to retrieve the information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6ed3d0-eefe-4ce4-bcba-62e0d141a3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(raster_vanu_b9.rio.crs)\n",
    "print(raster_vanu_b9.rio.nodata)\n",
    "print(raster_vanu_b9.rio.bounds())\n",
    "print(raster_vanu_b9.rio.width)\n",
    "print(raster_vanu_b9.rio.height)\n",
    "print(raster_vanu_b9.rio._manager.acquire().block_shapes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42907cee-99a4-44b5-9277-9489fa707861",
   "metadata": {},
   "source": [
    "The Coordinate Reference System for `raster_vanu_b9.rio.crs` is returned to be `EPSG:32759`. The no-data value is set to `0`, and the bounding box corners of our raster are returned by the output of `.bounds()`. The height and width in number of pixels are returned from `.rio.height` and `.rio.width`, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da504b6-f619-4902-8f77-082830b9e22c",
   "metadata": {},
   "source": [
    "## Visualizing raster data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3affe57a-3702-4089-97be-049369019e74",
   "metadata": {},
   "source": [
    "We've reviewed the attributes of our raster. Now let's see the raw values of the array with `.values`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4643aaf0-d59c-44af-9ec9-f4bd9faac418",
   "metadata": {},
   "outputs": [],
   "source": [
    "raster_vanu_b9.values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee02e73-e1ef-4f1b-8e18-adff45b9e55b",
   "metadata": {},
   "source": [
    "Printing this provides a quick glimpse of the values in our array (by way of showing pixels just on image corners). Since our raster is loaded in Python as a `DataArray` type, we can also plot it in a single line, similar to how we would with a `pandas` `DataFrame`, using `DataArray.plot()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd74d1ee-93a8-47a1-92e4-cec9b2fab46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "raster_vanu_b9.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33acadd5-be55-4ff8-ad80-417c3c895856",
   "metadata": {},
   "source": [
    "Observe that `rioxarray` conveniently enables us to plot this raster with spatial coordinates on the x and y axes.\n",
    "\n",
    "The plot displays the satellite image pixels for the spectral band `nir09` over our area of interest. According to the Sentinel-2 documentation, this band has a central wavelength of 945 nm, making it sensitive to water vapor. This band is one of the lower spatial resolution wavelengths captured by the instrument at a spatial resolution of 60 m. This makes it convenient here for quick demonstrations. \n",
    "\n",
    "It's important to note that the `band=1` in the image title refers to the order of the bands in the `DataArray`, not the Sentinel-2 band asset `nir09`.\n",
    "\n",
    "From a quick glance at the image, we can see that cloudy pixels exhibit high reflectance values, whereas the contrast of other areas is relatively low. This behavior is expected due to the band's sensitivity to water vapor. However, we can improve color contrast by including the option `robust=True`, which displays values between the 2nd and 98th percentiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd53cfa8-50d5-46c0-9454-a3a973e8f762",
   "metadata": {},
   "outputs": [],
   "source": [
    "raster_vanu_b9.plot(robust=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea68def5-b80f-4178-89e8-d4b6dd77ea92",
   "metadata": {},
   "source": [
    "This feature generally allows us to adjust the color limits to better accommodate most of the values in the image.\n",
    "\n",
    "However, it isn't always sufficient for getting a good representation of the data. In situations where the `robust=True` option doesn't work, you can also manually set the `vmin` and `vmax` parameters to customize the value range. For instance, you can plot values between 100 and 7000:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d79f95b-6ea8-4e0c-a846-e924c4ad1f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "raster_vanu_b9.plot(vmin=100, vmax=7000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77bb4f0c-f452-4563-9b79-649f0893363b",
   "metadata": {},
   "source": [
    "Now, if we want to plot a subset of this image more focused on the land masses, we can select a regin of pixels again. Let's look at the internal tiling for this band of the COG:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad503f7-75df-4eda-8f03-2f3a7833aca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(raster_vanu_b9.shape)\n",
    "tile_size = raster_vanu_b9.rio._manager.acquire().block_shapes\n",
    "tile_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998ec102-58c9-43e7-83ca-a08ec1f3fad8",
   "metadata": {},
   "source": [
    "Notice that the tile size is smaller because the resolution for this band (`nir09`) is lower (60 meters) than that of the `nir` (10 meters). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee815a2a-a39c-4417-b146-8580c89bf95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the center coordinates of the image\n",
    "center_x, center_y = raster_vanu_b9.sizes[\"x\"] // 2, raster_vanu_b9.sizes[\"y\"] // 2\n",
    "\n",
    "# Select a crop region using .isel()\n",
    "raster_vanu_b9_subset = raster_vanu_b9.isel(\n",
    "    x=slice(center_x - tile_size[0][0]*3, center_x + tile_size[0][0]),\n",
    "    y=slice(center_y - tile_size[0][0]*2, center_y + tile_size[0][0]*2)\n",
    ")\n",
    "\n",
    "print(raster_vanu_b9_subset.shape)\n",
    "\n",
    "raster_vanu_b9_subset.plot(robust=True)\n",
    "plt.title(\"Crop of Sentinel-2 COG (NIR)\")\n",
    "plt.xlabel(\"X Pixel\")\n",
    "plt.ylabel(\"Y Pixel\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2441cb95-36ef-4c7c-979f-ab1531337219",
   "metadata": {},
   "source": [
    "## Deciphering the Raster Coordinate Reference System (CRS)\n",
    "\n",
    "Another important piece of information we want to examine is the Coordinate Reference System (CRS), which can be accessed using `.rio.crs`. We will explore how the characteristics of the CRS are represented in our data file and their significance. To view the CRS string associated with our `DataArray`, we can utilize the `rio` accessor to obtain the `crs` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c3d43d-3a00-4cc9-a086-5774a2f99b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(raster_vanu_b9.rio.crs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1595a703-351a-43ff-9a58-489e83d24c78",
   "metadata": {},
   "source": [
    "EPSG codes offer a concise way to represent specific coordinate reference systems (CRS). However, for more comprehensive details about a CRS, such as its units of measurement, the `pyproj` library can be used. This library is specifically designed to handle the definition and function of coordinate reference systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d8f553-e990-498b-bb19-7d726edba3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "epsg = raster_vanu_b9.rio.crs.to_epsg()\n",
    "crs = CRS(epsg)\n",
    "crs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb889a7b-a52f-4d27-82b3-a8090a885c0d",
   "metadata": {},
   "source": [
    "The ``CRS`` class from the ``pyproj`` library enables us to create a CRS object. We can retrieve specific information about a CRS from this, as well as obtain a basic summarization of the associated CRS.\n",
    "\n",
    "One especially valuable attribute is ``area_of_use``, which indicates the geographic boundaries for which the CRS is designed to be applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da3dfcc-3a3c-40dd-b4d9-b0738b60c7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "crs.area_of_use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c02135-495d-4910-bfb3-51eb00157fb3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# More on the various attributes accessible for the CRS class can be viewed with the following:\n",
    "# help(crs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf88d92-437d-4b24-a205-f1189b059170",
   "metadata": {},
   "source": [
    "The ``pyproj`` CRS summary encompasses all the individual elements of the CRS that Python or other GIS software may require.\n",
    "\n",
    "For our example, the projection name is ``UTM zone 59S`` (the UTM system comprises 60 zones, each spanning 6 degrees of longitude) with an underlying datum of ``WGS84``. The CRS utilizes a Cartesian coordinate system with two axes, ``easting`` and ``northing``, measured in meters. This projection is applicable for a specific range of longitudes, from ``168°E`` to ``174°E``, in the southern hemisphere (from ``80.0°S`` to ``0.0°S``). The coordinate operation describes how the coordinates are projected (if applicable) onto a Cartesian (``x``, ``y``) plane. The ``Transverse Mercator`` projection is effective for regions with narrow longitudinal widths, as is the case with UTM zones. The datum serves as the reference point for coordinates. ``WGS 84`` is a widely used datum. It's important to note that the zone is specific to the UTM projection, and not all CRSs will have a designated zone."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae5d177-780e-4d64-ab12-1758988fabec",
   "metadata": {},
   "source": [
    "## Calculate Raster Statistics\n",
    "Knowing basic statistical values of a raster dataset can be valuable. We can obtain some, such as minimum, maximum, mean, and standard deviation quite easily for `xarray.DataArray`s like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be3b251-6e29-4f4f-8e87-41356af8d4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(raster_vanu_b9.min())\n",
    "print(raster_vanu_b9.max())\n",
    "print(raster_vanu_b9.mean())\n",
    "print(raster_vanu_b9.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "578b7706-0ba4-44e7-a95a-29e7598f9514",
   "metadata": {},
   "source": [
    "So with that, we can get key statistical values such as the minimum, maximum, mean, and standard deviation, along with the data type of the pixels. However, if you're interested in calculating specific quantiles, `xarray` provides the `.quantile()` method. For example, to determine the 25th and 75th percentiles, you can try the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a937fc-897f-4dc9-9834-3dcdcf36bfb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(raster_vanu_b9.quantile([0.25, 0.75]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd5d9705-eb65-4d86-9176-bd527791078f",
   "metadata": {},
   "source": [
    "## Dealing with Missing Data\n",
    "\n",
    "Up until now, we've visualized a band from a Sentinel-2 scene and computed its statistics. However, it's essential to factor in missing data. Raster datasets typically include a \"no data value\" or \"nodata.\" This value is used for pixels where data is absent, which may occur due to various reasons, such as sensor limitations or gaps in data collection. Often, missing data appears at the raster edges, especially when the data does not cover the entire area being analyzed.\n",
    "\n",
    "By design, raster datasets have a rectangular shape. Therefore, if the data doesn’t fully cover a region, the pixels at the boundary might be marked as nodata. For instance, sensor data might only cover a portion of a given area, leaving edges with no data.\n",
    "\n",
    "In this case, the nodata value for the dataset (`raster_vanu_b9.rio.nodata`) is 0. When we visualized the data or calculated statistics, these missing values were treated the same as valid data. This could result in misleading conclusions—such as a falsely low 25th percentile—because the nodata pixels, which are represented by zeros, can distort the calculation.\n",
    "\n",
    "To address this and more clearly distinguish missing data, we can use `nan` to represent these values. This can be done by setting `masked=True` when loading the raster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc1f709-f20a-401e-a385-813012b6b11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "raster_vanu_b9 = rioxarray.open_rasterio(s2_items[0].assets[\"nir09\"].href, masked=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59901deb-815b-4d30-800e-77750c143d36",
   "metadata": {},
   "source": [
    "You can also utilize the `where` function to filter out all the pixels that differ from the raster's `nodata` value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c4aa33-2175-4f8f-805f-9e3d509e89d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "raster_vanu_b9.where(raster_vanu_b9!=raster_vanu_b9.rio.nodata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a59735-80ef-48e9-aa4b-e61a6ba9e0ac",
   "metadata": {},
   "source": [
    "Both methods will convert the `nodata` value from 0 to `nan` and thus allow us to recalculate the statistics with the missing data excluded. Additionally, you can use the `.values` attribute with the statistical functions to obtain only the calculated values, without any of the object metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd43995-0ef4-4d1b-9156-d3c7b0613072",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(raster_vanu_b9.min().values)\n",
    "print(raster_vanu_b9.max().values)\n",
    "print(raster_vanu_b9.mean().values)\n",
    "print(raster_vanu_b9.std().values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd37eb14-61b3-41f9-a032-d5036b0a9ae8",
   "metadata": {},
   "source": [
    "It's worth mentioning that replacing `0` with `nan` to represent missing data will cause a change in the data type of the `DataArray` from integers to floats. This is an important consideration if the data type plays a crucial role in your specific use case or application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35426286-7ae8-4d71-8d11-d50d84572332",
   "metadata": {},
   "source": [
    "## Incorporating multiple bands \n",
    "\n",
    "Up to this point, we have examined a single-band raster, specifically the `nir` and subsequently `nir09` bands of a Sentinel-2 scene. However, if we want to see an easy-to-interpret RGB \"true-color\" version of the scene, we can look at the overview (TCI) asset. The Sentinel-2 True Color Image (TCI) is a full-resolution visual representation of the scene. This image is created by combining three specific optical bands—Red, Green, and Blue (RGB)—captured by the Sentinel-2 satellite’s MultiSpectral Instrument (MSI). With a spatial resolution of 10 meters, the TCI provides a detailed, visually intuitive view of the scene's captured area, assisting with quick assessments and visual analysis. Like the `nir09` band, we can load it using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6632bf17-2ca5-4b05-93c0-cb3e50b66d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "raster_vanu_overview = rioxarray.open_rasterio(s2_items[0].assets['visual'].href, overview_level=3)\n",
    "raster_vanu_overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0061d55-3d89-4e5c-9554-82bea618b3a2",
   "metadata": {},
   "source": [
    "Note that we provided an argument `overview_level=3`. For Sentinel-2 COGs, the spatial resolution at different overview levels depends on how much the image is downsampled. Sentinel-2 data products have a native spatial resolution of 10 meters for True Color Image (TCI) bands, but each overview level decreases this resolution by a factor of 2.\n",
    "\n",
    "If you read the TCI at overview level 3, the spatial resolution is effectively:\n",
    "- Level -1 (native): 10 meters\n",
    "- Level 0: 20 meters\n",
    "- Level 1: 40 meters\n",
    "- Level 2: 80 meters\n",
    "- Level 3: 160 meters\n",
    "\n",
    "When reading GeoTIFFs using the `open_rasterio()` function and printing the shape, the band number appears first. In the `xarray.DataArray` object, we can observe that the shape is now `(band: 3, y: 687, x: 687)`. From this, we can easily see the number of bands in the named band dimension. It's always advisable to check the shape of the raster array you are working with to make sure it meets your expectations, both after reading the image into an array and intermittently as you undergo manipulations of the dataset (e.g. calculating band ratios or warping). Many functions, particularly those used for plotting images, require the raster array to have a specific shape (e.g. 1 or 3 channels ordered a certain way). A very simple way to verify the shape is using the `.shape` attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f852c603-ef5f-4441-8801-732fd5410bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "raster_vanu_overview.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f3c419-674b-4a47-b4ce-beaff218a164",
   "metadata": {},
   "source": [
    "You can visualize the multi-band data using the `DataArray.plot.imshow()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f011c16b-9d01-45f5-8b27-9f54acd1f250",
   "metadata": {},
   "outputs": [],
   "source": [
    "raster_vanu_overview.plot.imshow()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc0d1df-c7ec-4d97-85d3-b988097020b8",
   "metadata": {},
   "source": [
    "Keep in mind that the `DataArray.plot.imshow()` function assumes the input `DataArray` has three channels, which corresponds to the RGB colormap. It is not compatible with image arrays that contain more than three channels. However, you can create a false-color image by substituting RGB channels with others.\n",
    "\n",
    "As illustrated in the figure above, the true-color image appears stretched. To visualize it with the correct aspect ratio, we can adjust the settings for `DataArray.plot.imshow()`.\n",
    "\n",
    "Given that the height-to-width ratio is 1:1 (as confirmed by the `rio.height` and `rio.width` attributes), let's set the aspect ratio to 1. For instance, this would be a good example to set the size of the plot to 5 inches and specify `aspect=1`. Note that when using the `aspect` argument, you must also provide a size, as stated in the `DataArray.plot.imshow()` documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b46e87-8215-41fc-9417-3e0b5501ee16",
   "metadata": {},
   "outputs": [],
   "source": [
    "raster_vanu_overview.plot.imshow(size=5, aspect=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef21ba30-6332-4a46-8230-ea0ca6c7cd91",
   "metadata": {},
   "source": [
    "Now let's say we want to work with not just the visual bands, but also some of the others, such as the `nir` band. We'll introduce a new tool and method for loading raster data from STAC into an `xarray.Dataset` called [Open Data Cube (ODC)](https://www.opendatacube.org/) which we imported as `odc`. \n",
    "\n",
    "`odc` features an extension for working with raster data from STAC-compliant catalogues. We will use this to more robustly work with the Sentinel-2 data that we queried for earlier.\n",
    "\n",
    "In the following cell, we do the following:\n",
    "\n",
    "1. **`s2_data = odc.stac.load(...)`**:\n",
    "   - This line uses the `odc` library to load data from the Sentinel-2 items we collected metadata for. The `load` function retrieves the actual raster data.\n",
    "\n",
    "2. **`items=s2_items`**:\n",
    "   - The `s2_items` refers to the Sentinel-2 items (scenes) we retrieved earlier from the STAC catalog search. These items contain the metadata and location of the relevant Sentinel-2 data, which will be loaded in this step.\n",
    "\n",
    "3. **`bands=[\"red\", \"green\", \"blue\", \"nir\", \"scl\"]`**:\n",
    "   - This specifies the spectral bands to load: Red, Green, Blue, and Near-Infrared (NIR). These are common bands used for analyzing land cover, vegetation health, and generating RGB images. We also include a band that isn't a necessary a wavelength range, but rather a classification layer (SCL) which we will use to know where cloudy pixels are.\n",
    "\n",
    "4. **`bbox=box_coords`**:\n",
    "   - The `bbox` argument defines the **Area of Interest (AOI)**, which is the geographic bounding box of the region we want to load data for (same one as used earlier for the subset of Vanuatu). The AOI is given as a list of coordinates that define the region's extent: `[min_longitude, min_latitude, max_longitude, max_latitude]`. \n",
    "\n",
    "5. **`progress=tqdm`**:\n",
    "   - The `progress` argument links the loading process to `tqdm`, which provides a progress bar. This is useful when loading large datasets, as it shows how much data has been loaded and gives an indication of how long the process might take.\n",
    "\n",
    "\n",
    "**IMPORTANT:** \n",
    "When provided a `bbox` argument, `odc` will automatically clip the generated raster to the bounds of the provided extent. This means we only return the pixels that we are actually interested in!\n",
    "\n",
    "The result, `s2_data`, is a `xarray.Dataset` that we can use for further analysis, such as visualization or classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c727769a-efe2-4c86-98af-0de8657472c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "s2_data = odc.stac.load(\n",
    "    items=s2_items,\n",
    "    bands=[\"red\", \"green\", \"blue\", \"nir\", \"scl\"],\n",
    "    bbox=bbox_coords,\n",
    "    progress=tqdm,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77655535-b292-4c93-8607-f2228f046038",
   "metadata": {},
   "source": [
    "The `odc.stac.load()` function returns an `xarray.Dataset` instead of an `xarray.DataArray` because we are loading more than one spectral band (`[\"red\", \"green\", \"blue\", \"nir\", \"scl\"]`). A `xarray.Dataset` is designed to hold a single variable (or band) of data. Since we are loading multiple bands, the function returns a `xarray.Dataset`, which is essentially a collection of `DataArray`s, where each `DataArray` corresponds to one of the requested bands. A `xarray.Dataset` can be thought of as a container for multiple `DataArray`s. It can hold multiple variables (e.g., bands like `red`, `green`, `blue`, `nir`), each with the same or different dimensions and coordinates. In this case, each band (e.g., `red`, `green`, etc.) is a separate `DataArray` within the `Dataset`.\n",
    "\n",
    "The `xarray.Dataset` structure allows you to work with multiple related variables (e.g., bands) in the same space, with shared coordinates (such as `latitude`, `longitude`, and `time`). This makes it easier to perform operations that involve multiple bands, such as stacking them together to create a RGB image or performing multi-band analysis.\n",
    "\n",
    "If you were loading only a single band, the `odc.stac.load()` function could return a `DataArray`, which is more suited to holding a single variable with associated dimensions and coordinates. However, because we requested multiple bands, a `xarray.Dataset` is the appropriate return type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4815bb-c18a-43cb-8d5b-55e3765010d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "s2_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7380a5c-47cf-4bed-9829-09577cca4a8a",
   "metadata": {},
   "source": [
    "Notice that the time dimension is automatically interpreted and correctly lines up with the 7 items we returned from `s2_search.matched()` and `len(s2_items)` earlier. This means we have 7 items overlapping the AOI from different image captures with different timestamps. `odc` doesn't assume we want to do any manipulation by default on the time dimension so it returns the dataset with the items stacked in a time series.\n",
    "\n",
    "In order to plot this image, let's composite the image over time, to produce a temporal depth of 1 where pixels represent an average of all of the timestamps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7c12c7-135f-47b5-8a7c-45ab26f86721",
   "metadata": {},
   "outputs": [],
   "source": [
    "s2_data_composite = s2_data.mean(dim='time')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386629f5-c5b8-4440-850c-0cd23f913536",
   "metadata": {},
   "source": [
    "Notice the new shape of the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52f43eb-7c72-43b3-9d56-b4a1187d4b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "s2_data_composite"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0e1311-a503-4a05-b152-d02ce4880500",
   "metadata": {},
   "source": [
    "Also notice here that the aspect ratio is not perfectly 1:1, but close enough (1.003:1) for use with a 1:1 aspect ratio when plotting. We can plot a true color image from the dataset by specifying the necessary bands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec48e22-d9ef-40dc-b707-377ce5fd98b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "s2_data_composite[[\"red\", \"green\", \"blue\"]].to_array(\"band\").plot.imshow(rgb=\"band\", robust=True, size=5, aspect=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0102314c-ce43-47db-9975-6f749d9e59c8",
   "metadata": {},
   "source": [
    "We can also plot any single band from the dataset like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9890ed-4130-4fbe-8436-1b5778d76d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "s2_data_composite[[\"nir\"]].to_array(\"band\").plot(robust=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "098a347e-587d-4b88-adbe-67795eb1a6a3",
   "metadata": {},
   "source": [
    "Let's say we want to take a look at the time series for a single band. We can do this by taking the average in the spatial dimensions (x and y) for each timestamp and plotting the values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ed1cbe-c7e9-4bcd-b0a1-ac29d2d169cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "s2_data_mean_time_series = s2_data.mean(dim=[\"y\", \"x\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb7d76f-d2cc-47dd-b652-d02acd72c783",
   "metadata": {},
   "source": [
    "Then we can extract the time series for one of bands. Near infrared is interesting because it is useful in determing seasonal phenology."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1d0687-6f3c-4364-9b14-4da1d895bb2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "nir_time_series = s2_data_mean_time_series[\"nir\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5774fb4-daf7-44f1-9c42-66cb545fbf42",
   "metadata": {},
   "source": [
    "We can plot a line plot for the near infrared averages for each timestamp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4fe5da1-7b7b-420f-be7a-37bcb8b6d482",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "nir_time_series.plot(label='Near Infrared Band', marker='o')\n",
    "\n",
    "plt.title(\"Time Series of Mean Near Infrared Band Values\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Mean Value\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c040a8f3-5d65-49c4-ac1f-e8cbe188d507",
   "metadata": {},
   "source": [
    "## Dealing with cloudy pixels\n",
    "\n",
    "To mask out clouds from Sentinel-2 data, we can use the 'SCL' (Scene Classification Layer) band, which includes cloud information. The SCL band classifies each pixel in the Sentinel-2 image, including cloud-related classes such as cloud shadows, medium-probability clouds, high-probability clouds, and cirrus clouds. By masking out these classes, we can remove cloud-covered pixels from the dataset.\n",
    "\n",
    "First we need to declare the cloud-related SCL classes. The common classes for clouds are:\n",
    "\n",
    "> Cloud Shadows: 3\n",
    ">\n",
    "> Low probability clouds: 7\n",
    ">\n",
    "> Medium probability clouds: 8\n",
    "> \n",
    "> High probability clouds: 9\n",
    "> \n",
    "> Thin cirrus: 10\n",
    "\n",
    "In order to mask out these cloud classes, we'll create a boolean mask using `xarray`'s `.isin()` method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eadfbb20-ef51-40e0-94bc-40ff154a0f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "cloud_classes = [3, 7, 8, 9, 10]  # Cloud-related SCL classes\n",
    "cloud_mask = s2_data['scl'].isin(cloud_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07187217-f24d-4a3b-9b96-13ecd4bcff8f",
   "metadata": {},
   "source": [
    "Now, we will use the cloud mask to filter out cloud-covered pixels in the red, green, blue, and NIR bands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d020dee6-337f-4856-a976-1c4e5b65b80d",
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_s2_data = s2_data[['red', 'green', 'blue', 'nir']].where(~cloud_mask, drop=False)  # Keep all pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "034b578a-574a-4190-9979-cffd1b8ddec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_s2_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6dbf87-e9d1-4094-bdcc-587eb0d1fdac",
   "metadata": {},
   "source": [
    "Let's plot the cloud-free data to verify the results. To do this, let's composite the imagery again to get a temporal average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411cf87f-333f-4572-8f40-5724ec7d99c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_s2_data_composite = masked_s2_data.mean(dim='time')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04965202-98a9-4cf5-a40e-81fffe6f24ef",
   "metadata": {},
   "source": [
    "Now we can plot an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "274ea664-9576-44f9-81fe-30c898e950b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_s2_data_composite[['red', 'green', 'blue']].to_array(\"band\").plot.imshow(rgb=\"band\", robust=True, size=5, aspect=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc85ee2-25bc-4345-8c7c-9b420c45b920",
   "metadata": {},
   "source": [
    "The image plot is looking a bit over-exposed. Let's adjust the value ranges to better fit the raster data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871781f5-9084-4bc7-afd7-5f49a636ee33",
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_s2_data_composite[['red', 'green', 'blue']].to_array(\"band\").plot.imshow(rgb=\"band\", robust=True, size=5, aspect=1, vmin=0, vmax=2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881fe9fe-91c5-4423-ad54-7ae31fa6586e",
   "metadata": {},
   "source": [
    "That looks pretty good! Now we have a cloud-masked composite. We can use this to get a clearer signal of near infrared over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "078472b2-1032-4eed-926c-e055a64e39c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_s2_data_mean_time_series = masked_s2_data.mean(dim=[\"y\", \"x\"])\n",
    "masked_nir_time_series = masked_s2_data_mean_time_series[\"nir\"]\n",
    "plt.figure(figsize=(10, 6))\n",
    "masked_nir_time_series.plot(label='Near Infrared Band', marker='o')\n",
    "\n",
    "plt.title(\"Time Series of Mean Near Infrared Band Values\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Mean Value\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e7f956-5587-4b09-8568-f9fe245d7962",
   "metadata": {},
   "source": [
    "What if we want to interpolate where the cloudy pixels are for each timestamp, however, instead of reducing to a single temporal composite? We can do this using the cloud mask and an interpolation (e.g. linear, nearest neighbor, spline) method on either the spatial (x and y) dimension or time. Combinations of these will imply different overhead and thus varying run times.\n",
    "\n",
    "For this example, we will interpolate on the time dimension. For the following, we need to get the cloud mask, this time actually changing the values for cloudy pixels to `NaN`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e12615d-00f0-4ea8-aabc-4874eff22add",
   "metadata": {},
   "outputs": [],
   "source": [
    "cloudy_pixels = s2_data[['red', 'green', 'blue', 'nir']].where(~cloud_mask, other=np.nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57df3561-ace7-414e-98ab-b5ab2ed40cb9",
   "metadata": {},
   "source": [
    "We will then introduce a new library, `dask`, which is used to parallelize expensive operations by chunking data into smaller parts and distributing those across available workers. Here, we select a chunk size based on the shape of the `DataArray`. The chunk size is a factor that is specific to the size of your data and should closely match any internal tiling paramaters if relevant (as is the case for COGs).  \n",
    "\n",
    "We will also apply downsampling to further reduce the computation time. In practice, you might not want to downsample your data if spatial resolution is very important, but here we do so for the purposes of a quick example. Note that in the downsampling step `coarsen()` we provide `x=4, y=4` which specifies the size of the neighborhood over which to average. In this case, it will average every 4 pixels along the x-axis and every 4 pixels along the y-axis. We also include `boundary='trim'` which determines how the edges of the DataArray are handled. With `boundary='trim'`, any excess pixels that do not fit into the averaging windows are removed from the result. For example, if the dimensions of `cloudy_pixels` are not exact multiples of 4, the excess pixels at the edges will be discarded. Lastly, `.mean()` is the coarsening operation. Calling `.mean()` computes the mean value of each coarse block defined by the `coarsen()` method. This results in a new `DataArray` with a lower spatial resolution, where each pixel represents the average value of a 4x4 block of pixels from the original `cloudy_pixels`.\n",
    "\n",
    "On the interpolation algorithm, we use one of the faster options, nearest neighbors. It's less accurate than alternatives like linear because it samples from a small number of adjacent pixels, but that's what also makes it relatively faster.\n",
    "\n",
    "Let's start with benchmarking time to run without chunking using `dask`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b342fa83-c68c-4deb-ab45-f9edea025c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "downsampled_s2_data = cloudy_pixels.coarsen(x=4, y=4, boundary='trim').mean()\n",
    "interpolated_downsampled = downsampled_s2_data.interpolate_na(dim='time', method='nearest')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e0fd84-7ee5-468d-a7cc-a6224bdf2eaf",
   "metadata": {},
   "source": [
    "Now, let's chunk the data along the spatial dimensions. We don't want to chunk on the time dimension, so we tell `dask` to leave that dimension alone. Dask treats `-1` as a special flag that means \"do not chunk\" or \"use the full length of this dimension as a single chunk.\" Essentially, `-1` is a shorthand to tell Dask that you want to keep the entire dimension in one piece."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c115dc40-d6c4-4757-b168-ae5894c03ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "cloudy_pixels_chunked = cloudy_pixels.chunk({'x': 1024, 'y': 1024, 'time': -1})\n",
    "downsampled_s2_data = cloudy_pixels_chunked.coarsen(x=4, y=4, boundary='trim').mean()\n",
    "interpolated_downsampled_chunked = downsampled_s2_data.interpolate_na(dim='time', method='nearest')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e349ad-ac66-4358-bf55-6e8dc742a164",
   "metadata": {},
   "source": [
    "Much faster! Let's plot a result for one of the timestamps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee48b21-4017-4712-85e7-bc1de04aee76",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Index of the second-to-last timestamp\n",
    "i = -2\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5), dpi=80)\n",
    "\n",
    "# Original RGB image before interpolation\n",
    "original_rgb = s2_data[[\"red\", \"green\", \"blue\"]].isel(time=i)\n",
    "original_rgb.to_array(\"band\").plot.imshow(ax=axes[0], rgb='band', vmin=0, vmax=2000, add_colorbar=False)\n",
    "axes[0].set_title(f'Original Time: {str(s2_data.time[i].values)[:10]}')\n",
    "\n",
    "# Interpolated RGB image\n",
    "interpolated_rgb = interpolated_downsampled_chunked[[\"red\", \"green\", \"blue\"]].isel(time=i)\n",
    "interpolated_rgb.to_array(\"band\").plot.imshow(ax=axes[1], rgb='band', vmin=0, vmax=2000, add_colorbar=False)\n",
    "axes[1].set_title(f'Interpolated Time: {str(interpolated_downsampled_chunked.time[i].values)[:10]}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f901b5-a483-4d98-9df6-e044900ab611",
   "metadata": {},
   "source": [
    "We can see that the interpolation isn't exactly good, but this example demomnstrates how we can examine time series data and consider challenges like cloud cover whilst improving efficiency with tools like `dask`!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
