{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0r_u53ftaEfA"
      },
      "source": [
        "# Segmentation workflow using PyTorch (U-Net/CNN) for flood and water mask detection\n",
        "\n",
        "1. **Data Acquisition**:\n",
        "   - Downloading the [Tropical and Sub-Tropical Flood and Water Masks dataset](https://pacificdata.org/data/dataset/tropical-and-sub-tropical-flood-and-water-masks).\n",
        "   - Fetching Sentinel-1 RTC (for SAR-based flood detection) and Sentinel-2 imagery from STAC for Vanuatu and the relevant surrounding region.\n",
        "\n",
        "2. **Preprocessing**:\n",
        "   - Aligning Sentinel-1 and/or Sentinel-2 data.\n",
        "   - Normalization, filtering, and masking.\n",
        "   - Preparing training data by pairing inputs (SAR and/or optical images) with water masks.\n",
        "\n",
        "3. **Model Definition**:\n",
        "   - Implementing a U-Net (CNN) in PyTorch for segmentation.\n",
        "\n",
        "4. **Training & Evaluation**:\n",
        "   - Splitting data into train/val/test sets.\n",
        "   - Data loading through batches.\n",
        "   - Loss function and optimizer.\n",
        "   - Model training loop.\n",
        "   - Performance evaluation with IoU and Accuracy Score.\n",
        "\n",
        "5. **Inference & Visualization**:\n",
        "   - Running the model on test images.\n",
        "   - Plotting predicted masks.\n",
        "\n",
        "This sets up a basic segmentation pipeline using U-Net and PyTorch, processing Sentinel-1/2 data for flood and water masks."
      ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !mamba install -y pytorch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "BHoO-oRc_I91"
   },
      "outputs": [],
      "source": [
        "import os\n",
        "import glob\n",
        "from datetime import datetime, timedelta\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset, random_split\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision import models\n",
        "from torchvision.models.resnet import resnet34\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import geopandas as gpd\n",
        "import rasterio\n",
        "import xarray as xr\n",
        "import rioxarray as rxr\n",
        "import planetary_computer\n",
        "import pystac_client\n",
        "import odc.stac\n",
        "import matplotlib.pyplot as plt\n",
        "from shapely.geometry import box\n",
        "from sklearn.metrics import jaccard_score, accuracy_score\n",
        "import dask\n",
        "import dask.array as da\n",
    "import dask.delayed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
      "source": [
        "The **Tropical and Sub-Tropical Flood and Water Masks** dataset provides 10-meter spatial resolution masks designed for developing machine learning models to classify flooding in satellite imagery. It comprises 513 GeoTIFF files, each corresponding to specific flood events and dates, covering 65 flood events since 2018 across 26 countries in tropical and sub-tropical regions.\n",
        "\n",
        "**Dataset Composition:**\n",
        "- **Classes:**\n",
        "  - `0`: No data\n",
        "  - `1`: Land (not flooded)\n",
        "  - `2`: Flooded areas\n",
        "  - `3`: Permanent water bodies\n",
        "- **Associated Dates:**\n",
        "  - **Activation Date:** Date when the flood event was registered in the [EMS Rapid Mapping system](https://mapping.emergency.copernicus.eu/).\n",
        "  - **Event Date:** Initial date of the flood occurrence.\n",
        "  - **Satellite Date:** Date of the latest satellite imagery used for generating the flood mask.\n",
        "\n",
        "The masks were generated by integrating data from the [ESA WorldCover 10m v100 product](https://worldcover2020.esa.int/) and vector layers from the Copernicus Emergency Management System (EMS) Rapid Mapping Activation events. \n",
        "\n",
        "This dataset is particularly useful for training flood detection models to identify and segment flooded areas in satellite images, and we'll use it in the specific geographic context of the Pacific region.\n",
        "\n",
        "The dataset is licensed under the Creative Commons Attribution-NonCommercial 4.0 License.\n",
        "\n",
        "We'll obtain the [data](https://pacificdata.org/data/dataset/tropical-and-sub-tropical-flood-and-water-masks) band then unzip the folder. The labels will be in a subfolder called 'tst-ground-truth-flood-masks' and accompanied by a metadata file which we will use for filtering and image acquisition."
      ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=18m6Q6A6z69tGMbswrqnvlFsfKRt_3qSY\n",
      "To: /home/jovyan/climate-change-natural-disasters/book/subset_vbos_smaller.zip\n",
      "100%|██████████████████████████████████████| 2.03M/2.03M [00:00<00:00, 38.6MB/s]\n"
     ]
    }
   ],
   "source": [
    "# Download the data (subset_vbos_smaller.zip)\n",
    "!gdown \"https://drive.google.com/uc?id=18m6Q6A6z69tGMbswrqnvlFsfKRt_3qSY\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  subset_vbos_smaller.zip\n",
      "  inflating: vbos/flood/subset_vbos_smaller/tst-ground-truth-flood-masks/EMSR636_AOI07_GRA_PRODUCT_ground_truth.tif  \n",
      "  inflating: vbos/flood/subset_vbos_smaller/tst-ground-truth-flood-masks/EMSR636_AOI05_GRA_PRODUCT_ground_truth.tif  \n",
      "  inflating: vbos/flood/subset_vbos_smaller/tst-ground-truth-flood-masks/EMSR507_AOI07_GRA_PRODUCT_ground_truth.tif  \n",
      "  inflating: vbos/flood/subset_vbos_smaller/tst-ground-truth-flood-masks/EMSR507_AOI01_DEL_PRODUCT_ground_truth.tif  \n",
      "  inflating: vbos/flood/subset_vbos_smaller/metadata/metadata.csv  \n",
      "  inflating: vbos/flood/subset_vbos_smaller/tst-ground-truth-flood-masks/EMSR507_AOI09_GRA_PRODUCT_ground_truth.tif  \n",
      "  inflating: vbos/flood/subset_vbos_smaller/tst-ground-truth-flood-masks/EMSR312_08CANDON_DEL_v1_ground_truth.tif  \n",
      "  inflating: vbos/flood/subset_vbos_smaller/tst-ground-truth-flood-masks/EMSR478_AOI04_GRA_PRODUCT_ground_truth.tif  \n",
      "  inflating: vbos/flood/subset_vbos_smaller/tst-ground-truth-flood-masks/EMSR312_02CALAYAN_GRA_v1_ground_truth.tif  \n",
      "  inflating: vbos/flood/subset_vbos_smaller/tst-ground-truth-flood-masks/EMSR556_AOI01_DEL_MONIT03_ground_truth.tif  \n",
      "  inflating: vbos/flood/subset_vbos_smaller/tst-ground-truth-flood-masks/EMSR507_AOI06_GRA_PRODUCT_ground_truth.tif  \n",
      "  inflating: vbos/flood/subset_vbos_smaller/tst-ground-truth-flood-masks/EMSR507_AOI08_GRA_PRODUCT_ground_truth.tif  \n",
      "  inflating: vbos/flood/subset_vbos_smaller/tst-ground-truth-flood-masks/EMSR636_AOI09_GRA_PRODUCT_ground_truth.tif  \n",
      "  inflating: vbos/flood/subset_vbos_smaller/tst-ground-truth-flood-masks/EMSR434_AOI05_GRA_PRODUCT_ground_truth.tif  \n",
      "  inflating: vbos/flood/subset_vbos_smaller/tst-ground-truth-flood-masks/EMSR478_AOI01_GRA_PRODUCT_ground_truth.tif  \n",
      "  inflating: vbos/flood/subset_vbos_smaller/tst-ground-truth-flood-masks/EMSR478_AOI02_GRA_PRODUCT_ground_truth.tif  \n",
      "  inflating: vbos/flood/subset_vbos_smaller/tst-ground-truth-flood-masks/EMSR312_08CANDON_DEL_MONIT01_v1_ground_truth.tif  \n",
      "  inflating: vbos/flood/subset_vbos_smaller/tst-ground-truth-flood-masks/EMSR636_AOI02_GRA_PRODUCT_ground_truth.tif  \n",
      "  inflating: vbos/flood/subset_vbos_smaller/tst-ground-truth-flood-masks/EMSR269_06VEITONGO_GRA_MONIT01_v1_ground_truth.tif  \n",
      "  inflating: vbos/flood/subset_vbos_smaller/tst-ground-truth-flood-masks/EMSR312_05TUGUEGARAO_DEL_v1_ground_truth.tif  \n",
      "  inflating: vbos/flood/subset_vbos_smaller/tst-ground-truth-flood-masks/EMSR312_05TUGUEGARAO_DEL_MONIT01_v1_ground_truth.tif  \n",
      "  inflating: vbos/flood/subset_vbos_smaller/tst-ground-truth-flood-masks/EMSR269_06VEITONGO_GRA_v2_ground_truth.tif  \n",
      "  inflating: vbos/flood/subset_vbos_smaller/tst-ground-truth-flood-masks/EMSR478_AOI03_GRA_PRODUCT_ground_truth.tif  \n",
      "  inflating: vbos/flood/subset_vbos_smaller/tst-ground-truth-flood-masks/EMSR312_07VIGAN_DEL_v1_ground_truth.tif  \n",
      "  inflating: vbos/flood/subset_vbos_smaller/tst-ground-truth-flood-masks/EMSR507_AOI05_DEL_PRODUCT_ground_truth.tif  \n",
      "  inflating: vbos/flood/subset_vbos_smaller/tst-ground-truth-flood-masks/EMSR312_07VIGAN_DEL_MONIT01_v1_ground_truth.tif  \n",
      "  inflating: vbos/flood/subset_vbos_smaller/tst-ground-truth-flood-masks/EMSR312_06ILAGAN_DEL_MONIT01_v1_ground_truth.tif  \n",
      "  inflating: vbos/flood/subset_vbos_smaller/tst-ground-truth-flood-masks/EMSR312_14BAGUIO_GRA_v2_ground_truth.tif  \n",
      "  inflating: vbos/flood/subset_vbos_smaller/tst-ground-truth-flood-masks/EMSR269_02FAHEFA_GRA_v1_ground_truth.tif  \n",
      "  inflating: vbos/flood/subset_vbos_smaller/tst-ground-truth-flood-masks/EMSR312_06ILAGAN_DEL_v1_ground_truth.tif  \n",
      "  inflating: vbos/flood/subset_vbos_smaller/tst-ground-truth-flood-masks/EMSR556_AOI01_DEL_PRODUCT_ground_truth.tif  \n",
      "  inflating: vbos/flood/subset_vbos_smaller/tst-ground-truth-flood-masks/EMSR556_AOI01_DEL_MONIT02_ground_truth.tif  \n",
      "  inflating: vbos/flood/subset_vbos_smaller/tst-ground-truth-flood-masks/EMSR556_AOI01_DEL_MONIT01_ground_truth.tif  \n",
      "  inflating: vbos/flood/subset_vbos_smaller/tst-ground-truth-flood-masks/EMSR507_AOI02_DEL_PRODUCT_ground_truth.tif  \n",
      "  inflating: vbos/flood/subset_vbos_smaller/tst-ground-truth-flood-masks/EMSR507_AOI03_DEL_PRODUCT_ground_truth.tif  \n"
     ]
    }
   ],
   "source": [
    "# Unzip the compressed data\n",
    "!mkdir -p vbos/flood/\n",
    "!unzip subset_vbos_smaller.zip -d vbos/flood/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "TD6n23m2KbsW"
   },
   "outputs": [],
   "source": [
    "# Get list of all labels\n",
    "mask_paths = glob.glob('vbos/flood/subset_vbos_smaller/tst-ground-truth-flood-masks/*.tif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
        },
        "id": "ydL4YnCNNVwS",
        "outputId": "aad77fd7-72ee-439c-cae6-976a531560f0"
      },
      "outputs": [
    {
     "data": {
      "text/plain": [
       "34"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
      ],
      "source": [
        "len(mask_paths)"
      ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
        },
        "id": "R4O5GdAYLlSK",
        "outputId": "823a5776-0242-45cd-e925-1df17434be81"
      },
      "outputs": [
        {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EMSR269_02FAHEFA_GRA_v1_ground_truth.tif\n",
      "EMSR269_06VEITONGO_GRA_MONIT01_v1_ground_truth.tif\n",
      "EMSR269_06VEITONGO_GRA_v2_ground_truth.tif\n",
      "EMSR312_02CALAYAN_GRA_v1_ground_truth.tif\n",
      "EMSR312_05TUGUEGARAO_DEL_MONIT01_v1_ground_truth.tif\n",
      "EMSR312_05TUGUEGARAO_DEL_v1_ground_truth.tif\n",
      "EMSR312_06ILAGAN_DEL_MONIT01_v1_ground_truth.tif\n",
      "EMSR312_06ILAGAN_DEL_v1_ground_truth.tif\n",
      "EMSR312_07VIGAN_DEL_MONIT01_v1_ground_truth.tif\n",
      "EMSR312_07VIGAN_DEL_v1_ground_truth.tif\n"
     ]
    }
   ],
   "source": [
    "ls vbos/flood/subset_vbos_smaller/tst-ground-truth-flood-masks/ | head"
   ]
  },
  {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's implement a U-Net model with a **ResNet34** encoder, adapted for image segmentation tasks. The model takes a **two-channel** input and outputs a **four-class** segmentation mask.\n",
        "\n",
        "This entails our first example of a [convolution neural network (CNN)](https://www.mathworks.com/discovery/convolutional-neural-network.html).\n",
        "\n",
        "#### Model Architecture\n",
        "\n",
        "1. **Encoder (Feature Extraction)**\n",
        "- Uses a **pre-trained ResNet34** as a feature extractor.\n",
        "- The first convolution layer is modified to accept **two input channels** (VV and VH SAR polarizations) instead of three (RGB).\n",
        "- The encoder progressively **downsamples** the input image while extracting high-level features:\n",
        "  - `encoder1`: Initial convolution layer with batch normalization and ReLU.\n",
        "  - `encoder2` → `encoder5`: ResNet34 convolutional layers, reducing spatial dimensions.\n",
        "\n",
        "2. **Bottleneck Layer**\n",
        "- A **1x1 convolution** expands the deepest encoder feature map to **1024 channels**.\n",
        "- Acts as the bridge between encoder and decoder.\n",
        "\n",
        "3. **Decoder (Upsampling with Skip Connections)**\n",
        "- The decoder **upsamples** the feature maps back to the original image resolution.\n",
        "- Uses **skip connections** to concatenate encoder outputs with corresponding decoder layers.\n",
        "- Each upsampling layer consists of:\n",
        "  - **Bilinear interpolation**\n",
        "  - **3x3 convolution**\n",
        "  - **ReLU activation**\n",
        "\n",
        "4. **Final Segmentation Output**\n",
        "- A **1x1 convolution** reduces the output to **4 channels** (matching the number of segmentation classes).\n",
        "- Ensures the final output size is **224x224**.\n",
        "\n",
        "#### Skip Connections and Interpolation Handling\n",
        "- Since downsampling reduces spatial dimensions, **skip connection tensors are interpolated** to match decoder sizes before concatenation.\n",
        "- `F.interpolate()` is used to **ensure spatial compatibility**.\n",
        "\n",
        "#### Forward Pass (Data Flow)\n",
        "1. Input image (B, 2, 224, 224) → passes through encoder (ResNet34) → generates feature maps. Herein, B stands for batch. That is just a small sample of the dataset provided to the model. We have to break down the dataset and provide in small samples of equal size in order to fit on the available RAM. \n",
        "2. Bottleneck layer processes the deepest feature representation.\n",
        "3. Decoder **upsamples** while concatenating interpolated skip connections from the encoder.\n",
        "4. Final **segmentation mask** is generated with shape `(B, 4, 224, 224)`.\n",
        "\n",
        "This architecture should do well for **flood and water segmentation** from Sentinel-1 images, given its ability to retain fine-grained spatial details while leveraging **ResNet34's strong feature extraction**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "gsy94-HL_OjD"
      },
      "outputs": [],
      "source": [
        "# Define U-Net model with ResNet34 backbone\n",
        "class UNetWithResNetEncoder(nn.Module):\n",
        "    def __init__(self, in_channels=2, out_channels=4):\n",
        "        super().__init__()\n",
        "\n",
        "        # Load pre-trained ResNet34 backbone\n",
        "        resnet = models.resnet34(pretrained=True)\n",
        "\n",
        "        # Modify first convolution to accept 2 channels\n",
        "        self.encoder1 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, 64, kernel_size=7, stride=2, padding=3, bias=False),\n",
        "            resnet.bn1,\n",
        "            resnet.relu\n",
        "        )\n",
        "\n",
        "        # Encoder layers\n",
        "        self.encoder2 = resnet.layer1  # (B, 64, H/4, W/4)\n",
        "        self.encoder3 = resnet.layer2  # (B, 128, H/8, W/8)\n",
        "        self.encoder4 = resnet.layer3  # (B, 256, H/16, W/16)\n",
        "        self.encoder5 = resnet.layer4  # (B, 512, H/32, W/32)\n",
        "\n",
        "        # Bottleneck\n",
        "        self.bottleneck = nn.Conv2d(512, 1024, kernel_size=3, padding=1)\n",
        "\n",
        "        # Decoder with skip connections\n",
        "        self.upconv1 = self._upsample(1024, 512)\n",
        "        self.upconv2 = self._upsample(512 + 512, 256)  # Add encoder5 skip connection\n",
        "        self.upconv3 = self._upsample(256 + 256, 128)  # Add encoder4 skip connection\n",
        "        self.upconv4 = self._upsample(128 + 128, 64)   # Add encoder3 skip connection\n",
        "        self.upconv5 = self._upsample(64 + 64, 64)     # Add encoder2 skip connection\n",
        "\n",
        "        # Final segmentation output\n",
        "        self.final_conv = nn.Conv2d(64, out_channels, kernel_size=1)\n",
        "\n",
        "    def _upsample(self, in_channels, out_channels):\n",
        "        \"\"\"Helper function to create upsampling layers.\"\"\"\n",
        "        return nn.Sequential(\n",
        "            nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=True),\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Encoder path\n",
        "        enc1 = self.encoder1(x)   # (B, 64, 112, 112)\n",
        "        enc2 = self.encoder2(enc1) # (B, 64, 56, 56)\n",
        "        enc3 = self.encoder3(enc2) # (B, 128, 28, 28)\n",
        "        enc4 = self.encoder4(enc3) # (B, 256, 14, 14)\n",
        "        enc5 = self.encoder5(enc4) # (B, 512, 7, 7)\n",
        "\n",
        "        # Bottleneck\n",
        "        x = self.bottleneck(enc5)  # (B, 1024, 7, 7)\n",
        "        x = self.upconv1(x)        # (B, 512, 14, 14)\n",
        "\n",
        "        # Ensure enc5 matches x before concatenation\n",
        "        enc5 = F.interpolate(enc5, size=x.shape[2:], mode=\"bilinear\", align_corners=True)\n",
        "        x = torch.cat([x, enc5], dim=1)\n",
        "\n",
        "        x = self.upconv2(x)  # (B, 256, 28, 28)\n",
        "\n",
        "        # Ensure enc4 matches x before concatenation\n",
        "        enc4 = F.interpolate(enc4, size=x.shape[2:], mode=\"bilinear\", align_corners=True)\n",
        "        x = torch.cat([x, enc4], dim=1)\n",
        "\n",
        "        x = self.upconv3(x)  # (B, 128, 56, 56)\n",
        "\n",
        "        # Ensure enc3 matches x before concatenation\n",
        "        enc3 = F.interpolate(enc3, size=x.shape[2:], mode=\"bilinear\", align_corners=True)\n",
        "        x = torch.cat([x, enc3], dim=1)\n",
        "\n",
        "        x = self.upconv4(x)  # (B, 64, 112, 112)\n",
        "\n",
        "        # Ensure enc2 matches x before concatenation\n",
        "        enc2 = F.interpolate(enc2, size=x.shape[2:], mode=\"bilinear\", align_corners=True)\n",
        "        x = torch.cat([x, enc2], dim=1)\n",
        "\n",
        "        x = self.upconv5(x)  # (B, 64, 224, 224)\n",
        "\n",
        "        # Ensure final output size is exactly (B, 4, 224, 224)\n",
        "        x = self.final_conv(x)  # (B, 4, 224, 224)\n",
        "        x = F.interpolate(x, size=(224, 224), mode=\"bilinear\", align_corners=True)  # Fix output size\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The following functions facilitate fetching Sentinel-1 Radiometrically Terrain Corrected (RTC) imagery from the Microsoft Planetary Computer STAC API, using bounding boxes derived from our raster label data.\n",
        "\n",
        "`fetch_s1_rtc_for_date(bounds, date)` fetches Sentinel-1 RTC data for a given date (or up to 3 days later) within the specified bounding box.\n",
        "\n",
        "**How It Works**\n",
        "1. Connects to the **Microsoft Planetary Computer STAC API**.\n",
        "2. Defines a **time window** of 3 days starting from the given date.\n",
        "3. Searches for available **Sentinel-1 RTC** data within the bounding box.\n",
        "4. Signs STAC items using **Planetary Computer authentication**.\n",
        "5. Loads the **first available scene** as an **xarray dataset**.\n",
        "\n",
        "**Inputs**\n",
        "- `bounds`: Bounding box **(left, bottom, right, top)** specifying the geographic area.\n",
        "- `date`: The target date **(YYYY-MM-DD format, string)**.\n",
        "\n",
        "**Returns**\n",
        "- An **xarray dataset** containing the Sentinel-1 RTC image.\n",
        "- If no data is found, returns **None**.\n",
        "\n",
        "\n",
        "\n",
        "`match_s1_rtc_to_dataframe(df, mask_path)` finds and fetches Sentinel-1 RTC imagery corresponding to a given ground truth mask.\n",
        "\n",
        "**How It Works**\n",
        "1. Searches for the **specified mask file** (`mask_path`) in the given DataFrame (`df`).\n",
        "2. Extracts the **satellite image acquisition date**.\n",
        "3. Converts the date format from **DD/MM/YYYY** to **YYYY-MM-DD**.\n",
        "4. Reads the **bounding box (extent)** of the mask using **Rasterio**.\n",
        "5. Calls `fetch_s1_rtc_for_date()` with the mask’s bounding box and acquisition date.\n",
        "6. Returns the matching Sentinel-1 RTC dataset.\n",
        "\n",
        "**Inputs**\n",
        "- `df`: A DataFrame containing mask metadata, including **file names and acquisition dates**.\n",
        "- `mask_path`: The **file name** of the ground truth flood mask.\n",
        "\n",
        "**Returns**\n",
        "- The **corresponding Sentinel-1 RTC image** as an xarray dataset.\n",
        "\n",
        "\n",
        "These functions are designed to fetch Sentinel-1 RTC data needed to match our flood masks. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "iE2Y1w-x_jFe"
      },
      "outputs": [],
      "source": [
        "# Function to fetch Sentinel-1 RTC data matching a given date (or up to 3 days after)\n",
        "def fetch_s1_rtc_for_date(bounds, date):\n",
        "    catalog = pystac_client.Client.open(\"https://planetarycomputer.microsoft.com/api/stac/v1\")\n",
        "\n",
        "    # Define the search time range (3-day + window)\n",
        "    date = datetime.strptime(date, \"%Y-%m-%d\")  # Convert to datetime object\n",
        "\n",
        "    start_date = date.strftime(\"%Y-%m-%d\")\n",
        "    end_date = (date + timedelta(days=3)).strftime(\"%Y-%m-%d\")\n",
        "\n",
        "    search = catalog.search(\n",
        "        collections=[\"sentinel-1-rtc\"],\n",
        "        bbox=[bounds.left, bounds.bottom, bounds.right, bounds.top],\n",
        "        datetime=f\"{start_date}/{end_date}\"\n",
        "    )\n",
        "\n",
    "    items = list(search.items())\n",
        "    if not items:\n",
        "        return None  # No matching data found\n",
        "\n",
        "    signed_items = [planetary_computer.sign(item) for item in items]\n",
        "\n",
        "    # Load the first available scene (or refine selection criteria as needed)\n",
    "    ds = odc.stac.load(signed_items[:1], bbox=bounds, dtype=\"float32\")\n",
        "\n",
        "    return ds #.mean(dim=\"time\") if ds else None  # Take mean if multiple images exist\n",
        "\n",
        "# Function to find matching Sentinel-1 RTC imagery for each row in DataFrame\n",
        "def match_s1_rtc_to_dataframe(df, mask_path):\n",
        "    filtered_row = df.loc[df[\"File Name\"] == mask_path].iloc[0]\n",
        "    sat_date = filtered_row[\"Satellite Date\"]\n",
        "    formatted_date = datetime.strptime(sat_date, \"%d/%m/%Y\").strftime(\"%Y-%m-%d\")\n",
        "\n",
        "\n",
        "    with rasterio.open(f\"tst-ground-truth-flood-masks/{mask_path}_ground_truth.tif\") as src:\n",
        "        bounds = src.bounds  # Get bounding box of the mask\n",
        "\n",
        "    s1_image = fetch_s1_rtc_for_date(bounds, formatted_date)\n",
        "\n",
        "    return s1_image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Load the metadata file into a dataframe and select labels for Vanuatu and a relevant surrounding region so as to provide sufficient data for model training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "grW1z7B9AWtg"
      },
      "outputs": [],
      "source": [
        "# Example DataFrame\n",
        "df = pd.read_csv('metadata/metadata.csv')\n",
        "# Filter for specific countries\n",
        "selected_countries = {\"Vanuatu\", \"Tonga\"} #, \"Timor-Leste\" , \"Philippines\", \"Viet Nam\", \"Australia\"}\n",
        "df = df[df[\"Country\"].isin(selected_countries)]\n",
        "\n",
        "\"\"\"\n",
        "# Limit the number of rows for the Philippines\n",
        "df_philippines = df[df[\"Country\"] == \"Philippines\"].head(5)  # Keep only 10 rows\n",
        "df_other = df[df[\"Country\"] != \"Philippines\"]  # Keep all other countries\n",
        "\n",
        "# Concatenate back\n",
        "df = pd.concat([df_philippines, df_other])\n",
        "\n",
        "# Reset index\n",
        "df = df.reset_index(drop=True)\n",
        "\"\"\"\n",
        "\n",
        "print(\"len(df):\", len(df))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now let's establish a dataset class.\n",
        "\n",
        "This `FloodDataset` class is a PyTorch `Dataset` that loads Sentinel-1 RTC images and corresponding flood masks, tiles them into fixed-size patches, and prepares them for training.\n",
        "\n",
        "Key inputs:\n",
        "- `mask_paths`: A list of file names representing flood masks.\n",
        "- `transform`: Optional data transformations (not used here).\n",
        "- `tile_size`: Size of the image and mask patches (default: 224x224).\n",
        "\n",
        "**Step 1: Load mask file names**\n",
        "The dataset uses mask file names from the metadata DataFrame (`df`).\n",
        "\n",
        "**Step 2: Fetch corresponding Sentinel-1 RTC images using Dask**\n",
        "- The function `match_s1_rtc_to_dataframe(df, mask_path)` fetches Sentinel-1 RTC data for each flood mask using STAC.\n",
        "- Uses **Dask Delayed** to parallelize the fetching process, reducing I/O overhead.\n",
        "\n",
        "**Step 3: Load masks, get Sentinel-1 images and convert them to tensors**\n",
        "- Read the ground truth flood mask from the file.\n",
        "- Get Sentinel-1 data. If Sentinel-1 data is unavailable for a mask in the requested time window, it is skipped.\n",
        "- Convert the Sentinel-1 image to a **PyTorch tensor**.\n",
        "- Convert the mask to a tensor and reshape to match PyTorch conventions.\n",
        "\n",
        "**Step 4: Tile images and masks into 224x224 patches**\n",
        "- `_tile_image_and_mask()` divides the large images and masks into smaller **224x224** tiles.\n",
        "- The resulting tiles are stored in lists.\n",
        "\n",
        "\n",
        "**Tiling Function (`_tile_image_and_mask`)**\n",
        "- Splits images and masks into fixed-size tiles.\n",
        "- Loops through the image and mask with a **stride** of `tile_size` (224 pixels).\n",
        "- Ensures that only **fully-sized tiles** (224x224) are added.\n",
        "\n",
        "\n",
        "**Dataset Methods**\n",
        "\n",
        "`__len__(self)` Returns the total number of 224x224 tiles.\n",
        "\n",
        "`__getitem__(self, idx)` returns the corresponding **image-mask pair** for training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "__77Zix1AeI0"
      },
      "outputs": [],
      "source": [
        "class FloodDataset(Dataset):\n",
        "    def __init__(self, mask_paths, transform=None, tile_size=224):\n",
        "        #self.mask_paths = filter_masks_within_vanuatu(mask_paths)\n",
        "        self.mask_paths = list(df[\"File Name\"])\n",
        "        self.transform = transform\n",
        "        self.image_list = []\n",
        "        self.mask_list = []\n",
        "        self.tile_size = tile_size\n",
        "\n",
        "        # Use Dask Delayed for parallel fetching\n",
        "        delayed_images = []\n",
        "        for mask_path in self.mask_paths:\n",
        "            #s1_image = fetch_stac_data_from_mask(mask_path, \"sentinel-1-rtc\", \"2018-01-01\", \"2018-12-31\")\n",
        "            #s2_image = fetch_stac_data_from_mask(mask_path, \"sentinel-2-l2a\", \"2018-01-01\", \"2018-12-31\")\n",
        "            s1_image = match_s1_rtc_to_dataframe(df, mask_path)\n",
        "            delayed_images.append(dask.delayed(s1_image))\n",
        "\n",
        "        # Compute all Dask delayed tasks at once (efficient batch processing)\n",
        "        computed_images = dask.compute(*delayed_images)\n",
        "\n",
        "        self.image_tiles = []\n",
        "        self.mask_tiles = []\n",
        "\n",
        "        for i, mask_path in enumerate(self.mask_paths):\n",
        "            if computed_images[i] is None:\n",
        "                continue  # Skip if no data was found\n",
        "\n",
        "            with rasterio.open(f\"tst-ground-truth-flood-masks/{mask_path}_ground_truth.tif\") as src:\n",
        "                mask = src.read(1).astype(np.float32)\n",
        "            try:\n",
        "                image_tensor = torch.tensor(computed_images[i].to_array().values, dtype=torch.float32)\n",
        "                mask_tensor = torch.tensor(mask).unsqueeze(0)\n",
        "\n",
        "                image_tiles, mask_tiles = self._tile_image_and_mask(image_tensor, mask_tensor)\n",
        "                self.image_tiles.extend(image_tiles)  # Flatten the dataset\n",
        "                self.mask_tiles.extend(mask_tiles)\n",
        "            except:\n",
        "                print(\"error when creating tensor\")\n",
        "                continue\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_tiles)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.image_tiles[idx], self.mask_tiles[idx]\n",
        "\n",
        "    def _tile_image_and_mask(self, image, mask):\n",
        "        #Tiles both the image and mask into patches of size (tile_size x tile_size).\n",
        "\n",
        "        height, width = image.shape[-2:]  # Ensure compatibility with tensor dimensions\n",
        "        image_tiles = []\n",
        "        mask_tiles = []\n",
        "\n",
        "        for i in range(0, height, self.tile_size):\n",
        "            for j in range(0, width, self.tile_size):\n",
        "                # Extract tiles\n",
        "                image_tile = image[..., i:i + self.tile_size, j:j + self.tile_size]\n",
        "                mask_tile = mask[..., i:i + self.tile_size, j:j + self.tile_size]\n",
        "\n",
        "                if mask_tile.shape[-2] == self.tile_size and mask_tile.shape[-1] == self.tile_size and image_tile.shape[-2] == self.tile_size and image_tile.shape[-1] == self.tile_size:\n",
        "                    image_tiles.append(image_tile.clone().detach())\n",
        "                    mask_tiles.append(mask_tile.clone().detach())\n",
        "\n",
        "        return image_tiles, mask_tiles"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Generate the dataset and split it into train, validation and test datasets.\n",
        "\n",
        "We are reserving 70% of samples for training, 20% for validation and approximately 10% for testing.\n",
        "\n",
        "Notice we are providing a transform to normalize image values to the range [0, 1]. This improves model stability and convergence since Sentinel-1 RTC images may have different ranges, therefore we need to ensure consistent input scaling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F4V-SjeMAjEW"
      },
      "outputs": [],
      "source": [
        "full_dataset = FloodDataset(mask_paths=mask_paths,\n",
        "                             transform=transforms.Compose([transforms.Normalize(0, 1)]), tile_size=224)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vt4JLLKzAqW1"
      },
      "outputs": [],
      "source": [
        "# Define split sizes\n",
        "train_size = int(0.7 * len(full_dataset))\n",
        "val_size = int(0.20 * len(full_dataset))\n",
        "test_size = len(full_dataset) - train_size - val_size  # Ensures total sums up correctly\n",
        "\n",
        "# Perform split\n",
        "train_dataset, val_dataset, test_dataset = random_split(full_dataset, [train_size, val_size, test_size])\n",
        "print(f\"Dataset sizes -> Train: {len(train_dataset)}, Val: {len(val_dataset)}, Test: {len(test_dataset)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's initialize **PyTorch DataLoaders** for training, validation, and testing datasets. DataLoaders efficiently manage data loading, batching, and shuffling.\n",
        "\n",
        "**Key Parameters**\n",
        "- **`train_dataset_`, `val_dataset`, `test_dataset`**:  \n",
        "  - These are **FloodDataset** instances containing **image-mask pairs**.\n",
        "- **`batch_size=2`**:  \n",
        "  - Each batch contains **2 image-mask pairs**.\n",
        "- **`shuffle=True` (Training Only)**:  \n",
        "  - Randomizes data order for better generalization.\n",
        "- **`shuffle=False` (Validation & Test Sets)**:  \n",
        "  - Keeps a fixed order for **consistent evaluation**.\n",
        "- **`drop_last=True`**:  \n",
        "  - Drops the last batch if it's smaller than `batch_size` to ensure **consistent batch sizes**.\n",
        "\n",
        "**How it works**\n",
        "- **`train_loader`**:  \n",
        "  - Feeds **randomized batches** for training, preventing the model from memorizing patterns.\n",
        "- **`val_loader`**:  \n",
        "  - Provides **sequential batches** for model validation.\n",
        "- **`test_loader`**:  \n",
        "  - Loads data for final model testing and performance evaluation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QtQOyCtpAtrw"
      },
      "outputs": [],
      "source": [
        "# Create DataLoaders\n",
        "train_loader = DataLoader(train_dataset_, batch_size=2, shuffle=True, drop_last=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=2, shuffle=False, drop_last=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=2, shuffle=False, drop_last=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now let's train our **U-Net** model. We'll specify some hyperparamets such as the loss function, optimizer and number of epochs to train for.\n",
        "\n",
        "\n",
        "**1. Model, Loss, and Optimizer Setup**\n",
        "- **`model = UNetWithResNetEncoder()`**  \n",
        "  - Initializes the **U-Net** model.\n",
        "- **`criterion = nn.CrossEntropyLoss()`**  \n",
        "  - Uses **CrossEntropyLoss** for multi-class segmentation.\n",
        "- **`optimizer = optim.Adam(model.parameters(), lr=0.001)`**  \n",
        "  - Uses the **Adam optimizer** with a learning rate of `0.001` to update the model parameters.\n",
        "\n",
        "\n",
        "**2. Training Loop**\n",
        "- The training loop runs for **70 epochs**, ensuring the model learns over multiple iterations.\n",
        "- `model.train()` sets the model in **training mode**, enabling dropout and batch normalization.\n",
        "- We load a batch of images and masks iteratively from `train_loader`.\n",
        "- `outputs = model(images)` performs forward propagation and generates predictions.\n",
        "- `loss = criterion(outputs, masks)` measures how different predictions are from ground truth.\n",
        "- Backpropagation and optimization: `loss.backward()`: Computes gradients. `optimizer.step()`: Updates model parameters.\n",
        "\n",
        "\n",
        "**4. Validation Phase**\n",
        "- `model.eval()` switches to **evaluation mode**, disabling dropout and batch norm updates.\n",
        "- `torch.no_grad()` stops gradient computation to **save memory and speed up inference**.\n",
        "- The validation loop computes loss but does not update model weights.\n",
        "\n",
        "**5. Print Epoch Statistics**\n",
        "- Computes average training and validation loss for each epoch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "gznIBi2TKBAs",
        "outputId": "c4d4e138-11b9-499c-d012-53173f6128e2"
      },
      "outputs": [],
      "source": [
        "# Model, Loss, Optimizer\n",
        "model = UNetWithResNetEncoder()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "print(\"Starting training\")\n",
        "\n",
        "# Training Loop\n",
        "for epoch in range(70):\n",
        "    model.train()  # Ensure the model is in training mode\n",
        "\n",
        "    # Training phase\n",
        "    train_loss = 0.0\n",
        "    for batch in train_loader:\n",
        "        images, masks = batch  # Unpack stacked tensors\n",
        "\n",
        "        if isinstance(images, list):\n",
        "            images = torch.cat(images, dim=0)  # Ensure 4D tensor (B, C, H, W)\n",
        "        if isinstance(masks, list):\n",
        "            masks = torch.cat(masks, dim=0)\n",
        "\n",
        "        images = torch.stack(images) if isinstance(images, list) else images\n",
        "        masks = torch.stack(masks) if isinstance(masks, list) else masks\n",
        "        masks = masks.squeeze()\n",
        "        masks = masks.long()  # Convert masks to LongTensor before computing loss\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)  # Forward pass\n",
        "        loss = criterion(outputs, masks)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "\n",
        "    # Average training loss for the epoch\n",
        "    train_loss /= len(train_loader)\n",
        "\n",
        "    # Validation phase\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    val_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            images, masks = batch  # Unpack stacked tensors\n",
        "\n",
        "            if isinstance(images, list):\n",
        "                images = torch.cat(images, dim=0)\n",
        "            if isinstance(masks, list):\n",
        "                masks = torch.cat(masks, dim=0)\n",
        "\n",
        "            images = torch.stack(images) if isinstance(images, list) else images\n",
        "            masks = torch.stack(masks) if isinstance(masks, list) else masks\n",
        "            masks = masks.squeeze()\n",
        "            masks = masks.long()  # Convert masks to LongTensor\n",
        "\n",
        "            outputs = model(images)  # Forward pass\n",
        "            loss = criterion(outputs, masks)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "    # Average validation loss for the epoch\n",
        "    val_loss /= len(val_loader)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}, Train Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Save the model to a file for future use. `torch.save(model.state_dict(), final_model_path)` saves the model's learned parameters (weights) to the specified path. The `.state_dict()` method is used because it only saves the model's parameters, not the entire model structure, which is a more efficient approach. This means we need to re-instate the model class any time in the future in which we want to run inference for this to be complete."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1jMfnhWWA5xF"
      },
      "outputs": [],
      "source": [
        "# Directory to save the model checkpoints\n",
        "MODEL_SAVE_DIR = 'model_s1'\n",
        "os.makedirs(MODEL_SAVE_DIR, exist_ok=True)\n",
        "\n",
        "# Save the final model after training\n",
        "final_model_path = os.path.join(MODEL_SAVE_DIR, \"final_model_70ep_vnu_tvt.pth\")\n",
        "torch.save(model.state_dict(), final_model_path)\n",
        "print(f\"Final model saved at {final_model_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we'll define an inference function that is used to make predictions with a trained model.\n",
        "\n",
        "`model.eval()`:\n",
        "   - This sets the model to evaluation mode. In evaluation mode, dropout and batch normalization behave differently than during training. This ensures that the model's behavior is consistent when making predictions.\n",
        "\n",
        "`with torch.no_grad()`:\n",
        "   - This disables the tracking of gradients, which is unnecessary during inference (i.e., when making predictions). Disabling gradients helps reduce memory usage and speeds up the computation.\n",
        "\n",
        "`output = model(image.unsqueeze(0))`:\n",
        "   - The `image.unsqueeze(0)` adds a batch dimension to the input image. Most models expect a batch of images, so even if there is only one image, it is wrapped in a batch.\n",
        "   - `model(image.unsqueeze(0))` runs the image through the trained model to generate an output (logits or raw predictions) for each class.\n",
        "\n",
        "`torch.softmax(output, dim=1)`:\n",
        "   - The softmax function is applied to the output along the class dimension (dimension 1). Softmax normalizes the output values into probabilities, so each pixel in the image will have a probability distribution across all the possible classes.\n",
        "\n",
        "`prediction = prediction.squeeze(0)`:\n",
        "   - This removes the batch dimension (which was added earlier using `unsqueeze(0)`), returning the prediction in the shape of a single image, but now with probabilities for each pixel in each class.\n",
        "\n",
        "`prediction = torch.argmax(prediction, dim=0).cpu().numpy()`:\n",
        "   - `torch.argmax(prediction, dim=0)` selects the class with the highest probability for each pixel (i.e., the predicted class label for each pixel).\n",
        "   - `.cpu().numpy()` moves the result to the CPU and converts the PyTorch tensor into a NumPy array, which is easier to handle for further processing or visualization outside of PyTorch.\n",
        "\n",
        "`return prediction`:\n",
        "   - The function returns the final prediction, which is a NumPy array containing the predicted class labels for each pixel."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qWJxH1JVA_RK"
      },
      "outputs": [],
      "source": [
        "# Inference Function\n",
        "def inference(model, image):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        output = model(image.unsqueeze(0))\n",
        "        prediction = torch.softmax(output, dim=1).squeeze(0)  # Softmax across channels\n",
        "        prediction = torch.argmax(prediction, dim=0).cpu().numpy()  # Get class labels\n",
        "    return prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's define how to execute the **evaluation** and **visualization** processes for the trained model. The two functions `evaluate()` and `visualize_prediction()` run the evaluation and visualize the model's predicitive capabilities. The model's performance is evaluated on the test set.\n",
        "\n",
        "`evaluate()`:\n",
        "- `model.eval()`: Just like before, sets the model to evaluation mode, ensuring no dropout and that batch normalization behaves appropriately.\n",
        "- `with torch.no_grad()`: Disables gradient computation, which is unnecessary during evaluation and reduces memory usage.\n",
        "- For each batch of images and corresponding ground truth masks, the model's predictions are calculated.\n",
        "  - `torch.argmax(outputs, dim=1)`: Converts the output from the model (which contains predictions across different classes) into the predicted class labels by selecting the class with the highest score for each pixel.\n",
        "  - Both the predicted and true mask tensors are flattened into 1D arrays for easier comparison.\n",
        "  - `jaccard_score`: Calculates the Intersection over Union (IoU), a measure of overlap between the predicted and ground truth masks.\n",
        "  - `accuracy_score`: Calculates the accuracy of the predicted masks compared to the ground truth.\n",
        "- After evaluating all the batches, we calculate and print the average IoU and accuracy scores across the dataset.\n",
        "\n",
        "\n",
        "`visualize_prediction()`:\n",
        "- Creates a figure with 3 subplots arranged horizontally (image tile, ground truth, prediction), each having the same size.\n",
        "- `vv_image = image[0].cpu().numpy()`: Converts the first channel of the input Sentinel-1 image tile to a NumPy array for visualization.\n",
        "- Displays the generated plots."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 603
        },
        "id": "5G_ROxKCsFFz",
        "outputId": "feed6b5c-b317-430b-b1b3-0d142358592a"
      },
      "outputs": [],
      "source": [
        "# Evaluation Function\n",
        "def evaluate(model, data_loader):\n",
        "    model.eval()\n",
        "    iou_scores, accuracy_scores = [], []\n",
        "    with torch.no_grad():\n",
        "        for images, masks in data_loader:\n",
        "            outputs = model(images)\n",
        "            preds = torch.argmax(outputs, dim=1)  # Convert [batch, 4, H, W] → [batch, H, W]\n",
        "            preds = preds.cpu().numpy().flatten()\n",
        "            masks = masks.cpu().numpy().flatten()\n",
        "            iou_scores.append(jaccard_score(masks, preds, average=\"macro\"))\n",
        "            accuracy_scores.append(accuracy_score(masks, preds))\n",
        "    print(f\"Mean IoU: {np.mean(iou_scores):.4f}, Mean Accuracy: {np.mean(accuracy_scores):.4f}\")\n",
        "\n",
        "# Visualization Function\n",
        "def visualize_prediction(image, mask, prediction):\n",
        "    fig, ax = plt.subplots(1, 3, figsize=(15, 5))\n",
        "\n",
        "    # Convert 2-channel image to single-channel (VV)\n",
        "    vv_image = image[0].cpu().numpy()  # Use first channel (VV)\n",
        "\n",
        "    ax[0].imshow(vv_image, cmap=\"gray\")  # Display as grayscale\n",
        "    ax[0].set_title(\"Input Image (VV)\")\n",
        "\n",
        "    ax[1].imshow(mask.squeeze(), cmap=\"gray\")\n",
        "    ax[1].set_title(\"Ground Truth\")\n",
        "\n",
        "    ax[2].imshow(prediction, cmap=\"gray\")\n",
        "    ax[2].set_title(\"Prediction\")\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# Run Evaluation\n",
        "evaluate(model, test_loader)\n",
        "\n",
        "# Test Inference and Visualization\n",
        "sample_image, sample_mask = test_dataset[0]\n",
        "predicted_mask = inference(model, sample_image)\n",
        "visualize_prediction(sample_image, sample_mask, predicted_mask)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now let's use an [administrative boundaries file from Pacific Data Hub](https://pacificdata.org/data/dataset/9dba1377-740c-429e-92ce-6a484657b4d9/resource/66ae054b-9b67-4876-b59c-0b078c31e800) to get imagery for Vanuatu and run inference on these areas with our trained model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "BI7QbefL5T9o"
      },
      "outputs": [],
      "source": [
        "admin_boundaries_gdf = gpd.read_file(\"./2016_phc_vut_iid_4326.geojson\").to_crs(epsg=\"4326\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Load the saved model from file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "IiIwIyr_97_O",
        "outputId": "acd3d4ce-6491-4dcf-db64-ce866cf029d0"
      },
      "outputs": [],
      "source": [
        "# Define the model architecture (must match the saved model)\n",
        "model = UNetWithResNetEncoder(in_channels=2, out_channels=4)  # Ensure the same architecture\n",
        "\n",
        "# Load the model weights\n",
        "model.load_state_dict(torch.load(\"model_s1/final_model_70ep_vnu_tvt.pth\"))\n",
        "\n",
        "# Set the model to evaluation mode (important for inference)\n",
        "model.eval()\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "New function to get relevant Sentinel-1 imagery within 10 days after the requested date."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "EGZ58vbzCw4w"
      },
      "outputs": [],
      "source": [
        "# Function to fetch Sentinel-1 RTC data matching a given date (or up to 10 days after)\n",
        "def fetch_s1_rtc_for_date_inference(bounds, date):\n",
        "    catalog = pystac_client.Client.open(\"https://planetarycomputer.microsoft.com/api/stac/v1\")\n",
        "\n",
        "    # Define the search time range (10-day + window)\n",
        "    date = datetime.strptime(date, \"%Y-%m-%d\")  # Convert to datetime object\n",
        "\n",
        "    start_date = date.strftime(\"%Y-%m-%d\")\n",
        "    end_date = (date + timedelta(days=10)).strftime(\"%Y-%m-%d\")\n",
        "\n",
        "    search = catalog.search(\n",
        "        collections=[\"sentinel-1-rtc\"],\n",
        "        bbox=bounds,\n",
        "        datetime=f\"{start_date}/{end_date}\"\n",
        "    )\n",
        "\n",
        "    items = list(search.get_items())\n",
        "    if not items:\n",
        "        return None  # No matching data found\n",
        "\n",
        "    signed_items = [planetary_computer.sign(item) for item in items]\n",
        "\n",
        "    # Load the first available scene (or refine selection criteria as needed)\n",
        "    ds = odc.stac.load(signed_items, dtype=\"float32\", chunks={'x': 512, 'y': 512}) # signed_items[:1]\n",
        "\n",
        "    return ds.mean(dim=\"time\") if ds else None  # Take mean if multiple images exist"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Get the imagery for each boundary. We are using a date just after a major earthquake as reference here, as floods present a heightened risk post-earthquake."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0XMO35cC5Uwq",
        "outputId": "f5f2c24f-bea3-4e19-b18b-676306e21cc0"
      },
      "outputs": [],
      "source": [
        "vanuatu_s1_imagery = []\n",
        "\n",
        "for idx, row in admin_boundaries_gdf.iterrows():\n",
        "    polygon = row.geometry\n",
        "    bbox = polygon.bounds  # (minx, miny, maxx, maxy)\n",
        "    event_date = '20/12/2024'\n",
        "    formatted_date = datetime.strptime(event_date, \"%d/%m/%Y\").strftime(\"%Y-%m-%d\")\n",
        "    s1_data = fetch_s1_rtc_for_date_inference(bounds=list(bbox), date=formatted_date)\n",
        "    vanuatu_s1_imagery.append(s1_data)\n",
        "    admin_name = row.iname\n",
        "    print(\"added \", admin_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "h0oi6tcpD-wM",
        "outputId": "19269956-7682-4f61-e85f-12d33f7327c2"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div><svg style=\"position: absolute; width: 0; height: 0; overflow: hidden\">\n",
              "<defs>\n",
              "<symbol id=\"icon-database\" viewBox=\"0 0 32 32\">\n",
              "<path d=\"M16 0c-8.837 0-16 2.239-16 5v4c0 2.761 7.163 5 16 5s16-2.239 16-5v-4c0-2.761-7.163-5-16-5z\"></path>\n",
              "<path d=\"M16 17c-8.837 0-16-2.239-16-5v6c0 2.761 7.163 5 16 5s16-2.239 16-5v-6c0 2.761-7.163 5-16 5z\"></path>\n",
              "<path d=\"M16 26c-8.837 0-16-2.239-16-5v6c0 2.761 7.163 5 16 5s16-2.239 16-5v-6c0 2.761-7.163 5-16 5z\"></path>\n",
              "</symbol>\n",
              "<symbol id=\"icon-file-text2\" viewBox=\"0 0 32 32\">\n",
              "<path d=\"M28.681 7.159c-0.694-0.947-1.662-2.053-2.724-3.116s-2.169-2.030-3.116-2.724c-1.612-1.182-2.393-1.319-2.841-1.319h-15.5c-1.378 0-2.5 1.121-2.5 2.5v27c0 1.378 1.122 2.5 2.5 2.5h23c1.378 0 2.5-1.122 2.5-2.5v-19.5c0-0.448-0.137-1.23-1.319-2.841zM24.543 5.457c0.959 0.959 1.712 1.825 2.268 2.543h-4.811v-4.811c0.718 0.556 1.584 1.309 2.543 2.268zM28 29.5c0 0.271-0.229 0.5-0.5 0.5h-23c-0.271 0-0.5-0.229-0.5-0.5v-27c0-0.271 0.229-0.5 0.5-0.5 0 0 15.499-0 15.5 0v7c0 0.552 0.448 1 1 1h7v19.5z\"></path>\n",
              "<path d=\"M23 26h-14c-0.552 0-1-0.448-1-1s0.448-1 1-1h14c0.552 0 1 0.448 1 1s-0.448 1-1 1z\"></path>\n",
              "<path d=\"M23 22h-14c-0.552 0-1-0.448-1-1s0.448-1 1-1h14c0.552 0 1 0.448 1 1s-0.448 1-1 1z\"></path>\n",
              "<path d=\"M23 18h-14c-0.552 0-1-0.448-1-1s0.448-1 1-1h14c0.552 0 1 0.448 1 1s-0.448 1-1 1z\"></path>\n",
              "</symbol>\n",
              "</defs>\n",
              "</svg>\n",
              "<style>/* CSS stylesheet for displaying xarray objects in jupyterlab.\n",
              " *\n",
              " */\n",
              "\n",
              ":root {\n",
              "  --xr-font-color0: var(--jp-content-font-color0, rgba(0, 0, 0, 1));\n",
              "  --xr-font-color2: var(--jp-content-font-color2, rgba(0, 0, 0, 0.54));\n",
              "  --xr-font-color3: var(--jp-content-font-color3, rgba(0, 0, 0, 0.38));\n",
              "  --xr-border-color: var(--jp-border-color2, #e0e0e0);\n",
              "  --xr-disabled-color: var(--jp-layout-color3, #bdbdbd);\n",
              "  --xr-background-color: var(--jp-layout-color0, white);\n",
              "  --xr-background-color-row-even: var(--jp-layout-color1, white);\n",
              "  --xr-background-color-row-odd: var(--jp-layout-color2, #eeeeee);\n",
              "}\n",
              "\n",
              "html[theme=\"dark\"],\n",
              "html[data-theme=\"dark\"],\n",
              "body[data-theme=\"dark\"],\n",
              "body.vscode-dark {\n",
              "  --xr-font-color0: rgba(255, 255, 255, 1);\n",
              "  --xr-font-color2: rgba(255, 255, 255, 0.54);\n",
              "  --xr-font-color3: rgba(255, 255, 255, 0.38);\n",
              "  --xr-border-color: #1f1f1f;\n",
              "  --xr-disabled-color: #515151;\n",
              "  --xr-background-color: #111111;\n",
              "  --xr-background-color-row-even: #111111;\n",
              "  --xr-background-color-row-odd: #313131;\n",
              "}\n",
              "\n",
              ".xr-wrap {\n",
              "  display: block !important;\n",
              "  min-width: 300px;\n",
              "  max-width: 700px;\n",
              "}\n",
              "\n",
              ".xr-text-repr-fallback {\n",
              "  /* fallback to plain text repr when CSS is not injected (untrusted notebook) */\n",
              "  display: none;\n",
              "}\n",
              "\n",
              ".xr-header {\n",
              "  padding-top: 6px;\n",
              "  padding-bottom: 6px;\n",
              "  margin-bottom: 4px;\n",
              "  border-bottom: solid 1px var(--xr-border-color);\n",
              "}\n",
              "\n",
              ".xr-header > div,\n",
              ".xr-header > ul {\n",
              "  display: inline;\n",
              "  margin-top: 0;\n",
              "  margin-bottom: 0;\n",
              "}\n",
              "\n",
              ".xr-obj-type,\n",
              ".xr-array-name {\n",
              "  margin-left: 2px;\n",
              "  margin-right: 10px;\n",
              "}\n",
              "\n",
              ".xr-obj-type {\n",
              "  color: var(--xr-font-color2);\n",
              "}\n",
              "\n",
              ".xr-sections {\n",
              "  padding-left: 0 !important;\n",
              "  display: grid;\n",
              "  grid-template-columns: 150px auto auto 1fr 0 20px 0 20px;\n",
              "}\n",
              "\n",
              ".xr-section-item {\n",
              "  display: contents;\n",
              "}\n",
              "\n",
              ".xr-section-item input {\n",
              "  display: inline-block;\n",
              "  opacity: 0;\n",
              "  height: 0;\n",
              "}\n",
              "\n",
              ".xr-section-item input + label {\n",
              "  color: var(--xr-disabled-color);\n",
              "}\n",
              "\n",
              ".xr-section-item input:enabled + label {\n",
              "  cursor: pointer;\n",
              "  color: var(--xr-font-color2);\n",
              "}\n",
              "\n",
              ".xr-section-item input:focus + label {\n",
              "  border: 2px solid var(--xr-font-color0);\n",
              "}\n",
              "\n",
              ".xr-section-item input:enabled + label:hover {\n",
              "  color: var(--xr-font-color0);\n",
              "}\n",
              "\n",
              ".xr-section-summary {\n",
              "  grid-column: 1;\n",
              "  color: var(--xr-font-color2);\n",
              "  font-weight: 500;\n",
              "}\n",
              "\n",
              ".xr-section-summary > span {\n",
              "  display: inline-block;\n",
              "  padding-left: 0.5em;\n",
              "}\n",
              "\n",
              ".xr-section-summary-in:disabled + label {\n",
              "  color: var(--xr-font-color2);\n",
              "}\n",
              "\n",
              ".xr-section-summary-in + label:before {\n",
              "  display: inline-block;\n",
              "  content: \"►\";\n",
              "  font-size: 11px;\n",
              "  width: 15px;\n",
              "  text-align: center;\n",
              "}\n",
              "\n",
              ".xr-section-summary-in:disabled + label:before {\n",
              "  color: var(--xr-disabled-color);\n",
              "}\n",
              "\n",
              ".xr-section-summary-in:checked + label:before {\n",
              "  content: \"▼\";\n",
              "}\n",
              "\n",
              ".xr-section-summary-in:checked + label > span {\n",
              "  display: none;\n",
              "}\n",
              "\n",
              ".xr-section-summary,\n",
              ".xr-section-inline-details {\n",
              "  padding-top: 4px;\n",
              "  padding-bottom: 4px;\n",
              "}\n",
              "\n",
              ".xr-section-inline-details {\n",
              "  grid-column: 2 / -1;\n",
              "}\n",
              "\n",
              ".xr-section-details {\n",
              "  display: none;\n",
              "  grid-column: 1 / -1;\n",
              "  margin-bottom: 5px;\n",
              "}\n",
              "\n",
              ".xr-section-summary-in:checked ~ .xr-section-details {\n",
              "  display: contents;\n",
              "}\n",
              "\n",
              ".xr-array-wrap {\n",
              "  grid-column: 1 / -1;\n",
              "  display: grid;\n",
              "  grid-template-columns: 20px auto;\n",
              "}\n",
              "\n",
              ".xr-array-wrap > label {\n",
              "  grid-column: 1;\n",
              "  vertical-align: top;\n",
              "}\n",
              "\n",
              ".xr-preview {\n",
              "  color: var(--xr-font-color3);\n",
              "}\n",
              "\n",
              ".xr-array-preview,\n",
              ".xr-array-data {\n",
              "  padding: 0 5px !important;\n",
              "  grid-column: 2;\n",
              "}\n",
              "\n",
              ".xr-array-data,\n",
              ".xr-array-in:checked ~ .xr-array-preview {\n",
              "  display: none;\n",
              "}\n",
              "\n",
              ".xr-array-in:checked ~ .xr-array-data,\n",
              ".xr-array-preview {\n",
              "  display: inline-block;\n",
              "}\n",
              "\n",
              ".xr-dim-list {\n",
              "  display: inline-block !important;\n",
              "  list-style: none;\n",
              "  padding: 0 !important;\n",
              "  margin: 0;\n",
              "}\n",
              "\n",
              ".xr-dim-list li {\n",
              "  display: inline-block;\n",
              "  padding: 0;\n",
              "  margin: 0;\n",
              "}\n",
              "\n",
              ".xr-dim-list:before {\n",
              "  content: \"(\";\n",
              "}\n",
              "\n",
              ".xr-dim-list:after {\n",
              "  content: \")\";\n",
              "}\n",
              "\n",
              ".xr-dim-list li:not(:last-child):after {\n",
              "  content: \",\";\n",
              "  padding-right: 5px;\n",
              "}\n",
              "\n",
              ".xr-has-index {\n",
              "  font-weight: bold;\n",
              "}\n",
              "\n",
              ".xr-var-list,\n",
              ".xr-var-item {\n",
              "  display: contents;\n",
              "}\n",
              "\n",
              ".xr-var-item > div,\n",
              ".xr-var-item label,\n",
              ".xr-var-item > .xr-var-name span {\n",
              "  background-color: var(--xr-background-color-row-even);\n",
              "  margin-bottom: 0;\n",
              "}\n",
              "\n",
              ".xr-var-item > .xr-var-name:hover span {\n",
              "  padding-right: 5px;\n",
              "}\n",
              "\n",
              ".xr-var-list > li:nth-child(odd) > div,\n",
              ".xr-var-list > li:nth-child(odd) > label,\n",
              ".xr-var-list > li:nth-child(odd) > .xr-var-name span {\n",
              "  background-color: var(--xr-background-color-row-odd);\n",
              "}\n",
              "\n",
              ".xr-var-name {\n",
              "  grid-column: 1;\n",
              "}\n",
              "\n",
              ".xr-var-dims {\n",
              "  grid-column: 2;\n",
              "}\n",
              "\n",
              ".xr-var-dtype {\n",
              "  grid-column: 3;\n",
              "  text-align: right;\n",
              "  color: var(--xr-font-color2);\n",
              "}\n",
              "\n",
              ".xr-var-preview {\n",
              "  grid-column: 4;\n",
              "}\n",
              "\n",
              ".xr-index-preview {\n",
              "  grid-column: 2 / 5;\n",
              "  color: var(--xr-font-color2);\n",
              "}\n",
              "\n",
              ".xr-var-name,\n",
              ".xr-var-dims,\n",
              ".xr-var-dtype,\n",
              ".xr-preview,\n",
              ".xr-attrs dt {\n",
              "  white-space: nowrap;\n",
              "  overflow: hidden;\n",
              "  text-overflow: ellipsis;\n",
              "  padding-right: 10px;\n",
              "}\n",
              "\n",
              ".xr-var-name:hover,\n",
              ".xr-var-dims:hover,\n",
              ".xr-var-dtype:hover,\n",
              ".xr-attrs dt:hover {\n",
              "  overflow: visible;\n",
              "  width: auto;\n",
              "  z-index: 1;\n",
              "}\n",
              "\n",
              ".xr-var-attrs,\n",
              ".xr-var-data,\n",
              ".xr-index-data {\n",
              "  display: none;\n",
              "  background-color: var(--xr-background-color) !important;\n",
              "  padding-bottom: 5px !important;\n",
              "}\n",
              "\n",
              ".xr-var-attrs-in:checked ~ .xr-var-attrs,\n",
              ".xr-var-data-in:checked ~ .xr-var-data,\n",
              ".xr-index-data-in:checked ~ .xr-index-data {\n",
              "  display: block;\n",
              "}\n",
              "\n",
              ".xr-var-data > table {\n",
              "  float: right;\n",
              "}\n",
              "\n",
              ".xr-var-name span,\n",
              ".xr-var-data,\n",
              ".xr-index-name div,\n",
              ".xr-index-data,\n",
              ".xr-attrs {\n",
              "  padding-left: 25px !important;\n",
              "}\n",
              "\n",
              ".xr-attrs,\n",
              ".xr-var-attrs,\n",
              ".xr-var-data,\n",
              ".xr-index-data {\n",
              "  grid-column: 1 / -1;\n",
              "}\n",
              "\n",
              "dl.xr-attrs {\n",
              "  padding: 0;\n",
              "  margin: 0;\n",
              "  display: grid;\n",
              "  grid-template-columns: 125px auto;\n",
              "}\n",
              "\n",
              ".xr-attrs dt,\n",
              ".xr-attrs dd {\n",
              "  padding: 0;\n",
              "  margin: 0;\n",
              "  float: left;\n",
              "  padding-right: 10px;\n",
              "  width: auto;\n",
              "}\n",
              "\n",
              ".xr-attrs dt {\n",
              "  font-weight: normal;\n",
              "  grid-column: 1;\n",
              "}\n",
              "\n",
              ".xr-attrs dt:hover span {\n",
              "  display: inline-block;\n",
              "  background: var(--xr-background-color);\n",
              "  padding-right: 10px;\n",
              "}\n",
              "\n",
              ".xr-attrs dd {\n",
              "  grid-column: 2;\n",
              "  white-space: pre-wrap;\n",
              "  word-break: break-all;\n",
              "}\n",
              "\n",
              ".xr-icon-database,\n",
              ".xr-icon-file-text2,\n",
              ".xr-no-icon {\n",
              "  display: inline-block;\n",
              "  vertical-align: middle;\n",
              "  width: 1em;\n",
              "  height: 1.5em !important;\n",
              "  stroke-width: 0;\n",
              "  stroke: currentColor;\n",
              "  fill: currentColor;\n",
              "}\n",
              "</style><pre class='xr-text-repr-fallback'>&lt;xarray.Dataset&gt; Size: 3GB\n",
              "Dimensions:      (y: 15298, x: 26835)\n",
              "Coordinates:\n",
              "  * y            (y) float64 122kB 8.292e+06 8.292e+06 ... 8.445e+06 8.445e+06\n",
              "  * x            (x) float64 215kB 5.189e+05 5.189e+05 ... 7.872e+05 7.872e+05\n",
              "    spatial_ref  int32 4B 32758\n",
              "Data variables:\n",
              "    vh           (y, x) float32 2GB dask.array&lt;chunksize=(512, 512), meta=np.ndarray&gt;\n",
              "    vv           (y, x) float32 2GB dask.array&lt;chunksize=(512, 512), meta=np.ndarray&gt;</pre><div class='xr-wrap' style='display:none'><div class='xr-header'><div class='xr-obj-type'>xarray.Dataset</div></div><ul class='xr-sections'><li class='xr-section-item'><input id='section-05ea4462-ab6e-4450-8c49-d402ed123ef9' class='xr-section-summary-in' type='checkbox' disabled ><label for='section-05ea4462-ab6e-4450-8c49-d402ed123ef9' class='xr-section-summary'  title='Expand/collapse section'>Dimensions:</label><div class='xr-section-inline-details'><ul class='xr-dim-list'><li><span class='xr-has-index'>y</span>: 15298</li><li><span class='xr-has-index'>x</span>: 26835</li></ul></div><div class='xr-section-details'></div></li><li class='xr-section-item'><input id='section-f19b4fd2-f744-4b57-932b-80afc4358864' class='xr-section-summary-in' type='checkbox'  checked><label for='section-f19b4fd2-f744-4b57-932b-80afc4358864' class='xr-section-summary' >Coordinates: <span>(3)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><ul class='xr-var-list'><li class='xr-var-item'><div class='xr-var-name'><span class='xr-has-index'>y</span></div><div class='xr-var-dims'>(y)</div><div class='xr-var-dtype'>float64</div><div class='xr-var-preview xr-preview'>8.292e+06 8.292e+06 ... 8.445e+06</div><input id='attrs-deeb5b05-fb6b-4742-908b-07d58944cde4' class='xr-var-attrs-in' type='checkbox' ><label for='attrs-deeb5b05-fb6b-4742-908b-07d58944cde4' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-4d02b595-8cf4-4eaa-a4b5-f6ead4734caf' class='xr-var-data-in' type='checkbox'><label for='data-4d02b595-8cf4-4eaa-a4b5-f6ead4734caf' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'><dt><span>units :</span></dt><dd>metre</dd><dt><span>resolution :</span></dt><dd>10.0</dd><dt><span>crs :</span></dt><dd>EPSG:32758</dd></dl></div><div class='xr-var-data'><pre>array([8292485.769396, 8292495.769396, 8292505.769396, ..., 8445435.769396,\n",
              "       8445445.769396, 8445455.769396])</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span class='xr-has-index'>x</span></div><div class='xr-var-dims'>(x)</div><div class='xr-var-dtype'>float64</div><div class='xr-var-preview xr-preview'>5.189e+05 5.189e+05 ... 7.872e+05</div><input id='attrs-8c3f79ac-bbf5-4c54-93a8-c71aa0a04dfb' class='xr-var-attrs-in' type='checkbox' ><label for='attrs-8c3f79ac-bbf5-4c54-93a8-c71aa0a04dfb' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-d2bc2151-4198-41e4-8f71-2d3f16d26b89' class='xr-var-data-in' type='checkbox'><label for='data-d2bc2151-4198-41e4-8f71-2d3f16d26b89' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'><dt><span>units :</span></dt><dd>metre</dd><dt><span>resolution :</span></dt><dd>10.0</dd><dt><span>crs :</span></dt><dd>EPSG:32758</dd></dl></div><div class='xr-var-data'><pre>array([518904.374867, 518914.374867, 518924.374867, ..., 787224.374867,\n",
              "       787234.374867, 787244.374867])</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span>spatial_ref</span></div><div class='xr-var-dims'>()</div><div class='xr-var-dtype'>int32</div><div class='xr-var-preview xr-preview'>32758</div><input id='attrs-1f85804b-d165-4441-9ab9-efe76e720434' class='xr-var-attrs-in' type='checkbox' ><label for='attrs-1f85804b-d165-4441-9ab9-efe76e720434' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-afc15320-b85e-44f8-85dd-aec886501844' class='xr-var-data-in' type='checkbox'><label for='data-afc15320-b85e-44f8-85dd-aec886501844' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'><dt><span>spatial_ref :</span></dt><dd>PROJCRS[&quot;WGS 84 / UTM zone 58S&quot;,BASEGEOGCRS[&quot;WGS 84&quot;,ENSEMBLE[&quot;World Geodetic System 1984 ensemble&quot;,MEMBER[&quot;World Geodetic System 1984 (Transit)&quot;],MEMBER[&quot;World Geodetic System 1984 (G730)&quot;],MEMBER[&quot;World Geodetic System 1984 (G873)&quot;],MEMBER[&quot;World Geodetic System 1984 (G1150)&quot;],MEMBER[&quot;World Geodetic System 1984 (G1674)&quot;],MEMBER[&quot;World Geodetic System 1984 (G1762)&quot;],MEMBER[&quot;World Geodetic System 1984 (G2139)&quot;],MEMBER[&quot;World Geodetic System 1984 (G2296)&quot;],ELLIPSOID[&quot;WGS 84&quot;,6378137,298.257223563,LENGTHUNIT[&quot;metre&quot;,1]],ENSEMBLEACCURACY[2.0]],PRIMEM[&quot;Greenwich&quot;,0,ANGLEUNIT[&quot;degree&quot;,0.0174532925199433]],ID[&quot;EPSG&quot;,4326]],CONVERSION[&quot;UTM zone 58S&quot;,METHOD[&quot;Transverse Mercator&quot;,ID[&quot;EPSG&quot;,9807]],PARAMETER[&quot;Latitude of natural origin&quot;,0,ANGLEUNIT[&quot;degree&quot;,0.0174532925199433],ID[&quot;EPSG&quot;,8801]],PARAMETER[&quot;Longitude of natural origin&quot;,165,ANGLEUNIT[&quot;degree&quot;,0.0174532925199433],ID[&quot;EPSG&quot;,8802]],PARAMETER[&quot;Scale factor at natural origin&quot;,0.9996,SCALEUNIT[&quot;unity&quot;,1],ID[&quot;EPSG&quot;,8805]],PARAMETER[&quot;False easting&quot;,500000,LENGTHUNIT[&quot;metre&quot;,1],ID[&quot;EPSG&quot;,8806]],PARAMETER[&quot;False northing&quot;,10000000,LENGTHUNIT[&quot;metre&quot;,1],ID[&quot;EPSG&quot;,8807]]],CS[Cartesian,2],AXIS[&quot;(E)&quot;,east,ORDER[1],LENGTHUNIT[&quot;metre&quot;,1]],AXIS[&quot;(N)&quot;,north,ORDER[2],LENGTHUNIT[&quot;metre&quot;,1]],USAGE[SCOPE[&quot;Navigation and medium accuracy spatial referencing.&quot;],AREA[&quot;Between 162°E and 168°E, southern hemisphere between 80°S and equator, onshore and offshore.&quot;],BBOX[-80,162,0,168]],ID[&quot;EPSG&quot;,32758]]</dd><dt><span>crs_wkt :</span></dt><dd>PROJCRS[&quot;WGS 84 / UTM zone 58S&quot;,BASEGEOGCRS[&quot;WGS 84&quot;,ENSEMBLE[&quot;World Geodetic System 1984 ensemble&quot;,MEMBER[&quot;World Geodetic System 1984 (Transit)&quot;],MEMBER[&quot;World Geodetic System 1984 (G730)&quot;],MEMBER[&quot;World Geodetic System 1984 (G873)&quot;],MEMBER[&quot;World Geodetic System 1984 (G1150)&quot;],MEMBER[&quot;World Geodetic System 1984 (G1674)&quot;],MEMBER[&quot;World Geodetic System 1984 (G1762)&quot;],MEMBER[&quot;World Geodetic System 1984 (G2139)&quot;],MEMBER[&quot;World Geodetic System 1984 (G2296)&quot;],ELLIPSOID[&quot;WGS 84&quot;,6378137,298.257223563,LENGTHUNIT[&quot;metre&quot;,1]],ENSEMBLEACCURACY[2.0]],PRIMEM[&quot;Greenwich&quot;,0,ANGLEUNIT[&quot;degree&quot;,0.0174532925199433]],ID[&quot;EPSG&quot;,4326]],CONVERSION[&quot;UTM zone 58S&quot;,METHOD[&quot;Transverse Mercator&quot;,ID[&quot;EPSG&quot;,9807]],PARAMETER[&quot;Latitude of natural origin&quot;,0,ANGLEUNIT[&quot;degree&quot;,0.0174532925199433],ID[&quot;EPSG&quot;,8801]],PARAMETER[&quot;Longitude of natural origin&quot;,165,ANGLEUNIT[&quot;degree&quot;,0.0174532925199433],ID[&quot;EPSG&quot;,8802]],PARAMETER[&quot;Scale factor at natural origin&quot;,0.9996,SCALEUNIT[&quot;unity&quot;,1],ID[&quot;EPSG&quot;,8805]],PARAMETER[&quot;False easting&quot;,500000,LENGTHUNIT[&quot;metre&quot;,1],ID[&quot;EPSG&quot;,8806]],PARAMETER[&quot;False northing&quot;,10000000,LENGTHUNIT[&quot;metre&quot;,1],ID[&quot;EPSG&quot;,8807]]],CS[Cartesian,2],AXIS[&quot;(E)&quot;,east,ORDER[1],LENGTHUNIT[&quot;metre&quot;,1]],AXIS[&quot;(N)&quot;,north,ORDER[2],LENGTHUNIT[&quot;metre&quot;,1]],USAGE[SCOPE[&quot;Navigation and medium accuracy spatial referencing.&quot;],AREA[&quot;Between 162°E and 168°E, southern hemisphere between 80°S and equator, onshore and offshore.&quot;],BBOX[-80,162,0,168]],ID[&quot;EPSG&quot;,32758]]</dd><dt><span>semi_major_axis :</span></dt><dd>6378137.0</dd><dt><span>semi_minor_axis :</span></dt><dd>6356752.314245179</dd><dt><span>inverse_flattening :</span></dt><dd>298.257223563</dd><dt><span>reference_ellipsoid_name :</span></dt><dd>WGS 84</dd><dt><span>longitude_of_prime_meridian :</span></dt><dd>0.0</dd><dt><span>prime_meridian_name :</span></dt><dd>Greenwich</dd><dt><span>geographic_crs_name :</span></dt><dd>WGS 84</dd><dt><span>horizontal_datum_name :</span></dt><dd>World Geodetic System 1984 ensemble</dd><dt><span>projected_crs_name :</span></dt><dd>WGS 84 / UTM zone 58S</dd><dt><span>grid_mapping_name :</span></dt><dd>transverse_mercator</dd><dt><span>latitude_of_projection_origin :</span></dt><dd>0.0</dd><dt><span>longitude_of_central_meridian :</span></dt><dd>165.0</dd><dt><span>false_easting :</span></dt><dd>500000.0</dd><dt><span>false_northing :</span></dt><dd>10000000.0</dd><dt><span>scale_factor_at_central_meridian :</span></dt><dd>0.9996</dd><dt><span>GeoTransform :</span></dt><dd>518899.374867063772398978471756 10 0 8292480.769396109506487846374512 0 10</dd></dl></div><div class='xr-var-data'><pre>array(32758, dtype=int32)</pre></div></li></ul></div></li><li class='xr-section-item'><input id='section-3fe7e052-5b01-4b8b-b2b1-927f0419388c' class='xr-section-summary-in' type='checkbox'  checked><label for='section-3fe7e052-5b01-4b8b-b2b1-927f0419388c' class='xr-section-summary' >Data variables: <span>(2)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><ul class='xr-var-list'><li class='xr-var-item'><div class='xr-var-name'><span>vh</span></div><div class='xr-var-dims'>(y, x)</div><div class='xr-var-dtype'>float32</div><div class='xr-var-preview xr-preview'>dask.array&lt;chunksize=(512, 512), meta=np.ndarray&gt;</div><input id='attrs-864d4478-1365-4450-98a4-26979c435d78' class='xr-var-attrs-in' type='checkbox' disabled><label for='attrs-864d4478-1365-4450-98a4-26979c435d78' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-dd3a4144-4157-4552-8cc5-7583561e1d43' class='xr-var-data-in' type='checkbox'><label for='data-dd3a4144-4157-4552-8cc5-7583561e1d43' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'></dl></div><div class='xr-var-data'><table>\n",
              "    <tr>\n",
              "        <td>\n",
              "            <table style=\"border-collapse: collapse;\">\n",
              "                <thead>\n",
              "                    <tr>\n",
              "                        <td> </td>\n",
              "                        <th> Array </th>\n",
              "                        <th> Chunk </th>\n",
              "                    </tr>\n",
              "                </thead>\n",
              "                <tbody>\n",
              "                    \n",
              "                    <tr>\n",
              "                        <th> Bytes </th>\n",
              "                        <td> 1.53 GiB </td>\n",
              "                        <td> 1.00 MiB </td>\n",
              "                    </tr>\n",
              "                    \n",
              "                    <tr>\n",
              "                        <th> Shape </th>\n",
              "                        <td> (15298, 26835) </td>\n",
              "                        <td> (512, 512) </td>\n",
              "                    </tr>\n",
              "                    <tr>\n",
              "                        <th> Dask graph </th>\n",
              "                        <td colspan=\"2\"> 1590 chunks in 5 graph layers </td>\n",
              "                    </tr>\n",
              "                    <tr>\n",
              "                        <th> Data type </th>\n",
              "                        <td colspan=\"2\"> float32 numpy.ndarray </td>\n",
              "                    </tr>\n",
              "                </tbody>\n",
              "            </table>\n",
              "        </td>\n",
              "        <td>\n",
              "        <svg width=\"170\" height=\"118\" style=\"stroke:rgb(0,0,0);stroke-width:1\" >\n",
              "\n",
              "  <!-- Horizontal lines -->\n",
              "  <line x1=\"0\" y1=\"0\" x2=\"120\" y2=\"0\" style=\"stroke-width:2\" />\n",
              "  <line x1=\"0\" y1=\"2\" x2=\"120\" y2=\"2\" />\n",
              "  <line x1=\"0\" y1=\"6\" x2=\"120\" y2=\"6\" />\n",
              "  <line x1=\"0\" y1=\"9\" x2=\"120\" y2=\"9\" />\n",
              "  <line x1=\"0\" y1=\"13\" x2=\"120\" y2=\"13\" />\n",
              "  <line x1=\"0\" y1=\"16\" x2=\"120\" y2=\"16\" />\n",
              "  <line x1=\"0\" y1=\"20\" x2=\"120\" y2=\"20\" />\n",
              "  <line x1=\"0\" y1=\"25\" x2=\"120\" y2=\"25\" />\n",
              "  <line x1=\"0\" y1=\"27\" x2=\"120\" y2=\"27\" />\n",
              "  <line x1=\"0\" y1=\"32\" x2=\"120\" y2=\"32\" />\n",
              "  <line x1=\"0\" y1=\"34\" x2=\"120\" y2=\"34\" />\n",
              "  <line x1=\"0\" y1=\"38\" x2=\"120\" y2=\"38\" />\n",
              "  <line x1=\"0\" y1=\"41\" x2=\"120\" y2=\"41\" />\n",
              "  <line x1=\"0\" y1=\"45\" x2=\"120\" y2=\"45\" />\n",
              "  <line x1=\"0\" y1=\"50\" x2=\"120\" y2=\"50\" />\n",
              "  <line x1=\"0\" y1=\"52\" x2=\"120\" y2=\"52\" />\n",
              "  <line x1=\"0\" y1=\"57\" x2=\"120\" y2=\"57\" />\n",
              "  <line x1=\"0\" y1=\"59\" x2=\"120\" y2=\"59\" />\n",
              "  <line x1=\"0\" y1=\"64\" x2=\"120\" y2=\"64\" />\n",
              "  <line x1=\"0\" y1=\"68\" x2=\"120\" y2=\"68\" style=\"stroke-width:2\" />\n",
              "\n",
              "  <!-- Vertical lines -->\n",
              "  <line x1=\"0\" y1=\"0\" x2=\"0\" y2=\"68\" style=\"stroke-width:2\" />\n",
              "  <line x1=\"4\" y1=\"0\" x2=\"4\" y2=\"68\" />\n",
              "  <line x1=\"11\" y1=\"0\" x2=\"11\" y2=\"68\" />\n",
              "  <line x1=\"18\" y1=\"0\" x2=\"18\" y2=\"68\" />\n",
              "  <line x1=\"25\" y1=\"0\" x2=\"25\" y2=\"68\" />\n",
              "  <line x1=\"29\" y1=\"0\" x2=\"29\" y2=\"68\" />\n",
              "  <line x1=\"36\" y1=\"0\" x2=\"36\" y2=\"68\" />\n",
              "  <line x1=\"43\" y1=\"0\" x2=\"43\" y2=\"68\" />\n",
              "  <line x1=\"50\" y1=\"0\" x2=\"50\" y2=\"68\" />\n",
              "  <line x1=\"57\" y1=\"0\" x2=\"57\" y2=\"68\" />\n",
              "  <line x1=\"61\" y1=\"0\" x2=\"61\" y2=\"68\" />\n",
              "  <line x1=\"68\" y1=\"0\" x2=\"68\" y2=\"68\" />\n",
              "  <line x1=\"75\" y1=\"0\" x2=\"75\" y2=\"68\" />\n",
              "  <line x1=\"82\" y1=\"0\" x2=\"82\" y2=\"68\" />\n",
              "  <line x1=\"89\" y1=\"0\" x2=\"89\" y2=\"68\" />\n",
              "  <line x1=\"93\" y1=\"0\" x2=\"93\" y2=\"68\" />\n",
              "  <line x1=\"100\" y1=\"0\" x2=\"100\" y2=\"68\" />\n",
              "  <line x1=\"107\" y1=\"0\" x2=\"107\" y2=\"68\" />\n",
              "  <line x1=\"114\" y1=\"0\" x2=\"114\" y2=\"68\" />\n",
              "  <line x1=\"120\" y1=\"0\" x2=\"120\" y2=\"68\" style=\"stroke-width:2\" />\n",
              "\n",
              "  <!-- Colored Rectangle -->\n",
              "  <polygon points=\"0.0,0.0 120.0,0.0 120.0,68.40916713247624 0.0,68.40916713247624\" style=\"fill:#8B4903A0;stroke-width:0\"/>\n",
              "\n",
              "  <!-- Text -->\n",
              "  <text x=\"60.000000\" y=\"88.409167\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" >26835</text>\n",
              "  <text x=\"140.000000\" y=\"34.204584\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" transform=\"rotate(-90,140.000000,34.204584)\">15298</text>\n",
              "</svg>\n",
              "        </td>\n",
              "    </tr>\n",
              "</table></div></li><li class='xr-var-item'><div class='xr-var-name'><span>vv</span></div><div class='xr-var-dims'>(y, x)</div><div class='xr-var-dtype'>float32</div><div class='xr-var-preview xr-preview'>dask.array&lt;chunksize=(512, 512), meta=np.ndarray&gt;</div><input id='attrs-32ab1a12-f7bc-4193-a42a-dc0203c84a71' class='xr-var-attrs-in' type='checkbox' disabled><label for='attrs-32ab1a12-f7bc-4193-a42a-dc0203c84a71' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-5f23783d-1d94-4bba-8282-54a2fa1d0756' class='xr-var-data-in' type='checkbox'><label for='data-5f23783d-1d94-4bba-8282-54a2fa1d0756' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'></dl></div><div class='xr-var-data'><table>\n",
              "    <tr>\n",
              "        <td>\n",
              "            <table style=\"border-collapse: collapse;\">\n",
              "                <thead>\n",
              "                    <tr>\n",
              "                        <td> </td>\n",
              "                        <th> Array </th>\n",
              "                        <th> Chunk </th>\n",
              "                    </tr>\n",
              "                </thead>\n",
              "                <tbody>\n",
              "                    \n",
              "                    <tr>\n",
              "                        <th> Bytes </th>\n",
              "                        <td> 1.53 GiB </td>\n",
              "                        <td> 1.00 MiB </td>\n",
              "                    </tr>\n",
              "                    \n",
              "                    <tr>\n",
              "                        <th> Shape </th>\n",
              "                        <td> (15298, 26835) </td>\n",
              "                        <td> (512, 512) </td>\n",
              "                    </tr>\n",
              "                    <tr>\n",
              "                        <th> Dask graph </th>\n",
              "                        <td colspan=\"2\"> 1590 chunks in 5 graph layers </td>\n",
              "                    </tr>\n",
              "                    <tr>\n",
              "                        <th> Data type </th>\n",
              "                        <td colspan=\"2\"> float32 numpy.ndarray </td>\n",
              "                    </tr>\n",
              "                </tbody>\n",
              "            </table>\n",
              "        </td>\n",
              "        <td>\n",
              "        <svg width=\"170\" height=\"118\" style=\"stroke:rgb(0,0,0);stroke-width:1\" >\n",
              "\n",
              "  <!-- Horizontal lines -->\n",
              "  <line x1=\"0\" y1=\"0\" x2=\"120\" y2=\"0\" style=\"stroke-width:2\" />\n",
              "  <line x1=\"0\" y1=\"2\" x2=\"120\" y2=\"2\" />\n",
              "  <line x1=\"0\" y1=\"6\" x2=\"120\" y2=\"6\" />\n",
              "  <line x1=\"0\" y1=\"9\" x2=\"120\" y2=\"9\" />\n",
              "  <line x1=\"0\" y1=\"13\" x2=\"120\" y2=\"13\" />\n",
              "  <line x1=\"0\" y1=\"16\" x2=\"120\" y2=\"16\" />\n",
              "  <line x1=\"0\" y1=\"20\" x2=\"120\" y2=\"20\" />\n",
              "  <line x1=\"0\" y1=\"25\" x2=\"120\" y2=\"25\" />\n",
              "  <line x1=\"0\" y1=\"27\" x2=\"120\" y2=\"27\" />\n",
              "  <line x1=\"0\" y1=\"32\" x2=\"120\" y2=\"32\" />\n",
              "  <line x1=\"0\" y1=\"34\" x2=\"120\" y2=\"34\" />\n",
              "  <line x1=\"0\" y1=\"38\" x2=\"120\" y2=\"38\" />\n",
              "  <line x1=\"0\" y1=\"41\" x2=\"120\" y2=\"41\" />\n",
              "  <line x1=\"0\" y1=\"45\" x2=\"120\" y2=\"45\" />\n",
              "  <line x1=\"0\" y1=\"50\" x2=\"120\" y2=\"50\" />\n",
              "  <line x1=\"0\" y1=\"52\" x2=\"120\" y2=\"52\" />\n",
              "  <line x1=\"0\" y1=\"57\" x2=\"120\" y2=\"57\" />\n",
              "  <line x1=\"0\" y1=\"59\" x2=\"120\" y2=\"59\" />\n",
              "  <line x1=\"0\" y1=\"64\" x2=\"120\" y2=\"64\" />\n",
              "  <line x1=\"0\" y1=\"68\" x2=\"120\" y2=\"68\" style=\"stroke-width:2\" />\n",
              "\n",
              "  <!-- Vertical lines -->\n",
              "  <line x1=\"0\" y1=\"0\" x2=\"0\" y2=\"68\" style=\"stroke-width:2\" />\n",
              "  <line x1=\"4\" y1=\"0\" x2=\"4\" y2=\"68\" />\n",
              "  <line x1=\"11\" y1=\"0\" x2=\"11\" y2=\"68\" />\n",
              "  <line x1=\"18\" y1=\"0\" x2=\"18\" y2=\"68\" />\n",
              "  <line x1=\"25\" y1=\"0\" x2=\"25\" y2=\"68\" />\n",
              "  <line x1=\"29\" y1=\"0\" x2=\"29\" y2=\"68\" />\n",
              "  <line x1=\"36\" y1=\"0\" x2=\"36\" y2=\"68\" />\n",
              "  <line x1=\"43\" y1=\"0\" x2=\"43\" y2=\"68\" />\n",
              "  <line x1=\"50\" y1=\"0\" x2=\"50\" y2=\"68\" />\n",
              "  <line x1=\"57\" y1=\"0\" x2=\"57\" y2=\"68\" />\n",
              "  <line x1=\"61\" y1=\"0\" x2=\"61\" y2=\"68\" />\n",
              "  <line x1=\"68\" y1=\"0\" x2=\"68\" y2=\"68\" />\n",
              "  <line x1=\"75\" y1=\"0\" x2=\"75\" y2=\"68\" />\n",
              "  <line x1=\"82\" y1=\"0\" x2=\"82\" y2=\"68\" />\n",
              "  <line x1=\"89\" y1=\"0\" x2=\"89\" y2=\"68\" />\n",
              "  <line x1=\"93\" y1=\"0\" x2=\"93\" y2=\"68\" />\n",
              "  <line x1=\"100\" y1=\"0\" x2=\"100\" y2=\"68\" />\n",
              "  <line x1=\"107\" y1=\"0\" x2=\"107\" y2=\"68\" />\n",
              "  <line x1=\"114\" y1=\"0\" x2=\"114\" y2=\"68\" />\n",
              "  <line x1=\"120\" y1=\"0\" x2=\"120\" y2=\"68\" style=\"stroke-width:2\" />\n",
              "\n",
              "  <!-- Colored Rectangle -->\n",
              "  <polygon points=\"0.0,0.0 120.0,0.0 120.0,68.40916713247624 0.0,68.40916713247624\" style=\"fill:#8B4903A0;stroke-width:0\"/>\n",
              "\n",
              "  <!-- Text -->\n",
              "  <text x=\"60.000000\" y=\"88.409167\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" >26835</text>\n",
              "  <text x=\"140.000000\" y=\"34.204584\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" transform=\"rotate(-90,140.000000,34.204584)\">15298</text>\n",
              "</svg>\n",
              "        </td>\n",
              "    </tr>\n",
              "</table></div></li></ul></div></li><li class='xr-section-item'><input id='section-20d31d1c-24c4-4b63-b3ff-4dc25b92818d' class='xr-section-summary-in' type='checkbox'  ><label for='section-20d31d1c-24c4-4b63-b3ff-4dc25b92818d' class='xr-section-summary' >Indexes: <span>(2)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><ul class='xr-var-list'><li class='xr-var-item'><div class='xr-index-name'><div>y</div></div><div class='xr-index-preview'>PandasIndex</div><input type='checkbox' disabled/><label></label><input id='index-c0df6ff2-88db-4c3f-8db1-887c4e3007c5' class='xr-index-data-in' type='checkbox'/><label for='index-c0df6ff2-88db-4c3f-8db1-887c4e3007c5' title='Show/Hide index repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-index-data'><pre>PandasIndex(Index([8292485.7693961095, 8292495.7693961095, 8292505.7693961095,\n",
              "       8292515.7693961095, 8292525.7693961095, 8292535.7693961095,\n",
              "       8292545.7693961095, 8292555.7693961095, 8292565.7693961095,\n",
              "       8292575.7693961095,\n",
              "       ...\n",
              "         8445365.76939611,   8445375.76939611,   8445385.76939611,\n",
              "         8445395.76939611,   8445405.76939611,   8445415.76939611,\n",
              "         8445425.76939611,   8445435.76939611,   8445445.76939611,\n",
              "         8445455.76939611],\n",
              "      dtype=&#x27;float64&#x27;, name=&#x27;y&#x27;, length=15298))</pre></div></li><li class='xr-var-item'><div class='xr-index-name'><div>x</div></div><div class='xr-index-preview'>PandasIndex</div><input type='checkbox' disabled/><label></label><input id='index-983e6abf-fef4-441e-ad1f-2e0aab41121f' class='xr-index-data-in' type='checkbox'/><label for='index-983e6abf-fef4-441e-ad1f-2e0aab41121f' title='Show/Hide index repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-index-data'><pre>PandasIndex(Index([518904.3748670638, 518914.3748670638, 518924.3748670638,\n",
              "       518934.3748670638, 518944.3748670638, 518954.3748670638,\n",
              "       518964.3748670638, 518974.3748670638, 518984.3748670638,\n",
              "       518994.3748670638,\n",
              "       ...\n",
              "       787154.3748670637, 787164.3748670637, 787174.3748670637,\n",
              "       787184.3748670637, 787194.3748670637, 787204.3748670637,\n",
              "       787214.3748670637, 787224.3748670637, 787234.3748670637,\n",
              "       787244.3748670637],\n",
              "      dtype=&#x27;float64&#x27;, name=&#x27;x&#x27;, length=26835))</pre></div></li></ul></div></li><li class='xr-section-item'><input id='section-218117c3-9b5f-4d46-9903-df7a2a909cd3' class='xr-section-summary-in' type='checkbox' disabled ><label for='section-218117c3-9b5f-4d46-9903-df7a2a909cd3' class='xr-section-summary'  title='Expand/collapse section'>Attributes: <span>(0)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><dl class='xr-attrs'></dl></div></li></ul></div></div>"
            ],
            "text/plain": [
              "<xarray.Dataset> Size: 3GB\n",
              "Dimensions:      (y: 15298, x: 26835)\n",
              "Coordinates:\n",
              "  * y            (y) float64 122kB 8.292e+06 8.292e+06 ... 8.445e+06 8.445e+06\n",
              "  * x            (x) float64 215kB 5.189e+05 5.189e+05 ... 7.872e+05 7.872e+05\n",
              "    spatial_ref  int32 4B 32758\n",
              "Data variables:\n",
              "    vh           (y, x) float32 2GB dask.array<chunksize=(512, 512), meta=np.ndarray>\n",
              "    vv           (y, x) float32 2GB dask.array<chunksize=(512, 512), meta=np.ndarray>"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vanuatu_s1_imagery[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Tile the images per boundary into 224x224 for input to the UNet-ResNet model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "EMSC9hDbEO8M"
      },
      "outputs": [],
      "source": [
        "def tile_and_preprocess(xr_dataset, tile_size=224, stride=224):\n",
        "    \"\"\"\n",
        "    Splits a large Sentinel-1 xarray dataset into smaller 224x224 tiles, ignoring edge tiles that are too small.\n",
        "    \"\"\"\n",
        "    if xr_dataset is not None:\n",
        "        # Extract VV and VH bands\n",
        "        vv = xr_dataset[\"vv\"].values  # (H, W)\n",
        "        vh = xr_dataset[\"vh\"].values  # (H, W)\n",
        "        \n",
        "        # Normalize the channels\n",
        "        vv = (vv - np.min(vv)) / (np.max(vv) - np.min(vv))\n",
        "        vh = (vh - np.min(vh)) / (np.max(vh) - np.min(vh))\n",
        "\n",
        "        # Stack into a 2-channel image (C, H, W)\n",
        "        full_image = np.stack([vv, vh], axis=0)  # Shape (2, H, W)\n",
        "\n",
        "        # Image dimensions\n",
        "        _, H, W = full_image.shape\n",
        "\n",
        "        # Store tiles\n",
        "        tiles = []\n",
        "\n",
        "        # Slide over the image and extract tiles\n",
        "        for y in range(0, H - tile_size + 1, stride):  # Ensure tile stays within bounds\n",
        "            for x in range(0, W - tile_size + 1, stride):\n",
        "                tile = full_image[:, y:y+tile_size, x:x+tile_size]\n",
        "                tile_tensor = torch.tensor(tile, dtype=torch.float32).unsqueeze(0)  # Add batch dim\n",
        "                tiles.append(tile_tensor)\n",
        "\n",
        "        return tiles\n",
        "    else:\n",
        "        print(\"No Sentinel-1 data available for this region around the requested date.\")\n",
        "        return\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Run inference on the tiles per admin boundary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cEGEthGVGDVe",
        "outputId": "e6505774-5b04-4d56-88aa-c6b315006bcd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing region 0...\n"
          ]
        }
      ],
      "source": [
        "results = []\n",
        "\n",
        "for idx, s1_dataset in enumerate(vanuatu_s1_imagery):\n",
        "    print(f\"Processing region {idx}...\")\n",
        "\n",
        "    # Generate tiles\n",
        "    tiles = tile_and_preprocess(s1_dataset, tile_size=224, stride=224)\n",
        "\n",
        "    region_results = []\n",
        "\n",
        "    if tiles:\n",
        "        for tile in tiles:\n",
        "            tile = tile.to(device)\n",
        "            \n",
        "            # Run inference\n",
        "            with torch.no_grad():\n",
        "                output = model(tile)\n",
        "\n",
        "            # Convert output tensor to NumPy array\n",
        "            prediction = output.squeeze().cpu().numpy()\n",
        "            region_results.append(prediction)\n",
        "\n",
        "        results.append(region_results)\n",
        "        print(f\"Inference completed for region {idx}\")\n",
        "    else:\n",
        "        print(f\"No tiles generated for region {idx}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NYX2TzIPGRQ7"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
