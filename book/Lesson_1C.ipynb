{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0r_u53ftaEfA"
      },
      "source": [
        "# Segmentation workflow using PyTorch (U-Net/CNN) for flood and water mask detection\n",
        "\n",
        "1. **Data Acquisition**:\n",
        "   - Downloading the [Tropical and Sub-Tropical Flood and Water Masks dataset](https://pacificdata.org/data/dataset/tropical-and-sub-tropical-flood-and-water-masks).\n",
        "   - Fetching Sentinel-1 RTC (for SAR-based flood detection) and Sentinel-2 imagery from STAC for Vanuatu and the relevant surrounding region.\n",
        "\n",
        "2. **Preprocessing**:\n",
        "   - Aligning Sentinel-1 and/or Sentinel-2 data.\n",
        "   - Normalization, filtering, and masking.\n",
        "   - Preparing training data by pairing inputs (SAR and/or optical images) with water masks.\n",
        "\n",
        "3. **Model Definition**:\n",
        "   - Implementing a U-Net (CNN) in PyTorch for segmentation.\n",
        "\n",
        "4. **Training & Evaluation**:\n",
        "   - Splitting data into train/val/test sets.\n",
        "   - Data loading through batches.\n",
        "   - Loss function and optimizer.\n",
        "   - Model training loop.\n",
        "   - Performance evaluation with IoU and Accuracy Score.\n",
        "\n",
        "5. **Inference & Visualization**:\n",
        "   - Running the model on test images.\n",
        "   - Plotting predicted masks.\n",
        "\n",
        "This sets up a basic segmentation pipeline using U-Net and PyTorch, processing Sentinel-1/2 data for flood and water masks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e-2KJkUbLIsy",
        "outputId": "f848884e-4232-4be9-a09f-90063eaf7c1f"
      },
      "outputs": [],
      "source": [
        "cd ./vbos/flood/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Obtain the data [here](https://pacificdata.org/data/dataset/tropical-and-sub-tropical-flood-and-water-masks) by clicking \"Download all data (81.56 MB)\" and then unzip the folder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "TD6n23m2KbsW"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "mask_paths = glob.glob('tst-ground-truth-flood-masks/*.tif')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ydL4YnCNNVwS",
        "outputId": "6a698624-8bb5-4734-f1a8-192d865cd2a8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "513"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(mask_paths)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R4O5GdAYLlSK",
        "outputId": "823a5776-0242-45cd-e925-1df17434be81"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EMSR264_01AMBILOPE_DEL_v2_ground_truth.tif\n",
            "EMSR264_02AMBANJA_DEL_v2_ground_truth.tif\n",
            "EMSR264_05MAROVOAY_DEL_v2_ground_truth.tif\n",
            "EMSR264_06MAHAMBO_DEL_v2_ground_truth.tif\n",
            "EMSR264_07AMBATOBE_DEL_v2_ground_truth.tif\n",
            "EMSR264_08VATOMANDRY_DEL_v2_ground_truth.tif\n",
            "EMSR264_09MAHANORO_DEL_v1_ground_truth.tif\n",
            "EMSR264_10MASOMELOKA_DEL_v2_ground_truth.tif\n",
            "EMSR264_11NOSYVARIKA_DEL_v2_ground_truth.tif\n",
            "EMSR264_12FARAFANGANA_DEL_v2_ground_truth.tif\n"
          ]
        }
      ],
      "source": [
        "ls tst-ground-truth-flood-masks/ | head"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gznIBi2TKBAs",
        "outputId": "c4d4e138-11b9-499c-d012-53173f6128e2"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from datetime import datetime, timedelta\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset, random_split\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision import models\n",
        "from torchvision.models.resnet import resnet34\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import geopandas as gpd\n",
        "import rasterio\n",
        "import xarray as xr\n",
        "import rioxarray as rxr\n",
        "import planetary_computer\n",
        "import pystac_client\n",
        "import odc.stac\n",
        "import matplotlib.pyplot as plt\n",
        "from shapely.geometry import box\n",
        "from sklearn.metrics import jaccard_score, accuracy_score\n",
        "import dask\n",
        "import dask.array as da\n",
        "import dask.delayed\n",
        "\n",
        "\n",
        "# Define U-Net model with ResNet34 backbone\n",
        "class UNetWithResNetEncoder(nn.Module):\n",
        "    def __init__(self, in_channels=2, out_channels=4):\n",
        "        super().__init__()\n",
        "\n",
        "        # Load pre-trained ResNet34 backbone\n",
        "        resnet = models.resnet34(pretrained=True)\n",
        "\n",
        "        # Modify first convolution to accept 2 channels\n",
        "        self.encoder1 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, 64, kernel_size=7, stride=2, padding=3, bias=False),\n",
        "            resnet.bn1,\n",
        "            resnet.relu\n",
        "        )\n",
        "\n",
        "        # Encoder layers\n",
        "        self.encoder2 = resnet.layer1  # (B, 64, H/4, W/4)\n",
        "        self.encoder3 = resnet.layer2  # (B, 128, H/8, W/8)\n",
        "        self.encoder4 = resnet.layer3  # (B, 256, H/16, W/16)\n",
        "        self.encoder5 = resnet.layer4  # (B, 512, H/32, W/32)\n",
        "\n",
        "        # Bottleneck\n",
        "        self.bottleneck = nn.Conv2d(512, 1024, kernel_size=3, padding=1)\n",
        "\n",
        "        # Decoder with skip connections\n",
        "        self.upconv1 = self._upsample(1024, 512)\n",
        "        self.upconv2 = self._upsample(512 + 512, 256)  # Add encoder5 skip connection\n",
        "        self.upconv3 = self._upsample(256 + 256, 128)  # Add encoder4 skip connection\n",
        "        self.upconv4 = self._upsample(128 + 128, 64)   # Add encoder3 skip connection\n",
        "        self.upconv5 = self._upsample(64 + 64, 64)     # Add encoder2 skip connection\n",
        "\n",
        "        # Final segmentation output\n",
        "        self.final_conv = nn.Conv2d(64, out_channels, kernel_size=1)\n",
        "\n",
        "    def _upsample(self, in_channels, out_channels):\n",
        "        \"\"\"Helper function to create upsampling layers.\"\"\"\n",
        "        return nn.Sequential(\n",
        "            nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=True),\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Encoder path\n",
        "        enc1 = self.encoder1(x)   # (B, 64, 112, 112)\n",
        "        enc2 = self.encoder2(enc1) # (B, 64, 56, 56)\n",
        "        enc3 = self.encoder3(enc2) # (B, 128, 28, 28)\n",
        "        enc4 = self.encoder4(enc3) # (B, 256, 14, 14)\n",
        "        enc5 = self.encoder5(enc4) # (B, 512, 7, 7)\n",
        "\n",
        "        # Bottleneck\n",
        "        x = self.bottleneck(enc5)  # (B, 1024, 7, 7)\n",
        "        x = self.upconv1(x)        # (B, 512, 14, 14)\n",
        "\n",
        "        # Ensure enc5 matches x before concatenation\n",
        "        enc5 = F.interpolate(enc5, size=x.shape[2:], mode=\"bilinear\", align_corners=True)\n",
        "        x = torch.cat([x, enc5], dim=1)\n",
        "\n",
        "        x = self.upconv2(x)  # (B, 256, 28, 28)\n",
        "\n",
        "        # Ensure enc4 matches x before concatenation\n",
        "        enc4 = F.interpolate(enc4, size=x.shape[2:], mode=\"bilinear\", align_corners=True)\n",
        "        x = torch.cat([x, enc4], dim=1)\n",
        "\n",
        "        x = self.upconv3(x)  # (B, 128, 56, 56)\n",
        "\n",
        "        # Ensure enc3 matches x before concatenation\n",
        "        enc3 = F.interpolate(enc3, size=x.shape[2:], mode=\"bilinear\", align_corners=True)\n",
        "        x = torch.cat([x, enc3], dim=1)\n",
        "\n",
        "        x = self.upconv4(x)  # (B, 64, 112, 112)\n",
        "\n",
        "        # Ensure enc2 matches x before concatenation\n",
        "        enc2 = F.interpolate(enc2, size=x.shape[2:], mode=\"bilinear\", align_corners=True)\n",
        "        x = torch.cat([x, enc2], dim=1)\n",
        "\n",
        "        x = self.upconv5(x)  # (B, 64, 224, 224)\n",
        "\n",
        "        # Ensure final output size is exactly (B, 4, 224, 224)\n",
        "        x = self.final_conv(x)  # (B, 4, 224, 224)\n",
        "        x = F.interpolate(x, size=(224, 224), mode=\"bilinear\", align_corners=True)  # Fix output size\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "# Function to filter mask files that overlap with Vanuatu\n",
        "vanuatu_bbox = [166.0, -20.0, 170.5, -13.0]  # Bounding box for Vanuatu\n",
        "eastasia_pacific_bbox = [140.0, -30.0, 175.0, 35.0]  # Prioritizing East Asia & Vanuatu\n",
        "def filter_masks_within_vanuatu(mask_paths):\n",
        "    valid_masks = []\n",
        "    for mask_path in mask_paths:\n",
        "        with rasterio.open(mask_path) as src:\n",
        "            mask_bounds = src.bounds\n",
        "            mask_box = box(*mask_bounds)\n",
        "            #print(\"mask_box: \", mask_box)\n",
        "            vanuatu_box = box(*eastasia_pacific_bbox)\n",
        "            if mask_box.intersects(vanuatu_box):\n",
        "                valid_masks.append(mask_path)\n",
        "    print(\"len(valid_masks): \", len(valid_masks))\n",
        "    valid_masks = valid_masks[:10]\n",
        "    return valid_masks\n",
        "\n",
        "\n",
        "# Function to fetch Sentinel-1 and Sentinel-2 data dynamically using mask bounds (parallelized)\n",
        "@dask.delayed\n",
        "def fetch_stac_data_from_mask(mask_path, collection, start_date, end_date):\n",
        "    with rasterio.open(mask_path) as src:\n",
        "        bounds = src.bounds\n",
        "    bbox = [bounds.left, bounds.bottom, bounds.right, bounds.top]\n",
        "\n",
        "    catalog = pystac_client.Client.open(\"https://planetarycomputer.microsoft.com/api/stac/v1\")\n",
        "    search = catalog.search(\n",
        "        collections=[collection],\n",
        "        bbox=bbox,\n",
        "        datetime=f\"{start_date}/{end_date}\"\n",
        "    )\n",
        "\n",
        "    items = list(search.get_items())\n",
        "    if not items:\n",
        "        return None  # No matching data found\n",
        "\n",
        "    signed_items = [planetary_computer.sign(item) for item in items]\n",
        "    print(\"len(signed_items): \", len(signed_items))\n",
        "    #for si in signed_items:\n",
        "    #    print(\"polarizations: \", si.properties[\"sar:polarizations\"])\n",
        "\n",
        "    # Lazy loading with Dask\n",
        "    ds = odc.stac.load(signed_items, dtype=\"float32\", chunks={'x': 512, 'y': 512}) # bands=None, # bands= [\"red\", \"green\", \"blue\"],\n",
        "\n",
        "    return ds.mean(dim=\"time\")  # Compute mean lazily\n",
        "\n",
        "\n",
        "# Function to fetch Sentinel-1 RTC data matching a given date (or up to 3 days after)\n",
        "def fetch_s1_rtc_for_date(bounds, date):\n",
        "    catalog = pystac_client.Client.open(\"https://planetarycomputer.microsoft.com/api/stac/v1\")\n",
        "\n",
        "    # Define the search time range (3-day + window)\n",
        "    date = datetime.strptime(date, \"%Y-%m-%d\")  # Convert to datetime object\n",
        "\n",
        "    start_date = date.strftime(\"%Y-%m-%d\")\n",
        "    end_date = (date + timedelta(days=3)).strftime(\"%Y-%m-%d\")\n",
        "\n",
        "    search = catalog.search(\n",
        "        collections=[\"sentinel-1-rtc\"],\n",
        "        bbox=[bounds.left, bounds.bottom, bounds.right, bounds.top],\n",
        "        datetime=f\"{start_date}/{end_date}\"\n",
        "    )\n",
        "\n",
        "    items = list(search.get_items())\n",
        "    if not items:\n",
        "        return None  # No matching data found\n",
        "\n",
        "    signed_items = [planetary_computer.sign(item) for item in items]\n",
        "\n",
        "    # Load the first available scene (or refine selection criteria as needed)\n",
        "    ds = odc.stac.load(signed_items[:1], dtype=\"float32\", chunks={'x': 512, 'y': 512})\n",
        "\n",
        "    return ds.mean(dim=\"time\") if ds else None  # Take mean if multiple images exist\n",
        "\n",
        "# Function to find matching Sentinel-1 RTC imagery for each row in DataFrame\n",
        "def match_s1_rtc_to_dataframe(df, mask_path):\n",
        "    filtered_row = df.loc[df[\"File Name\"] == mask_path].iloc[0]\n",
        "    sat_date = filtered_row[\"Satellite Date\"]\n",
        "    formatted_date = datetime.strptime(sat_date, \"%d/%m/%Y\").strftime(\"%Y-%m-%d\")\n",
        "\n",
        "\n",
        "    with rasterio.open(f\"tst-ground-truth-flood-masks/{mask_path}_ground_truth.tif\") as src:\n",
        "        bounds = src.bounds  # Get bounding box of the mask\n",
        "\n",
        "    s1_image = fetch_s1_rtc_for_date(bounds, formatted_date)\n",
        "\n",
        "    return s1_image\n",
        "\n",
        "# Example DataFrame\n",
        "df = pd.read_csv('metadata/metadata.csv')\n",
        "# Filter for specific countries\n",
        "selected_countries = {\"Vanuatu\", \"Tonga\"} #, \"Timor-Leste\" , \"Philippines\", \"Viet Nam\", \"Australia\"}\n",
        "df = df[df[\"Country\"].isin(selected_countries)]\n",
        "\n",
        "\"\"\"\n",
        "# Limit the number of rows for the Philippines\n",
        "df_philippines = df[df[\"Country\"] == \"Philippines\"].head(5)  # Keep only 10 rows\n",
        "df_other = df[df[\"Country\"] != \"Philippines\"]  # Keep all other countries\n",
        "\n",
        "# Concatenate back\n",
        "df = pd.concat([df_philippines, df_other])\n",
        "\n",
        "# Reset index\n",
        "df = df.reset_index(drop=True)\n",
        "\"\"\"\n",
        "\n",
        "print(\"len(df):\", len(df))\n",
        "\n",
        "\n",
        "class FloodDataset(Dataset):\n",
        "    def __init__(self, mask_paths, transform=None, tile_size=224):\n",
        "        #self.mask_paths = filter_masks_within_vanuatu(mask_paths)\n",
        "        self.mask_paths = list(df[\"File Name\"])\n",
        "        self.transform = transform\n",
        "        self.image_list = []\n",
        "        self.mask_list = []\n",
        "        self.tile_size = tile_size\n",
        "\n",
        "        # Use Dask Delayed for parallel fetching\n",
        "        delayed_images = []\n",
        "        for mask_path in self.mask_paths:\n",
        "            #s1_image = fetch_stac_data_from_mask(mask_path, \"sentinel-1-rtc\", \"2018-01-01\", \"2018-12-31\")\n",
        "            #s2_image = fetch_stac_data_from_mask(mask_path, \"sentinel-2-l2a\", \"2018-01-01\", \"2018-12-31\")\n",
        "            s1_image = match_s1_rtc_to_dataframe(df, mask_path)\n",
        "            delayed_images.append(dask.delayed(s1_image))\n",
        "\n",
        "        # Compute all Dask delayed tasks at once (efficient batch processing)\n",
        "        computed_images = dask.compute(*delayed_images)\n",
        "\n",
        "        self.image_tiles = []\n",
        "        self.mask_tiles = []\n",
        "\n",
        "        for i, mask_path in enumerate(self.mask_paths):\n",
        "            if computed_images[i] is None:\n",
        "                continue  # Skip if no data was found\n",
        "\n",
        "            with rasterio.open(f\"tst-ground-truth-flood-masks/{mask_path}_ground_truth.tif\") as src:\n",
        "                mask = src.read(1).astype(np.float32)\n",
        "            try:\n",
        "                image_tensor = torch.tensor(computed_images[i].to_array().values, dtype=torch.float32)\n",
        "                mask_tensor = torch.tensor(mask).unsqueeze(0)\n",
        "\n",
        "                image_tiles, mask_tiles = self._tile_image_and_mask(image_tensor, mask_tensor)\n",
        "                self.image_tiles.extend(image_tiles)  # Flatten the dataset\n",
        "                self.mask_tiles.extend(mask_tiles)\n",
        "            except:\n",
        "                print(\"error when creating tensor\")\n",
        "                continue\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_tiles)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.image_tiles[idx], self.mask_tiles[idx]\n",
        "\n",
        "    def _tile_image_and_mask(self, image, mask):\n",
        "        #Tiles both the image and mask into patches of size (tile_size x tile_size).\n",
        "\n",
        "        height, width = image.shape[-2:]  # Ensure compatibility with tensor dimensions\n",
        "        image_tiles = []\n",
        "        mask_tiles = []\n",
        "\n",
        "        for i in range(0, height, self.tile_size):\n",
        "            for j in range(0, width, self.tile_size):\n",
        "                # Extract tiles\n",
        "                image_tile = image[..., i:i + self.tile_size, j:j + self.tile_size]\n",
        "                mask_tile = mask[..., i:i + self.tile_size, j:j + self.tile_size]\n",
        "\n",
        "                if mask_tile.shape[-2] == self.tile_size and mask_tile.shape[-1] == self.tile_size and image_tile.shape[-2] == self.tile_size and image_tile.shape[-1] == self.tile_size:\n",
        "                    image_tiles.append(image_tile.clone().detach())\n",
        "                    mask_tiles.append(mask_tile.clone().detach())\n",
        "\n",
        "        return image_tiles, mask_tiles\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "full_dataset = FloodDataset(mask_paths=mask_paths,\n",
        "                             transform=transforms.Compose([transforms.Normalize(0, 1)]), tile_size=224)\n",
        "\n",
        "# Define split sizes\n",
        "train_size = int(0.7 * len(full_dataset))\n",
        "val_size = int(0.15 * len(full_dataset))\n",
        "test_size = len(full_dataset) - train_size - val_size  # Ensures total sums up correctly\n",
        "\n",
        "# Perform split\n",
        "train_dataset, val_dataset, test_dataset = random_split(full_dataset, [train_size, val_size, test_size])\n",
        "\n",
        "# Create DataLoaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True, drop_last=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=2, shuffle=False, drop_last=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=2, shuffle=False, drop_last=True)\n",
        "\n",
        "print(f\"Dataset sizes -> Train: {len(train_dataset)}, Val: {len(val_dataset)}, Test: {len(test_dataset)}\")\n",
        "\n",
        "# Model, Loss, Optimizer\n",
        "model = UNetWithResNetEncoder()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "print(\"Starting training\")\n",
        "\n",
        "# Training Loop\n",
        "for epoch in range(70):\n",
        "    model.train()  # Ensure the model is in training mode\n",
        "\n",
        "    # Training phase\n",
        "    train_loss = 0.0\n",
        "    for batch in train_loader:\n",
        "        images, masks = batch  # Unpack stacked tensors\n",
        "\n",
        "        if isinstance(images, list):\n",
        "            images = torch.cat(images, dim=0)  # Ensure 4D tensor (B, C, H, W)\n",
        "        if isinstance(masks, list):\n",
        "            masks = torch.cat(masks, dim=0)\n",
        "\n",
        "        images = torch.stack(images) if isinstance(images, list) else images\n",
        "        masks = torch.stack(masks) if isinstance(masks, list) else masks\n",
        "        masks = masks.squeeze()\n",
        "        masks = masks.long()  # Convert masks to LongTensor before computing loss\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)  # Forward pass\n",
        "        loss = criterion(outputs, masks)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "\n",
        "    # Average training loss for the epoch\n",
        "    train_loss /= len(train_loader)\n",
        "\n",
        "    # Validation phase\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    val_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            images, masks = batch  # Unpack stacked tensors\n",
        "\n",
        "            if isinstance(images, list):\n",
        "                images = torch.cat(images, dim=0)\n",
        "            if isinstance(masks, list):\n",
        "                masks = torch.cat(masks, dim=0)\n",
        "\n",
        "            images = torch.stack(images) if isinstance(images, list) else images\n",
        "            masks = torch.stack(masks) if isinstance(masks, list) else masks\n",
        "            masks = masks.squeeze()\n",
        "            masks = masks.long()  # Convert masks to LongTensor\n",
        "\n",
        "            outputs = model(images)  # Forward pass\n",
        "            loss = criterion(outputs, masks)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "    # Average validation loss for the epoch\n",
        "    val_loss /= len(val_loader)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}, Train Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}\")\n",
        "\n",
        "# Inference Function\n",
        "def inference(model, image):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        output = model(image.unsqueeze(0))\n",
        "        prediction = torch.softmax(output, dim=1).squeeze(0)  # Softmax across channels\n",
        "        prediction = torch.argmax(prediction, dim=0).cpu().numpy()  # Get class labels\n",
        "    return prediction\n",
        "\n",
        "\n",
        "# Directory to save the model checkpoints\n",
        "MODEL_SAVE_DIR = 'model_s1'\n",
        "os.makedirs(MODEL_SAVE_DIR, exist_ok=True)\n",
        "\n",
        "# Save the final model after training\n",
        "final_model_path = os.path.join(MODEL_SAVE_DIR, \"final_model_100ep_vnu_tvt_1.pth\")\n",
        "torch.save(model.state_dict(), final_model_path)\n",
        "print(f\"Final model saved at {final_model_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5G_ROxKCsFFz"
      },
      "outputs": [],
      "source": [
        "# Inference Function\n",
        "def inference(model, image):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        output = model(image.unsqueeze(0))\n",
        "        prediction = torch.softmax(output, dim=1).squeeze(0)  # Softmax across channels\n",
        "        prediction = torch.argmax(prediction, dim=0).cpu().numpy()  # Get class labels\n",
        "    return prediction\n",
        "\n",
        "\n",
        "# Evaluation Function\n",
        "def evaluate(model, data_loader):\n",
        "    model.eval()\n",
        "    iou_scores, accuracy_scores = [], []\n",
        "    with torch.no_grad():\n",
        "        for images, masks in data_loader:\n",
        "            outputs = model(images)\n",
        "            preds = torch.argmax(outputs, dim=1)  # Convert [batch, 4, H, W] â†’ [batch, H, W]\n",
        "            preds = preds.cpu().numpy().flatten()\n",
        "            masks = masks.cpu().numpy().flatten()\n",
        "            iou_scores.append(jaccard_score(masks, preds, average=\"macro\"))\n",
        "            accuracy_scores.append(accuracy_score(masks, preds))\n",
        "    print(f\"Mean IoU: {np.mean(iou_scores):.4f}, Mean Accuracy: {np.mean(accuracy_scores):.4f}\")\n",
        "\n",
        "# Visualization Function\n",
        "def visualize_prediction(image, mask, prediction):\n",
        "    fig, ax = plt.subplots(1, 3, figsize=(15, 5))\n",
        "\n",
        "    # Convert 2-channel image to single-channel (VV)\n",
        "    vv_image = image[0].cpu().numpy()  # Use first channel (VV)\n",
        "\n",
        "    ax[0].imshow(vv_image, cmap=\"gray\")  # Display as grayscale\n",
        "    ax[0].set_title(\"Input Image (VV)\")\n",
        "\n",
        "    ax[1].imshow(mask.squeeze(), cmap=\"gray\")\n",
        "    ax[1].set_title(\"Ground Truth\")\n",
        "\n",
        "    ax[2].imshow(prediction, cmap=\"gray\")\n",
        "    ax[2].set_title(\"Prediction\")\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# Run Evaluation\n",
        "evaluate(model, test_loader)\n",
        "\n",
        "# Test Inference and Visualization\n",
        "sample_image, sample_mask = test_dataset[0]\n",
        "predicted_mask = inference(model, sample_image)\n",
        "visualize_prediction(sample_image, sample_mask, predicted_mask)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EMSC9hDbEO8M"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
