{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5504dc3d",
   "metadata": {},
   "source": [
    "# Land Accounts inference on Sentinel-2 GeoMAD with Random Forest\n",
    "\n",
    "This workflow demonstrates how to use a\n",
    "[Sentinel-2](https://www.esa.int/Applications/Observing_the_Earth/Copernicus/Sentinel-2)\n",
    "[GeoMedian annual satellite imagery composite](https://github.com/digitalearthpacific/dep-geomad)\n",
    "for segmenting land use / land cover (LULC) using a\n",
    "[GPU-accelerated Random Forest classifier](https://developer.nvidia.com/blog/accelerating-random-forests-up-to-45x-using-cuml/).\n",
    "We will pursue this objective by integrating ground truth land use land cover data\n",
    "from the VBoS from 2020. To make this scalable to all of Vanuatu, we use an\n",
    "[administrative boundaries dataset from Pacific data hub](https://pacificdata.org/data/dataset/2016_vut_phc_admin_boundaries/resource/66ae054b-9b67-4876-b59c-0b078c31e800).\n",
    "\n",
    "In this notebook, we will demonstrate the following:\n",
    "\n",
    "1. **Data Acquisition**:\n",
    "   - We use **Sentinel-2 L2A** data accessed via the [Digital Earth Pacific STAC catalog](http://stac.digitalearthpacific.org/). The search is filtered by parameters like a region of interest (AOI) and time range to obtain suitable imagery.\n",
    "\n",
    "2. **Preprocessing**:\n",
    "   - The Sentinel-2 imagery contains several spectral bands (e.g., Red, Green, Blue, Near-Infrared, Short-wave Infrared). These are extracted and combined into a single dataset for analysis. Remote sensing indices useful for land use / land cover mapping are calculated from these bands. Additionally, the imagery is masked to remove areas outside the regions of interest so as to focus on the relevant pixels. We use 5 out of 6 provinces making up the nation of Vanuatu for training, and one for testing.\n",
    "\n",
    "3. **Feature Extraction**:\n",
    "   - Features for the classifier are extracted from the Sentinel-2 spectral bands. Here, we will use the reflectance values from the Red, Green, Blue, Near-Infrared (NIR), and Short-wave Infrared (SWIR) bands. We will compute remote sensing indices (NDVI, MNDWI, SAVI, BSI) from these bands as the final feature set.\n",
    "\n",
    "4. **Ground Truth Data Integration**:\n",
    "   - A shapefile containing polygons attributed by land cover/land use is loaded into a [GeoDataFrame](https://geopandas.org/en/stable/docs/reference/api/geopandas.GeoDataFrame.html). This allows us to create multi-class labels for the pixels in the Sentinel-2 imagery.\n",
    "\n",
    "5. **Data Splitting**:\n",
    "   - To ensure correct model training, we split the features and labels into training (80%) and testing (20%) sets. A 'seed' value is used for the random number generator to ensure this random split is reproducible.\n",
    "\n",
    "6. **Random Forest Classification**:\n",
    "   - We train a **Random Forest** classifier to predict land use/land cover on a pixel-wise basis. The `n_estimators` parameter is a key hyperparameter, determining the number of decision trees in the forest. Random Forest leverages the collective wisdom of multiple decision trees to make accurate predictions.\n",
    "\n",
    "7. **Prediction**:\n",
    "   - We will use the trained classifier to predict the likelihood of lulc types for each pixel in the test image/province.\n",
    "\n",
    "8. **Evaluation**:\n",
    "   - After making predictions on the test partition, we evaluate the model's performance using metrics such as accuracy and F1-score. This allows us to assess the performance of the Random Forest model and the effectiveness of the selected features.\n",
    "\n",
    "9. **Visualization**:\n",
    "   - We visualize the predictions by plotting the classified map, where lulc types are indicated by specific color codes.\n",
    "\n",
    "At the end, you will have trained a model to predict land use + land cover in Vanuatu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e019beec",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mamba install --channel rapidsai --quiet --yes cuml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0f59b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "\n",
    "# import hvplot.xarray\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import odc.stac\n",
    "import rasterio.features\n",
    "import rioxarray\n",
    "import xarray as xr\n",
    "\n",
    "# from cuml import RandomForestClassifier\n",
    "# from dask_ml.model_selection import train_test_split\n",
    "# from geocube.api.core import make_geocube\n",
    "from pystac_client import Client\n",
    "from shapely.geometry import box, mapping, shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae18d31",
   "metadata": {},
   "source": [
    "## Data Acquisition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d19d60f",
   "metadata": {},
   "source": [
    "Let's read the LULC data into a GeoDataFrame.\n",
    "\n",
    "A [GeoDataFrame](https://geopandas.org/en/stable/docs/reference/geodataframe.html) is a type of data structure used to store geographic data in Python, provided by the [GeoPandas](https://geopandas.org/en/stable/) library. It extends the functionality of a pandas DataFrame to handle spatial data, enabling geospatial analysis and visualization. Like a pandas DataFrame, a GeoDataFrame is a tabular data structure with labeled axes (rows and columns), but it adds special features to work with geometric objects, such as:\n",
    "- a geometry column\n",
    "- a CRS\n",
    "- accessibility to spatial operations (e.g.  intersection, union, buffering, and spatial joins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20250c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Version of the LULC model (based on ROIs.zip)\n",
    "VERSION = \"v9\"\n",
    "YEAR = 2020  # year to run inference on\n",
    "PROVINCE_INFERENCE = \"SHEFA\"  # Vanuatu province, choose from [\"TORBA\", \"SANMA\", \"PENAMA\", \"MALAMPA\", \"TAFEA\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff470298",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the administrative boundaries (2016_phc_vut_pid_4326.geojson)\n",
    "!wget https://pacificdata.org/data/dataset/9dba1377-740c-429e-92ce-6a484657b4d9/resource/3d490d87-99c0-47fd-98bd-211adaf44f71/download/2016_phc_vut_pid_4326.geojson"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a92966",
   "metadata": {},
   "source": [
    "Read and inspect the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1016abd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "lulc_gdf = gpd.read_file(\"./ROIs_v9.zip\")  # \"./ROIs_v5.shp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9499dc54",
   "metadata": {},
   "outputs": [],
   "source": [
    "admin_boundaries_gdf = gpd.read_file(\"./2016_phc_vut_pid_4326.geojson\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36c3fd9",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "admin_boundaries_gdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "678c8c9e",
   "metadata": {},
   "source": [
    "Create raster image and label xarray datarrays for each province."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a01acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not admin_boundaries_gdf.index.name == \"pname\":\n",
    "    admin_boundaries_gdf = admin_boundaries_gdf.set_index(\n",
    "        keys=\"pname\"  # set province name as the index\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b75e817",
   "metadata": {},
   "outputs": [],
   "source": [
    "admin_boundaries_gdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80406ee6",
   "metadata": {},
   "source": [
    "Get geometries of one province."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d687ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "GEOM_INFERENCE = admin_boundaries_gdf.loc[PROVINCE_INFERENCE].geometry\n",
    "GEOM_INFERENCE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b437985",
   "metadata": {},
   "source": [
    "Get Sentinel-2 GeoMedian composite data for year 2020 for one province"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b97bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "STAC_URL = \"http://stac.digitalearthpacific.org/\"\n",
    "stac_client = Client.open(STAC_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e97fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_test = lulc_gdf.query(expr=f\"Pname == '{PROVINCE_INFERENCE}'\")\n",
    "\n",
    "s2_search = stac_client.search(\n",
    "    collections=[\"dep_s2_geomad\"],\n",
    "    intersects=GEOM_INFERENCE,\n",
    "    datetime=str(YEAR),\n",
    ")\n",
    "# Retrieve all items from search results\n",
    "s2_items = s2_search.item_collection()\n",
    "print(\"len(s2_items): \", len(s2_items))\n",
    "\n",
    "s2_data_inference = odc.stac.load(\n",
    "    items=s2_items,\n",
    "    bands=[\"blue\", \"green\", \"red\", \"nir08\", \"swir16\"],\n",
    "    chunks={\"x\": 1024, \"y\": 1024, \"bands\": -1, \"time\": -1},\n",
    "    resolution=20,\n",
    ")\n",
    "s2_data_inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24fd3b5d",
   "metadata": {},
   "source": [
    "Buffer the geometries to include some coastal offshore areas to account for any\n",
    "classes/ROIs that might be relevant and overlapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac052e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep projection aligned with raster\n",
    "raster_crs = s2_data_inference.rio.crs\n",
    "print(raster_crs)\n",
    "\n",
    "# Get only the select province and reproject\n",
    "gdf_reprojected_test = admin_boundaries_gdf.loc[[PROVINCE_INFERENCE]].to_crs(\n",
    "    crs=raster_crs\n",
    ")\n",
    "\n",
    "# Buffer in raster units (meters if UTM)\n",
    "geom_buffered_test = gdf_reprojected_test.buffer(distance=5)[PROVINCE_INFERENCE]\n",
    "geom_buffered_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15f4043",
   "metadata": {},
   "source": [
    "Clip the Sentinel-2 data to be within the buffered geometries only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2628f46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clip inference province\n",
    "s2_clipped_inference = s2_data_inference.rio.clip(geometries=[geom_buffered_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779ef55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot inference province\n",
    "s2_rgb = s2_clipped_inference[[\"red\", \"green\", \"blue\"]]\n",
    "s2_rgb_array = s2_rgb.to_array(\"band\")  # now dims: band, y, x\n",
    "s2_rgb_array_squeezed = s2_rgb_array.squeeze(dim=\"time\", drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09db7f7d",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "s2_rgb_array_squeezed.plot.imshow(size=4, vmin=0, vmax=3000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c8eb40c",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "Calculate remote sensing indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6b114d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate remote sensing indices useful for mapping LULC\n",
    "def compute_indices(ds):\n",
    "    red = ds[\"red\"]\n",
    "    green = ds[\"green\"]\n",
    "    blue = ds[\"blue\"]\n",
    "    nir = ds[\"nir08\"]\n",
    "    swir = ds[\"swir16\"]\n",
    "    eps = 1e-6\n",
    "    return xr.Dataset(\n",
    "        {\n",
    "            \"NDVI\": (nir - red) / (nir + red + eps),\n",
    "            \"MNDWI\": (green - swir) / (green + swir + eps),\n",
    "            \"SAVI\": ((nir - red) / (nir + red + 0.5 + eps)) * 1.5,\n",
    "            \"BSI\": ((swir + red) - (nir + blue)) / ((swir + red) + (nir + blue) + eps),\n",
    "        }\n",
    "    )\n",
    "\n",
    "\n",
    "index_data_test = compute_indices(s2_clipped_inference).squeeze(\"time\", drop=True)\n",
    "print(index_data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4aeb832",
   "metadata": {},
   "source": [
    "Rasterize labels from the ROIs for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e25387",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Rasterize labels\n",
    "width_test, height_test = s2_clipped_inference.x.size, s2_clipped_inference.y.size\n",
    "bands = [\"red\", \"green\", \"blue\", \"nir08\"]\n",
    "\n",
    "gdf_test = gdf_test.to_crs(epsg=s2_clipped_inference.rio.crs.to_epsg())\n",
    "\n",
    "gdf_rpg = lulc_gdf.to_crs(s2_clipped_inference.rio.crs)\n",
    "\n",
    "unique_classes = gdf_rpg[\"ROI\"].unique()\n",
    "# class_mapping = {cls: i+1 for i, cls in enumerate(unique_classes)}\n",
    "class_mapping = {\n",
    "    cls: i for i, cls in enumerate(unique_classes)\n",
    "}  # zero-based, assumes existence of no data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f89370c0",
   "metadata": {},
   "source": [
    "Flatten pixels and only retain the those that overlap with an ROI.\n",
    "The labels (ROIs) are sparse, so we will throw out pixels in regions between ROIs (unlabeled)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81003a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_test = index_data_test.to_array().stack(flattened_pixel=(\"y\", \"x\"))\n",
    "# labels_test = rasterized_labels_test.to_array().stack(flattened_pixel=(\"y\", \"x\"))\n",
    "\n",
    "features_test = features_test[:].transpose(\"flattened_pixel\", \"variable\").compute()\n",
    "\n",
    "print(\"features_test shape:\", features_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2141e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(features_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6888c974",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0c7c50",
   "metadata": {},
   "source": [
    "## Random Forest Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24296ceb",
   "metadata": {},
   "source": [
    "Now we will set up a small [random forest classifider](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) with 10 trees. We use a [seed](https://towardsdatascience.com/why-do-we-set-a-random-state-in-machine-learning-models-bb2dc68d8431) (`random_state`) to ensure reproducibility. Calling the `.fit()` method on the classifier will initiate training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb93c5bc",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Train a Random Forest classifier\n",
    "# clf = RandomForestClassifier(n_estimators=100, random_state=42)  # n_estimators=10\n",
    "# clf.fit(X_train.data, y_train.data)\n",
    "\n",
    "# Load trained Random Forest classifier\n",
    "clf = TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b2464d8",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b0dd6a5",
   "metadata": {},
   "source": [
    "Once the trained classifier has been loaded, we can use it to make predictions on one province."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8094bc6f",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "y_pred = clf.predict(features_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64fe9d93",
   "metadata": {},
   "source": [
    "As a reminder, these are what each class number represents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909f8e85",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "print(\"Class mapping:\")\n",
    "for key, val in class_mapping.items():\n",
    "    print(val, key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e2832e",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e0ad34",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_map = y_pred.reshape((height_test, width_test))\n",
    "predicted_map_xr = xr.DataArray(\n",
    "    data=predicted_map,\n",
    "    coords=s2_clipped_inference.coords,  # coords=rasterized_labels_test.coords\n",
    ")\n",
    "print(np.unique(y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8f7223",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_map_xr.hvplot.image(height=600, rasterize=True, cmap=\"Set1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa5bd02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rasterized_labels_test.ROI_numeric.hvplot.image(rasterize=True, cmap=\"Set1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69ae7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "compatible_array = predicted_map_xr.astype(\"int32\")\n",
    "\n",
    "# Rasterize to polygons\n",
    "polygons = list(\n",
    "    rasterio.features.shapes(\n",
    "        compatible_array.values, transform=compatible_array.rio.transform()\n",
    "    )\n",
    ")\n",
    "\n",
    "# Convert polygons to GeoDataFrame\n",
    "prediction_gdf = gpd.GeoDataFrame(\n",
    "    [{\"geometry\": shape(geom), \"value\": value} for geom, value in polygons],\n",
    "    crs=\"EPSG:3832\",\n",
    ")\n",
    "# print(prediction_gdf)\n",
    "print(prediction_gdf.value.unique())\n",
    "\n",
    "prediction_gdf.to_file(\n",
    "    f\"./predicted_lulc_utm_{PROVINCE_INFERENCE}_{YEAR}.geojson\", driver=\"GeoJSON\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea093154",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_gdf.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ec96ba",
   "metadata": {},
   "source": [
    "You can run these predictions on every province, collect the geodataframes in a list, and combine them into a final, unified nationwide LULC vector dataset like so (placeholder code, you need to generate the predictions first):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e127ced0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction_gdf_merged_nationwide = pd.concat(\n",
    "#     [\n",
    "#         prediction_gdf_TORBA,\n",
    "#         prediction_gdf_SANMA,\n",
    "#         prediction_gdf_PENAMA,\n",
    "#         prediction_gdf_MALAMPA,\n",
    "#         prediction_gdf_SHEFA,\n",
    "#         prediction_gdf_TAFEA,\n",
    "#     ],\n",
    "#     ignore_index=True,\n",
    "# )\n",
    "#\n",
    "# prediction_gdf_merged_nationwide.to_file(\n",
    "#     f\"./predicted_lulc_utm_nationwide_{YEAR}.geojson\", driver=\"GeoJSON\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a530d17",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "py:percent,ipynb"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
